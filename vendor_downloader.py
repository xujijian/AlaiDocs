#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‚å•†å®˜ç½‘ä¸‹è½½å™¨ - ç›´æ¥ä»å„å¤§å‚å•†å®˜ç½‘ä¸‹è½½ç”µæºèµ„æ–™
ä½¿ç”¨ site: æœç´¢é™å®šç¬¦ï¼Œè´¨é‡æ›´é«˜ï¼ŒæˆåŠŸç‡æ›´é«˜
"""

import json
import time
import random
import requests
import sys
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse
import re

try:
    from ddgs import DDGS
except ImportError:
    from duckduckgo_search import DDGS

# é…ç½®
DOWNLOADS_DIR = Path("downloads_continuous")
RESULTS_FILE = DOWNLOADS_DIR / "results.jsonl"
KEYWORDS_FILE = DOWNLOADS_DIR / "explored_keywords.json"
VENDORS_FILE = DOWNLOADS_DIR / "explored_vendors.json"
USED_KEYWORDS_FILE = DOWNLOADS_DIR / "used_keywords.json"

# é—®é¢˜å‚å•†åˆ—è¡¨ï¼ˆæœç´¢ç»“æœä¸å¯é ï¼Œæš‚æ—¶è·³è¿‡ï¼‰
SKIP_VENDORS = set()  # æš‚æ—¶ä¸è·³è¿‡ä»»ä½•å‚å•†

# é»˜è®¤å‚å•†åˆ—è¡¨ï¼ˆå¦‚æœæ²¡æœ‰æ¢ç´¢æ–‡ä»¶ï¼‰
DEFAULT_VENDOR_SITES = {
    'TI': 'ti.com',
    'ADI': 'analog.com',
    'ST': 'st.com',
    'Infineon': 'infineon.com',
    'onsemi': 'onsemi.com',
    'NXP': 'nxp.com',
    'Microchip': 'microchip.com',
    'Renesas': 'renesas.com',
    'MPS': 'monolithicpower.com',
    'ROHM': 'rohm.com',
    'Vishay': 'vishay.com',
    'Diodes': 'diodes.com',
    'PI': 'power.com',
    'Navitas': 'navitassemi.com',
    'Richtek': 'richtek.com',
    'AOS': 'aosmd.com',
    'Vicor': 'vicorpower.com'
}

# é»˜è®¤å…³é”®è¯åˆ—è¡¨ï¼ˆå¦‚æœæ²¡æœ‰æ¢ç´¢æ–‡ä»¶ï¼‰
DEFAULT_POWER_KEYWORDS = [
    'dc-dc converter datasheet',
    'buck converter',
    'boost converter',
    'flyback converter',
    'power management',
    'voltage regulator',
    'switching regulator',
    'pmic datasheet',
    'power supply design',
    'application note power'
]

def load_used_keywords():
    """åŠ è½½å·²ä½¿ç”¨çš„å…³é”®è¯"""
    if USED_KEYWORDS_FILE.exists():
        try:
            with open(USED_KEYWORDS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('used', []))
        except Exception:
            pass
    return set()

def save_used_keyword(keyword):
    """ä¿å­˜å·²ä½¿ç”¨çš„å…³é”®è¯"""
    used = load_used_keywords()
    used.add(keyword)
    try:
        with open(USED_KEYWORDS_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'used': list(used),
                'count': len(used)
            }, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜å·²ä½¿ç”¨å…³é”®è¯å¤±è´¥: {e}")

def load_explored_data():
    """åŠ è½½æ¢ç´¢å¾—åˆ°çš„å…³é”®è¯å’Œå‚å•†"""
    vendors = DEFAULT_VENDOR_SITES.copy()
    keywords = DEFAULT_POWER_KEYWORDS.copy()
    
    # å°è¯•åŠ è½½æ¢ç´¢çš„å…³é”®è¯
    if KEYWORDS_FILE.exists():
        try:
            with open(KEYWORDS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                explored_keywords = data.get('keywords', [])
                if explored_keywords:
                    print(f"ğŸ“¥ åŠ è½½ Gemini æ¢ç´¢çš„ {len(explored_keywords)} ä¸ªå…³é”®è¯")
                    keywords = explored_keywords
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å…³é”®è¯æ–‡ä»¶å¤±è´¥: {e}")
    
    # å°è¯•åŠ è½½æ¢ç´¢çš„å‚å•†
    if VENDORS_FILE.exists():
        try:
            with open(VENDORS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                explored_vendors = data.get('vendors', {})
                if explored_vendors:
                    print(f"ğŸ“¥ åŠ è½½ Gemini æ¢ç´¢çš„ {len(explored_vendors)} ä¸ªå‚å•†")
                    vendors.update(explored_vendors)
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å‚å•†æ–‡ä»¶å¤±è´¥: {e}")
    
    # è¿‡æ»¤æ‰å·²ä½¿ç”¨çš„å…³é”®è¯
    used_keywords = load_used_keywords()
    if used_keywords:
        original_count = len(keywords)
        keywords = [k for k in keywords if k not in used_keywords]
        filtered_count = original_count - len(keywords)
        if filtered_count > 0:
            print(f"ğŸ”„ è¿‡æ»¤æ‰ {filtered_count} ä¸ªå·²ä½¿ç”¨çš„å…³é”®è¯ï¼Œå‰©ä½™ {len(keywords)} ä¸ª")
    
    return vendors, keywords

# åœ¨å¯åŠ¨æ—¶åŠ è½½
VENDOR_SITES, POWER_KEYWORDS = load_explored_data()

def sanitize_filename(title: str, max_length: int = 200) -> str:
    """æ¸…ç†æ–‡ä»¶å"""
    filename = re.sub(r'[<>:"/\\|?*]', '', title)
    if len(filename) > max_length:
        filename = filename[:max_length]
    filename = filename.strip('. ')
    return filename or "untitled"

def is_pdf_url(url: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦æ˜¯PDF URL"""
    url_lower = url.lower()
    if url_lower.endswith('.pdf'):
        return True
    
    pdf_patterns = [
        '/pdf/', '/lit/', '/datasheet/', '/ds/',
        'filetype=pdf', 'type=pdf', '.pdf?',
        '/media/', '/technical-documentation/',
        '/application-note/', '/an/'
    ]
    return any(p in url_lower for p in pdf_patterns)

def download_pdf(url: str, vendor_dir: Path, title: str) -> tuple:
    """ä¸‹è½½PDF"""
    try:
        filename = sanitize_filename(title)
        if not filename.lower().endswith('.pdf'):
            filename += '.pdf'
        
        filepath = vendor_dir / filename
        
        if filepath.exists():
            return True, str(filepath), None
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30, stream=True)
        response.raise_for_status()
        
        content_type = response.headers.get('Content-Type', '').lower()
        if 'pdf' not in content_type and 'octet-stream' not in content_type:
            return False, None, f"éPDF: {content_type}"
        
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        
        if filepath.stat().st_size < 1024:
            filepath.unlink()
            return False, None, "æ–‡ä»¶å¤ªå°"
        
        return True, str(filepath), None
        
    except Exception as e:
        return False, None, str(e)[:50]

def save_result(result: dict):
    """ä¿å­˜ç»“æœ"""
    DOWNLOADS_DIR.mkdir(parents=True, exist_ok=True)
    with open(RESULTS_FILE, 'a', encoding='utf-8') as f:
        f.write(json.dumps(result, ensure_ascii=False) + '\n')

def search_vendor_site(vendor: str, site: str, keyword: str, max_results: int = 20) -> list:
    """æœç´¢ç‰¹å®šå‚å•†å®˜ç½‘"""
    query = f"site:{site} {keyword} filetype:pdf"
    
    try:
        with DDGS() as ddgs:
            results = []
            for result in ddgs.text(query, max_results=max_results):
                url = result.get('href', '')
                if is_pdf_url(url) and site in url:
                    results.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')
                    })
            return results
    except Exception as e:
        print(f"    âš ï¸  æœç´¢å¤±è´¥: {e}")
        return []

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    selected_vendors = None
    if len(sys.argv) > 1:
        # æ”¯æŒ: python vendor_downloader.py TI,ADI,ST
        vendor_arg = sys.argv[1]
        input_vendors = [v.strip() for v in vendor_arg.split(',')]
        
        # ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
        vendor_map = {k.upper(): k for k in VENDOR_SITES.keys()}
        selected_vendors = []
        invalid = []
        
        for v in input_vendors:
            v_upper = v.upper()
            if v_upper in vendor_map:
                selected_vendors.append(vendor_map[v_upper])
            else:
                invalid.append(v)
        
        if invalid:
            print(f"âŒ æ— æ•ˆçš„å‚å•†åç§°: {', '.join(invalid)}")
            print(f"\nå¯ç”¨å‚å•†: {', '.join(VENDOR_SITES.keys())}")
            return
    
    # è¿‡æ»¤å‚å•†åˆ—è¡¨
    if selected_vendors:
        vendors_to_process = {k: v for k, v in VENDOR_SITES.items() if k in selected_vendors}
    else:
        vendors_to_process = VENDOR_SITES
    
    print("="*70)
    print("å‚å•†å®˜ç½‘ç”µæºèµ„æ–™ä¸‹è½½å™¨")
    print("="*70)
    print(f"ç›®æ ‡å‚å•†: {len(vendors_to_process)} ä¸ª - {', '.join(vendors_to_process.keys())}")
    print(f"æœç´¢å…³é”®è¯: {len(POWER_KEYWORDS)} ä¸ª")
    print(f"ç­–ç•¥: è½®æµè®¿é—®å„å‚å•†ï¼Œé¿å…è¢«è¯†åˆ«ä¸ºæœºå™¨äºº")
    print("="*70)
    print()
    
    total_downloaded = 0
    total_searched = 0
    vendor_stats = {vendor: 0 for vendor in vendors_to_process.keys()}
    
    # å¤–å±‚å¾ªç¯ï¼šå…³é”®è¯
    for keyword_idx, keyword in enumerate(POWER_KEYWORDS, 1):
        print(f"\n{'='*70}")
        print(f"å…³é”®è¯ [{keyword_idx}/{len(POWER_KEYWORDS)}]: {keyword}")
        print(f"{'='*70}")
        
        # å†…å±‚å¾ªç¯ï¼šè½®æµè®¿é—®æ¯ä¸ªå‚å•†
        for vendor, site in vendors_to_process.items():
            # è·³è¿‡é—®é¢˜å‚å•†
            if vendor in SKIP_VENDORS:
                print(f"\n  {vendor} ({site}) - â­ï¸  è·³è¿‡ï¼ˆç½‘ç«™ä¸å…¼å®¹ï¼‰")
                continue
            
            print(f"\n  {vendor} ({site})")
            
            vendor_dir = DOWNLOADS_DIR / vendor.lower()
            vendor_dir.mkdir(parents=True, exist_ok=True)
            
            results = search_vendor_site(vendor, site, keyword, max_results=150)
            total_searched += 1
            
            if not results:
                print(f"    âš ï¸  æœªæ‰¾åˆ°PDF")
                time.sleep(random.uniform(1, 2))
                continue
            
            print(f"    æ‰¾åˆ° {len(results)} ä¸ªPDFï¼Œå¼€å§‹ä¸‹è½½...")
            
            # ä¸‹è½½æ‰€æœ‰æ‰¾åˆ°çš„PDFï¼ˆæœ€å¤š100ä¸ªï¼‰
            download_limit = min(len(results), 100)
            downloaded_this_round = 0
            
            for i, result in enumerate(results[:download_limit], 1):
                url = result.get('url', '')
                title = result.get('title', 'Untitled')
                
                success, filepath, error = download_pdf(url, vendor_dir, title)
                
                if success:
                    downloaded_this_round += 1
                    vendor_stats[vendor] += 1
                    total_downloaded += 1
                    if i % 20 == 0 or i <= 3:  # æ¯20ä¸ªæˆ–å‰3ä¸ªæ˜¾ç¤º
                        print(f"      âœ… [{i}/{download_limit}] {Path(filepath).name[:50]}...")
                    
                    save_result({
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'query': f"site:{site} {keyword}",
                        'vendor': vendor,
                        'title': title,
                        'url': url,
                        'filetype': 'pdf',
                        'filepath': str(filepath),
                        'status': 'success',
                        'error': None,
                        'official': True
                    })
                else:
                    if i <= 2:
                        print(f"      âŒ [{i}/{download_limit}] {error}")
                
                time.sleep(random.uniform(0.3, 0.5))
            
            if downloaded_this_round > 0:
                print(f"    ğŸ“¥ æœ¬è½®ä¸‹è½½: {downloaded_this_round} ä¸ª")
            
            # æ¯ä¸ªå‚å•†åçŸ­æš‚å»¶è¿Ÿ
            time.sleep(random.uniform(1, 2))
        
        # æ¯ä¸ªå…³é”®è¯å®Œæˆåï¼Œæ ‡è®°ä¸ºå·²ä½¿ç”¨
        save_used_keyword(keyword)
        
        # æ¯ä¸ªå…³é”®è¯åæ˜¾ç¤ºè¿›åº¦
        print(f"\n  æ€»è®¡å·²ä¸‹è½½: {total_downloaded} ä¸ª")
        time.sleep(random.uniform(2, 3))
    
    print("\n" + "="*70)
    print("ä¸‹è½½å®Œæˆ")
    print("="*70)
    print(f"âœ… æˆåŠŸä¸‹è½½: {total_downloaded} ä¸ª")
    print(f"ğŸ” æœç´¢æ¬¡æ•°: {total_searched} æ¬¡")
    print(f"ğŸ“ ä¿å­˜ä½ç½®: {DOWNLOADS_DIR}")
    print()
    print("è¿™äº›æ–‡ä»¶è´¨é‡é«˜ï¼Œæ¥è‡ªå®˜æ–¹ç½‘ç«™ï¼")
    print("åˆ†ç±»å™¨ä¼šè‡ªåŠ¨å¤„ç†è¿™äº›æ–‡ä»¶ã€‚")
    print()
    
    # æŒ‰å‚å•†ç»Ÿè®¡
    if vendor_stats:
        print("å„å‚å•†ä¸‹è½½ç»Ÿè®¡:")
        for vendor, count in sorted(vendor_stats.items(), key=lambda x: x[1], reverse=True):
            if count > 0:
                print(f"  {vendor}: {count} ä¸ª")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å…³é”®è¯
    print()
    print("="*70)
    print("æç¤ºï¼š")
    print("1. å¯é‡æ–°è¿è¡Œæ­¤æ‰¹å¤„ç†ï¼Œä¼šè‡ªåŠ¨åŠ è½½æ–°å…³é”®è¯")
    print("2. è¿è¡Œ start_download_only.bat ç”Ÿæˆæ›´å¤šå…³é”®è¯")
    print("3. å·²ç”¨å…³é”®è¯ä¼šè‡ªåŠ¨è·³è¿‡")
    print("="*70)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        print("å·²ä¸‹è½½çš„æ–‡ä»¶å·²ä¿å­˜")
