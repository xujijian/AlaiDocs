#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæ¢å¤ä¸‹è½½å™¨ - ä¸“é—¨æ¢å¤ä¸¢å¤±çš„æ–‡ä»¶
ç»“åˆ continuous_searcher ä¸€èµ·å·¥ä½œï¼ŒåŠ é€Ÿæ¢å¤
"""

import json
import time
import random
import requests
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse
import re

# DuckDuckGo æœç´¢
try:
    from ddgs import DDGS
except ImportError:
    try:
        from duckduckgo_search import DDGS
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…æœç´¢åº“")
        print("   è¿è¡Œ: pip install ddgs")
        exit(1)

# é…ç½®
DOWNLOADS_DIR = Path("downloads_continuous")
RESULTS_FILE = DOWNLOADS_DIR / "results.jsonl"
SUMMARY_FILE = DOWNLOADS_DIR / "summary.csv"

def sanitize_filename(title: str, max_length: int = 200) -> str:
    """æ¸…ç†æ–‡ä»¶å"""
    # ç§»é™¤éæ³•å­—ç¬¦
    filename = re.sub(r'[<>:"/\\|?*]', '', title)
    # é™åˆ¶é•¿åº¦
    if len(filename) > max_length:
        filename = filename[:max_length]
    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
    filename = filename.strip('. ')
    return filename or "untitled"

def download_pdf(url: str, save_dir: Path, title: str) -> tuple:
    """
    ä¸‹è½½PDFæ–‡ä»¶
    è¿”å›: (æˆåŠŸ, æ–‡ä»¶è·¯å¾„, é”™è¯¯ä¿¡æ¯)
    """
    try:
        # ç”Ÿæˆæ–‡ä»¶å
        filename = sanitize_filename(title)
        if not filename.lower().endswith('.pdf'):
            filename += '.pdf'
        
        filepath = save_dir / filename
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
        if filepath.exists():
            return True, str(filepath), None
        
        # ä¸‹è½½
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30, stream=True)
        response.raise_for_status()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯PDF
        content_type = response.headers.get('Content-Type', '').lower()
        if 'pdf' not in content_type and 'application/octet-stream' not in content_type:
            return False, None, f"ä¸æ˜¯PDFæ–‡ä»¶: {content_type}"
        
        # ä¿å­˜æ–‡ä»¶
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        
        # éªŒè¯æ–‡ä»¶å¤§å°
        if filepath.stat().st_size < 1024:  # å°äº1KB
            filepath.unlink()
            return False, None, "æ–‡ä»¶å¤ªå°"
        
        return True, str(filepath), None
        
    except requests.Timeout:
        return False, None, "ä¸‹è½½è¶…æ—¶"
    except requests.RequestException as e:
        return False, None, f"ä¸‹è½½å¤±è´¥: {str(e)}"
    except Exception as e:
        return False, None, f"é”™è¯¯: {str(e)}"

def save_result(result: dict):
    """ä¿å­˜ä¸‹è½½ç»“æœ"""
    DOWNLOADS_DIR.mkdir(parents=True, exist_ok=True)
    
    with open(RESULTS_FILE, 'a', encoding='utf-8') as f:
        f.write(json.dumps(result, ensure_ascii=False) + '\n')

def search_ddg(query: str, max_results: int = 20) -> list:
    """ä½¿ç”¨ DuckDuckGo æœç´¢"""
    try:
        with DDGS() as ddgs:
            results = []
            for result in ddgs.text(query, max_results=max_results):
                url = result.get('href', '')
                # åªä¿ç•™å¯èƒ½æ˜¯ PDF çš„ URL
                if is_likely_pdf_url(url):
                    results.append({
                        'title': result.get('title', ''),
                        'url': url,
                        'snippet': result.get('body', '')
                    })
            return results
    except Exception as e:
        print(f"  âš ï¸  æœç´¢å¤±è´¥: {e}")
        return []

def is_likely_pdf_url(url: str) -> bool:
    """åˆ¤æ–­ URL æ˜¯å¦å¯èƒ½æ˜¯ PDF"""
    url_lower = url.lower()
    
    # æ˜ç¡®çš„ PDF URL
    if url_lower.endswith('.pdf'):
        return True
    
    # å¸¸è§çš„ PDF è·¯å¾„ç‰¹å¾
    pdf_indicators = [
        '/pdf/', '/pdfs/', '/downloads/', '/datasheet/',
        'filetype=pdf', 'type=pdf', '.pdf?',
        'lit/pdf', 'media/pdf', 'doc/pdf',
        '/ds/', '/an/', 'technical-documentation'
    ]
    
    return any(indicator in url_lower for indicator in pdf_indicators)

def load_recovery_queries():
    """åŠ è½½æ¢å¤æœç´¢æ¸…å•"""
    recovery_file = Path("recovery_searches.json")
    if not recovery_file.exists():
        print("âŒ æ‰¾ä¸åˆ° recovery_searches.json")
        print("   è¯·å…ˆè¿è¡Œ: python create_recovery_plan.py")
        return []
    
    with open(recovery_file, 'r', encoding='utf-8') as f:
        queries = json.load(f)
    
    return queries

def recovery_search():
    """æ‰§è¡Œæ¢å¤æœç´¢"""
    queries = load_recovery_queries()
    
    if not queries:
        return
    
    print("="*70)
    print("å¿«é€Ÿæ¢å¤ä¸‹è½½å™¨")
    print("="*70)
    print(f"æ¢å¤æœç´¢æ¸…å•: {len(queries)} ä¸ªæŸ¥è¯¢")
    print(f"ç›®æ ‡: å¿«é€Ÿæ¢å¤ä¸¢å¤±çš„ 2000+ ä¸ªæ–‡ä»¶")
    print(f"ç­–ç•¥: é«˜ä¼˜å…ˆçº§æŸ¥è¯¢ + å¿«é€Ÿä¸‹è½½")
    print("="*70)
    print()
    
    total_downloaded = 0
    total_failed = 0
    
    for idx, query_info in enumerate(queries, 1):
        query = query_info['query']
        vendor = query_info.get('vendor', 'unknown')
        doc_type = query_info.get('doc_type', 'unknown')
        priority = query_info.get('priority', 0)
        
        print(f"\n[{idx}/{len(queries)}] ä¼˜å…ˆçº§: {priority}")
        print(f"æŸ¥è¯¢: {query}")
        print(f"ç±»å‹: {vendor} / {doc_type}")
        
        try:
            # æœç´¢
            results = search_ddg(query, max_results=20)
            
            if not results:
                print("  âš ï¸  æœªæ‰¾åˆ°ç»“æœ")
                continue
            
            print(f"  æ‰¾åˆ° {len(results)} ä¸ªPDFé“¾æ¥")
            
            # ä¸‹è½½å‰ 10 ä¸ª
            downloaded_count = 0
            for i, result in enumerate(results[:10], 1):
                url = result.get('url', '')
                title = result.get('title', 'Untitled')
                
                # ç¡®å®šå‚å•†ç›®å½•
                if vendor != 'Unknown' and vendor != 'unknown':
                    vendor_dir = DOWNLOADS_DIR / vendor.lower()
                else:
                    vendor_dir = DOWNLOADS_DIR / "unknown"
                
                vendor_dir.mkdir(parents=True, exist_ok=True)
                
                # ä¸‹è½½
                success, filepath, error = download_pdf(url, vendor_dir, title)
                
                if success:
                    downloaded_count += 1
                    total_downloaded += 1
                    print(f"    âœ… [{i}/10] {Path(filepath).name[:50]}...")
                    
                    # ä¿å­˜è®°å½•
                    save_result({
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'query': query,
                        'title': title,
                        'url': url,
                        'filetype': 'pdf',
                        'filepath': str(filepath),
                        'status': 'success',
                        'error': None,
                        'recovery': True  # æ ‡è®°ä¸ºæ¢å¤ä¸‹è½½
                    })
                else:
                    total_failed += 1
                    if i <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªå¤±è´¥
                        print(f"    âŒ [{i}/10] {error}")
                
                # çŸ­æš‚å»¶è¿Ÿ
                time.sleep(random.uniform(1, 2))
            
            print(f"  ğŸ“¥ æœ¬è½®ä¸‹è½½: {downloaded_count} ä¸ª")
            
            # æ¯æ¬¡æœç´¢åå»¶è¿Ÿ
            time.sleep(random.uniform(3, 5))
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"  âŒ æœç´¢å¤±è´¥: {e}")
            continue
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("å¿«é€Ÿæ¢å¤å®Œæˆ")
    print("="*70)
    print(f"âœ… æˆåŠŸä¸‹è½½: {total_downloaded} ä¸ª")
    print(f"âŒ å¤±è´¥: {total_failed} ä¸ª")
    print(f"ğŸ“ ä¿å­˜ä½ç½®: {DOWNLOADS_DIR}")
    print()
    print("ä¸‹ä¸€æ­¥:")
    print("1. è¿™äº›æ–‡ä»¶ä¼šè¢«åˆ†ç±»å™¨è‡ªåŠ¨å¤„ç†")
    print("2. continuous_searcher ç»§ç»­è¿è¡Œï¼Œä¼šè¡¥å……æ›´å¤šæ–‡ä»¶")
    print("3. å»ºè®®è¿è¡Œå‡ è½®æ¢å¤ä¸‹è½½ï¼ˆCtrl+C å¯éšæ—¶åœæ­¢ï¼‰")

if __name__ == "__main__":
    recovery_search()
