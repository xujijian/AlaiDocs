#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google æœç´¢å¼•æ“é€‚é…å™¨
åŸºäº googlesearch-python åº“å®ç°çš„ Google æœç´¢åŠŸèƒ½

ä½œè€…: AI åŠ©æ‰‹
ç‰ˆæœ¬: 1.0.0
æ—¥æœŸ: 2026-01-27
Python: 3.10+
ä¾èµ–: googlesearch-python, requests, selenium
"""

import logging
import re
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import unquote, urlparse

try:
    from googlesearch import search as google_search
    GOOGLESEARCH_AVAILABLE = True
except ImportError:
    GOOGLESEARCH_AVAILABLE = False
    print("è­¦å‘Š: googlesearch-python åº“æœªå®‰è£…")
    print("è¯·è¿è¡Œ: pip install googlesearch-python")

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================

# ä¾›åº”å•†åŸŸåç™½åå•ï¼ˆä¸ ddg_fetcher_browser.py ä¿æŒä¸€è‡´ï¼‰
VENDOR_DOMAINS = {
    "ti": ["ti.com", "www.ti.com"],
    "st": ["st.com", "www.st.com"],
    "analog": ["analog.com", "www.analog.com"],
    "infineon": ["infineon.com", "www.infineon.com"],
    "onsemi": ["onsemi.com", "www.onsemi.com"],
    "renesas": ["renesas.com", "www.renesas.com"],
    "nxp": ["nxp.com", "www.nxp.com"],
    "microchip": ["microchip.com", "www.microchip.com"],
    "rohm": ["rohm.com", "www.rohm.com"],
    "toshiba": ["toshiba-semiconductor.com", "www.toshiba-semiconductor.com"],
    "vishay": ["vishay.com", "www.vishay.com"],
    "mps": ["monolithicpower.com", "www.monolithicpower.com"],
    "pi": ["power.com", "www.power.com"],
    "vicor": ["vicorpower.com", "www.vicorpower.com"],
    "navitas": ["navitassemi.com", "www.navitassemi.com"],
    "diodes": ["diodes.com", "www.diodes.com"],
    "aos": ["aosmd.com", "www.aosmd.com"],
    "richtek": ["richtek.com", "www.richtek.com"],
    "silergy": ["silergy.com", "www.silergy.com"],
}

# æ‰€æœ‰ç™½åå•åŸŸåï¼ˆæ‰å¹³åŒ–ï¼‰
ALL_WHITELIST_DOMAINS = set()
for domains in VENDOR_DOMAINS.values():
    ALL_WHITELIST_DOMAINS.update(domains)


class GoogleFetcher:
    """Google æœç´¢å¼•æ“é€‚é…å™¨"""
    
    def __init__(
        self,
        output_dir: Path,
        results_limit: int = 50,
        download_limit: int = 20,
        domain_whitelist: Optional[List[str]] = None,
        logger: Optional[logging.Logger] = None,
        headless: bool = True,
        download_timeout: int = 300
    ):
        """
        åˆå§‹åŒ– Google æœç´¢å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            results_limit: æœç´¢ç»“æœæ•°é‡é™åˆ¶
            download_limit: ä¸‹è½½æ–‡ä»¶æ•°é‡é™åˆ¶
            domain_whitelist: åŸŸåç™½åå•ï¼ˆNone åˆ™ä½¿ç”¨é»˜è®¤ç™½åå•ï¼‰
            logger: æ—¥å¿—è®°å½•å™¨
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            download_timeout: ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        if not GOOGLESEARCH_AVAILABLE:
            raise ImportError("googlesearch-python åº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install googlesearch-python")
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.results_limit = results_limit
        self.download_limit = download_limit
        self.domain_whitelist = domain_whitelist or list(ALL_WHITELIST_DOMAINS)
        self.logger = logger or logging.getLogger(__name__)
        self.headless = headless
        self.download_timeout = download_timeout
        
        # æµè§ˆå™¨é©±åŠ¨
        self.driver = None
        self.download_dir = self.output_dir / "_temp_downloads"
        self.download_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_queries": 0,
            "total_results": 0,
            "total_downloads": 0,
            "total_size": 0,
            "vendor_downloads": {},
        }
    
    def _init_chrome_driver(self):
        """åˆå§‹åŒ– Chrome WebDriver"""
        if self.driver:
            return
        
        self.logger.info("ğŸŒ åˆå§‹åŒ– Chrome æµè§ˆå™¨...")
        
        options = Options()
        
        if self.headless:
            options.add_argument("--headless=new")
        
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-software-rasterizer")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        
        # ä¸‹è½½è®¾ç½®
        prefs = {
            "download.default_directory": str(self.download_dir.absolute()),
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "plugins.always_open_pdf_externally": True,
            "safebrowsing.enabled": False,
        }
        options.add_experimental_option("prefs", prefs)
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            self.logger.info("âœ… Chrome æµè§ˆå™¨å·²å¯åŠ¨")
        except Exception as e:
            self.logger.error(f"âŒ å¯åŠ¨ Chrome å¤±è´¥: {e}")
            raise
    
    def search(self, query: str) -> List[Dict]:
        """
        ä½¿ç”¨ Google æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å« url, title, description
        """
        self.logger.info(f"ğŸ” Google æœç´¢: {query}")
        self.stats["total_queries"] += 1
        
        results = []
        
        try:
            # ä½¿ç”¨ googlesearch-python åº“æœç´¢
            search_results = google_search(
                query,
                num_results=self.results_limit * 2,  # å¤šæœç´¢ä¸€äº›ç¡®ä¿æœ‰è¶³å¤Ÿç»“æœ
                lang="en",
                sleep_interval=2,  # ä¸¤æ¬¡è¯·æ±‚ä¹‹é—´æš‚åœ2ç§’
                timeout=30
            )
            
            for url in search_results:
                # æ£€æŸ¥åŸŸåç™½åå•
                if not self._is_whitelisted_domain(url):
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ PDF
                if not self._is_pdf_url(url):
                    continue
                
                results.append({
                    "url": url,
                    "title": self._extract_title_from_url(url),
                    "description": f"æ¥è‡ª {urlparse(url).netloc}",
                    "vendor": self._get_vendor_from_url(url)
                })
                
                if len(results) >= self.results_limit:
                    break
            
            self.logger.info(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            self.stats["total_results"] += len(results)
            
        except Exception as e:
            self.logger.error(f"âŒ Google æœç´¢å¤±è´¥: {e}")
        
        return results
    
    def download_result(self, result: Dict) -> Optional[Path]:
        """
        ä¸‹è½½å•ä¸ªæœç´¢ç»“æœ
        
        Args:
            result: æœç´¢ç»“æœå­—å…¸
            
        Returns:
            Optional[Path]: ä¸‹è½½æˆåŠŸè¿”å›æ–‡ä»¶è·¯å¾„ï¼Œå¦åˆ™è¿”å› None
        """
        url = result["url"]
        vendor = result.get("vendor", "unknown")
        
        self.logger.info(f"ğŸ“¥ ä¸‹è½½: {url}")
        
        try:
            if not self.driver:
                self._init_chrome_driver()
            
            # æ¸…ç©ºä¸´æ—¶ä¸‹è½½ç›®å½•
            for file in self.download_dir.iterdir():
                if file.is_file():
                    file.unlink()
            
            # ä½¿ç”¨æµè§ˆå™¨ä¸‹è½½
            self.driver.get(url)
            
            # ç­‰å¾…ä¸‹è½½å®Œæˆ
            downloaded_file = self._wait_for_download()
            
            if not downloaded_file:
                self.logger.warning(f"âš ï¸  ä¸‹è½½å¤±è´¥: {url}")
                return None
            
            # ç§»åŠ¨åˆ°ä¾›åº”å•†ç›®å½•
            vendor_dir = self.output_dir / vendor
            vendor_dir.mkdir(parents=True, exist_ok=True)
            
            dest_path = vendor_dir / downloaded_file.name
            
            # é¿å…é‡å
            counter = 1
            while dest_path.exists():
                stem = downloaded_file.stem
                suffix = downloaded_file.suffix
                dest_path = vendor_dir / f"{stem}_{counter}{suffix}"
                counter += 1
            
            downloaded_file.rename(dest_path)
            
            file_size = dest_path.stat().st_size
            self.stats["total_downloads"] += 1
            self.stats["total_size"] += file_size
            self.stats["vendor_downloads"][vendor] = self.stats["vendor_downloads"].get(vendor, 0) + 1
            
            self.logger.info(f"âœ… å·²ä¿å­˜: {dest_path.name} ({file_size / 1024:.1f} KB)")
            
            return dest_path
            
        except Exception as e:
            self.logger.error(f"âŒ ä¸‹è½½å¤±è´¥ {url}: {e}")
            return None
    
    def _wait_for_download(self, timeout: int = None) -> Optional[Path]:
        """ç­‰å¾…ä¸‹è½½å®Œæˆ"""
        timeout = timeout or self.download_timeout
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            files = list(self.download_dir.glob("*"))
            
            # è¿‡æ»¤æ‰ä¸´æ—¶æ–‡ä»¶
            completed_files = [
                f for f in files
                if f.is_file() and not f.name.endswith((".crdownload", ".tmp"))
            ]
            
            if completed_files:
                # ç­‰å¾…æ–‡ä»¶å¤§å°ç¨³å®š
                time.sleep(2)
                return completed_files[0]
            
            time.sleep(1)
        
        return None
    
    def _is_whitelisted_domain(self, url: str) -> bool:
        """æ£€æŸ¥ URL æ˜¯å¦åœ¨åŸŸåç™½åå•ä¸­"""
        try:
            domain = urlparse(url).netloc.lower()
            return any(wd in domain for wd in self.domain_whitelist)
        except:
            return False
    
    def _is_pdf_url(self, url: str) -> bool:
        """æ£€æŸ¥ URL æ˜¯å¦æŒ‡å‘ PDF æ–‡ä»¶"""
        try:
            url_lower = url.lower()
            if url_lower.endswith(".pdf"):
                return True
            parsed = urlparse(url)
            if ".pdf" in parsed.path.lower():
                return True
            return False
        except:
            return False
    
    def _extract_title_from_url(self, url: str) -> str:
        """ä» URL æå–æ ‡é¢˜"""
        try:
            parsed = urlparse(url)
            filename = parsed.path.split("/")[-1]
            title = filename.replace(".pdf", "").replace(".PDF", "")
            title = unquote(title)
            title = title.replace("_", " ").replace("-", " ").replace("+", " ")
            return title[:100] if title else "æœªçŸ¥æ ‡é¢˜"
        except:
            return "æœªçŸ¥æ ‡é¢˜"
    
    def _get_vendor_from_url(self, url: str) -> str:
        """ä» URL è¯†åˆ«ä¾›åº”å•†"""
        try:
            domain = urlparse(url).netloc.lower()
            for vendor, domains in VENDOR_DOMAINS.items():
                if any(d in domain for d in domains):
                    return vendor
            return "unknown"
        except:
            return "unknown"
    
    def fetch_and_download(self, query: str) -> Dict:
        """
        æœç´¢å¹¶ä¸‹è½½ï¼ˆå®Œæ•´æµç¨‹ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        results = self.search(query)
        
        downloaded_count = 0
        for result in results[:self.download_limit]:
            if self.download_result(result):
                downloaded_count += 1
        
        return {
            "query": query,
            "results_found": len(results),
            "files_downloaded": downloaded_count,
        }
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            try:
                self.driver.quit()
                self.logger.info("ğŸ”’ Chrome æµè§ˆå™¨å·²å…³é—­")
            except:
                pass
            self.driver = None
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        self.close()


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£ï¼ˆç”¨äºæµ‹è¯•ï¼‰
# ============================================================================

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Google æœç´¢å¼•æ“æµ‹è¯•å·¥å…·")
    parser.add_argument("query", help="æœç´¢æŸ¥è¯¢")
    parser.add_argument("-o", "--output", default="./downloads_google", help="è¾“å‡ºç›®å½•")
    parser.add_argument("-r", "--results", type=int, default=20, help="æœç´¢ç»“æœæ•°é‡")
    parser.add_argument("-d", "--downloads", type=int, default=10, help="ä¸‹è½½æ•°é‡é™åˆ¶")
    parser.add_argument("--visible", action="store_true", help="æ˜¾ç¤ºæµè§ˆå™¨çª—å£")
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    
    logger = logging.getLogger(__name__)
    
    try:
        with GoogleFetcher(
            output_dir=args.output,
            results_limit=args.results,
            download_limit=args.downloads,
            headless=not args.visible,
            logger=logger
        ) as fetcher:
            logger.info(f"ğŸš€ å¼€å§‹æœç´¢: {args.query}")
            result = fetcher.fetch_and_download(args.query)
            
            logger.info("\n" + "="*60)
            logger.info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
            logger.info("="*60)
            logger.info(f"æœç´¢ç»“æœ: {result['results_found']}")
            logger.info(f"ä¸‹è½½æ–‡ä»¶: {result['files_downloaded']}")
            
            stats = fetcher.get_stats()
            logger.info(f"æ€»ä¸‹è½½é‡: {stats['total_size'] / (1024*1024):.2f} MB")
            logger.info(f"ä¾›åº”å•†åˆ†å¸ƒ: {dict(stats['vendor_downloads'])}")
            logger.info("="*60)
    
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
