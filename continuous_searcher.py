#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨åŒ–æŒç»­æœç´¢ä¸‹è½½ç³»ç»Ÿ
æ•´åˆ ChatGPT å…³é”®è¯ç”Ÿæˆ + DuckDuckGo æœç´¢ + è‡ªåŠ¨ä¸‹è½½

ä½œè€…: AI åŠ©æ‰‹
ç‰ˆæœ¬: 1.0.0
æ—¥æœŸ: 2026-01-26
Python: 3.10+
"""

import argparse
import json
import logging
import sys
import time
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set

from chatgpt_keyword_generator import GeminiKeywordGenerator
from keyword_manager import KeywordManager

# å°è¯•å¯¼å…¥ ddg_fetcher_browser æˆ– ddg_fetcherï¼ˆä¼˜å…ˆï¼‰æˆ– Google Fetcher
try:
    from ddg_fetcher_browser import DDGFetcher
    USE_BROWSER = True
    USE_GOOGLE = False
except ImportError:
    try:
        from ddg_fetcher import DDGFetcher
        USE_BROWSER = False
        USE_GOOGLE = False
    except ImportError:
        from google_fetcher import GoogleFetcher
        USE_GOOGLE = True
        USE_BROWSER = False


class ContinuousSearcher:
    """æŒç»­æœç´¢ä¸‹è½½å™¨"""
    
    def __init__(
        self,
        output_dir: Path,
        keyword_db_path: Path,
        config: Optional[Dict] = None,
        logger: Optional[logging.Logger] = None
    ):
        """
        åˆå§‹åŒ–æŒç»­æœç´¢å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            keyword_db_path: å…³é”®è¯æ•°æ®åº“è·¯å¾„
            config: é…ç½®å­—å…¸
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logger or logging.getLogger(__name__)
        self.config = config or self._default_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.keyword_manager = KeywordManager(keyword_db_path, self.logger)
        self.chatgpt_generator = None
        self.google_fetcher = None  # Google æœç´¢å™¨
        self.ddg_fetcher = None     # DuckDuckGo æœç´¢å™¨ï¼ˆå¤‡ç”¨ï¼‰
        self.ddgs = None            # DuckDuckGo ç®€å•æœç´¢ï¼ˆAPIï¼‰
        
        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        self.current_round = 0
        self.total_files_downloaded = 0
        self.total_size_downloaded = 0
        
        # çŠ¶æ€æ–‡ä»¶
        self.state_file = self.output_dir / "continuous_search_state.json"
        self._load_state()
    
    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            # ChatGPT é…ç½®
            "chatgpt_headless": False,  # é¦–æ¬¡è¿è¡Œå»ºè®® Falseï¼Œä»¥ä¾¿ç™»å½•
            "keywords_per_round": 10,   # æ¯è½®ç”Ÿæˆçš„å…³é”®è¯æ•°
            "focus_areas": [            # é‡ç‚¹å…³æ³¨é¢†åŸŸ
                "DC-DC converter",
                "buck converter",
                "boost converter",
                "flyback",
                "automotive power",
                "high efficiency",
            ],
            
            # æœç´¢é…ç½®
            "results_per_keyword": 20,  # æ¯ä¸ªå…³é”®è¯çš„æœç´¢ç»“æœæ•°
            "max_downloads_per_keyword": 10,  # æ¯ä¸ªå…³é”®è¯æœ€å¤šä¸‹è½½æ•°
            
            # å¾ªç¯æ§åˆ¶
            "max_rounds": 100,          # æœ€å¤§è½®æ•°ï¼ˆ0=æ— é™ï¼‰
            "min_files_per_round": 5,   # æ¯è½®æœ€å°‘ä¸‹è½½æ–‡ä»¶æ•°ï¼ˆä½äºæ­¤å€¼æå‰ç»“æŸï¼‰
            "round_interval": 300,      # è½®æ¬¡é—´éš”ï¼ˆç§’ï¼‰
            
            # ä¸‹è½½æ§åˆ¶
            "total_size_limit_gb": 50,  # æ€»ä¸‹è½½å¤§å°é™åˆ¶ï¼ˆGBï¼‰
            "total_files_limit": 5000,  # æ€»æ–‡ä»¶æ•°é™åˆ¶
            
            # å…¶ä»–
            "save_state_interval": 5,   # ä¿å­˜çŠ¶æ€é—´éš”ï¼ˆè½®æ•°ï¼‰
        }
    
    def _load_state(self):
        """åŠ è½½è¿è¡ŒçŠ¶æ€"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                    self.current_round = state.get("current_round", 0)
                    self.total_files_downloaded = state.get("total_files_downloaded", 0)
                    self.total_size_downloaded = state.get("total_size_downloaded", 0)
                    self.logger.info(f"ğŸ“¥ åŠ è½½è¿è¡ŒçŠ¶æ€: ç¬¬ {self.current_round} è½®")
            except Exception as e:
                self.logger.error(f"âŒ åŠ è½½çŠ¶æ€å¤±è´¥: {e}")
    
    def _save_state(self):
        """ä¿å­˜è¿è¡ŒçŠ¶æ€"""
        try:
            state = {
                "current_round": self.current_round,
                "total_files_downloaded": self.total_files_downloaded,
                "total_size_downloaded": self.total_size_downloaded,
                "last_updated": datetime.now().isoformat()
            }
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, indent=2)
            self.logger.debug(f"ğŸ’¾ ä¿å­˜è¿è¡ŒçŠ¶æ€")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")
    
    def _initialize_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        self.logger.info("ğŸ”§ åˆå§‹åŒ–ç»„ä»¶...")
        
        # åˆå§‹åŒ– Gemini ç”Ÿæˆå™¨
        self.chatgpt_generator = GeminiKeywordGenerator(
            logger=self.logger,
            headless=self.config["chatgpt_headless"]
        )
        self.chatgpt_generator.start()
        
        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        if not self.chatgpt_generator.check_login_status():
            self.logger.error("âŒ Gemini æœªç™»å½•ï¼Œæ— æ³•ç»§ç»­")
            return False
        
        self.logger.info("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
        return True
    
    def _cleanup_components(self):
        """æ¸…ç†ç»„ä»¶"""
        if self.chatgpt_generator:
            self.chatgpt_generator.stop()
            self.chatgpt_generator = None
        
        if self.google_fetcher:
            self.google_fetcher.close()
            self.google_fetcher = None
        
        if self.ddg_fetcher:
            if hasattr(self.ddg_fetcher, 'close'):
                self.ddg_fetcher.close()
            self.ddg_fetcher = None
    
    def _generate_keywords(self) -> List[str]:
        """ç”Ÿæˆæ–°çš„å…³é”®è¯"""
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ¯ ç¬¬ {self.current_round + 1} è½®: ç”Ÿæˆå…³é”®è¯")
        self.logger.info(f"{'='*60}\n")
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡
        context = self._prepare_context()
        
        # ç”Ÿæˆå…³é”®è¯
        keywords = self.chatgpt_generator.generate_keywords(
            context=context,
            num_keywords=self.config["keywords_per_round"],
            focus_areas=self.config["focus_areas"]
        )
        
        if not keywords:
            self.logger.warning("âš ï¸  æœªèƒ½ç”Ÿæˆå…³é”®è¯")
            return []
        
        # è¿‡æ»¤å…³é”®è¯ï¼šä¼˜å…ˆæ–°çš„ï¼Œå…è®¸é‡ç”¨æ—§çš„æœ‰æ•ˆå…³é”®è¯
        new_keywords = self.keyword_manager.filter_new_keywords(
            keywords, 
            allow_reuse_days=self.config.get("keyword_reuse_days", 7)
        )
        
        if not new_keywords:
            self.logger.warning("âš ï¸  æ— æ–°å…³é”®è¯ï¼Œå°†é‡ç”¨æ‰€æœ‰ç”Ÿæˆçš„å…³é”®è¯")
            new_keywords = keywords  # å…¨éƒ¨ä½¿ç”¨
        
        self.logger.info(f"âœ… ç”Ÿæˆ {len(keywords)} ä¸ªå…³é”®è¯ï¼Œ{len(new_keywords)} ä¸ªå¯ç”¨")
        
        return new_keywords
    
    def _prepare_context(self) -> Dict:
        """å‡†å¤‡ ChatGPT çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        stats = self.keyword_manager.get_statistics()
        recent_keywords = self.keyword_manager.get_recent_keywords(limit=10)
        
        # åˆ†æå·²ä¸‹è½½æ–‡ä»¶
        vendors_found = self._analyze_downloaded_vendors()
        recent_topics = self._extract_recent_topics()
        
        context = {
            "downloaded_count": self.total_files_downloaded,
            "vendors": list(vendors_found),
            "used_keywords": recent_keywords,
            "recent_topics": recent_topics,
            "current_round": self.current_round,
        }
        
        return context
    
    def _analyze_downloaded_vendors(self) -> Set[str]:
        """åˆ†æå·²ä¸‹è½½çš„ä¾›åº”å•†"""
        vendors = set()
        for vendor_dir in self.output_dir.iterdir():
            if vendor_dir.is_dir() and not vendor_dir.name.startswith(('_', '.')):
                vendors.add(vendor_dir.name)
        return vendors
    
    def _extract_recent_topics(self) -> List[str]:
        """æå–æœ€è¿‘çš„ä¸»é¢˜ï¼ˆä»å…³é”®è¯ä¸­ï¼‰"""
        recent = self.keyword_manager.get_recent_keywords(limit=20)
        
        # ç®€å•æå–ä¸»è¦è¯æ±‡
        topics = set()
        for kw in recent:
            words = kw.lower().split()
            for word in words:
                if len(word) > 4 and word not in ["datasheet", "guide", "note"]:
                    topics.add(word)
                    if len(topics) >= 10:
                        break
            if len(topics) >= 10:
                break
        
        return list(topics)
    
    def _search_and_download(self, keywords: List[str]) -> Dict:
        """æœç´¢å¹¶ä¸‹è½½"""
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ” ç¬¬ {self.current_round + 1} è½®: æœç´¢ä¸ä¸‹è½½")
        self.logger.info(f"{'='*60}\n")
        
        round_stats = {
            "files_downloaded": 0,
            "total_size": 0,
            "keyword_results": {}
        }
        
        for i, keyword in enumerate(keywords, 1):
            self.logger.info(f"\n--- å…³é”®è¯ {i}/{len(keywords)}: {keyword} ---")
            
            # æœç´¢å¹¶ä¸‹è½½
            try:
                result = self._process_single_keyword(keyword)
                
                # æ›´æ–°ç»Ÿè®¡
                files_found = result["files_downloaded"]
                size = result["total_size"]
                
                round_stats["files_downloaded"] += files_found
                round_stats["total_size"] += size
                round_stats["keyword_results"][keyword] = result
                
                # æ›´æ–°å…³é”®è¯ç®¡ç†å™¨
                self.keyword_manager.add_keyword(keyword, files_found, size)
                
                self.logger.info(f"âœ… {keyword}: {files_found} æ–‡ä»¶, {size / (1024*1024):.2f} MB")
                
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†å…³é”®è¯å¤±è´¥: {e}")
                self.keyword_manager.add_keyword(keyword, 0, 0)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é™åˆ¶
            if self._check_limits():
                self.logger.warning("âš ï¸  è¾¾åˆ°ä¸‹è½½é™åˆ¶ï¼Œåœæ­¢æœ¬è½®")
                break
        
        return round_stats
    
    def _process_single_keyword(self, keyword: str) -> Dict:
        """å¤„ç†å•ä¸ªå…³é”®è¯çš„æœç´¢ä¸‹è½½"""
        result = {
            "files_downloaded": 0,
            "total_size": 0,
            "urls_found": 0
        }
        
        try:
            # åˆå§‹åŒ–æœç´¢å™¨ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
            if USE_GOOGLE:
                if not self.google_fetcher:
                    self.logger.info("ğŸŒ åˆå§‹åŒ– Google æœç´¢å™¨...")
                    self.google_fetcher = GoogleFetcher(
                        output_dir=self.output_dir,
                        results_limit=self.config.get("results_per_keyword", 20),
                        download_limit=self.config.get("max_downloads_per_keyword", 10),
                        domain_whitelist=self.config.get("domain_whitelist"),
                        logger=self.logger,
                        headless=self.config.get("headless", True)
                    )
                
                # ä½¿ç”¨ Google æœç´¢
                self.logger.info(f"ğŸ” ä½¿ç”¨ Google æœç´¢: {keyword}")
                fetch_result = self.google_fetcher.fetch_and_download(keyword)
                
                result["files_downloaded"] = fetch_result.get("files_downloaded", 0)
                result["urls_found"] = fetch_result.get("results_found", 0)
                
                # è·å–å®é™…ä¸‹è½½å¤§å°
                stats = self.google_fetcher.get_stats()
                result["total_size"] = stats.get("total_size", 0)
                
            else:
                # ä½¿ç”¨ DuckDuckGo æœç´¢ï¼ˆå¤‡ç”¨ï¼‰
                self.logger.info(f"ğŸ” ä½¿ç”¨ DuckDuckGo æœç´¢: {keyword}")
                
                if not self.ddg_fetcher:
                    # DuckDuckGo ä½¿ç”¨ç®€å•çš„ API æ–¹å¼ï¼ˆéæµè§ˆå™¨ï¼‰
                    # ç›´æ¥è°ƒç”¨ ddgs è¿›è¡Œæœç´¢
                    from ddgs import DDGS
                    self.ddgs = DDGS()
                
                # ä½¿ç”¨ DDGS æœç´¢
                try:
                    results = list(self.ddgs.text(keyword, max_results=self.config.get("results_per_keyword", 20)))
                    result["urls_found"] = len(results)
                    
                    # è¿‡æ»¤ PDF é“¾æ¥
                    pdf_urls = [r['href'] for r in results if r['href'].lower().endswith('.pdf')]
                    
                    # ä¸‹è½½ PDF
                    downloads = 0
                    total_size = 0
                    max_downloads = self.config.get("max_downloads_per_keyword", 10)
                    
                    for url in pdf_urls[:max_downloads]:
                        try:
                            # ç®€å•ä¸‹è½½
                            import requests
                            response = requests.get(url, timeout=30, stream=True)
                            if response.status_code == 200:
                                filename = Path(url).name or "download.pdf"
                                filepath = self.output_dir / filename
                                
                                with open(filepath, 'wb') as f:
                                    for chunk in response.iter_content(chunk_size=8192):
                                        f.write(chunk)
                                
                                file_size = filepath.stat().st_size
                                total_size += file_size
                                downloads += 1
                                self.logger.info(f"âœ… å·²ä¸‹è½½: {filename} ({file_size/1024/1024:.2f} MB)")
                        except Exception as e:
                            self.logger.debug(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
                            continue
                    
                    result["files_downloaded"] = downloads
                    result["total_size"] = total_size
                    
                except Exception as e:
                    self.logger.error(f"DuckDuckGo æœç´¢å¤±è´¥: {e}")
                    result["files_downloaded"] = 0
            
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†å…³é”®è¯å¤±è´¥: {e}", exc_info=True)
        
        return result
    
    def _check_limits(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é™åˆ¶"""
        size_limit_bytes = self.config["total_size_limit_gb"] * 1024 * 1024 * 1024
        
        if self.total_size_downloaded >= size_limit_bytes:
            self.logger.warning(f"âš ï¸  è¾¾åˆ°æ€»å¤§å°é™åˆ¶: {self.config['total_size_limit_gb']} GB")
            return True
        
        if self.total_files_downloaded >= self.config["total_files_limit"]:
            self.logger.warning(f"âš ï¸  è¾¾åˆ°æ€»æ–‡ä»¶æ•°é™åˆ¶: {self.config['total_files_limit']}")
            return True
        
        return False
    
    def _print_round_summary(self, round_stats: Dict):
        """æ‰“å°æœ¬è½®æ€»ç»“"""
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ“Š ç¬¬ {self.current_round + 1} è½®æ€»ç»“")
        self.logger.info(f"{'='*60}")
        self.logger.info(f"æœ¬è½®ä¸‹è½½æ–‡ä»¶: {round_stats['files_downloaded']}")
        self.logger.info(f"æœ¬è½®ä¸‹è½½å¤§å°: {round_stats['total_size'] / (1024*1024):.2f} MB")
        self.logger.info(f"ç´¯è®¡ä¸‹è½½æ–‡ä»¶: {self.total_files_downloaded}")
        self.logger.info(f"ç´¯è®¡ä¸‹è½½å¤§å°: {self.total_size_downloaded / (1024*1024*1024):.2f} GB")
        self.logger.info(f"{'='*60}\n")
    
    def run(self):
        """è¿è¡ŒæŒç»­æœç´¢"""
        self.logger.info("\n" + "ğŸš€ å¯åŠ¨æŒç»­æœç´¢ä¸‹è½½ç³»ç»Ÿ".center(60, "="))
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        self.logger.info(f"å…³é”®è¯æ•°æ®åº“: {self.keyword_manager.db_path}")
        self.logger.info("")
        
        # åˆå§‹åŒ–ç»„ä»¶
        if not self._initialize_components():
            return
        
        try:
            self.is_running = True
            
            while self.is_running:
                # æ£€æŸ¥è½®æ•°é™åˆ¶
                if self.config["max_rounds"] > 0 and self.current_round >= self.config["max_rounds"]:
                    self.logger.info(f"âœ… è¾¾åˆ°æœ€å¤§è½®æ•°: {self.config['max_rounds']}")
                    break
                
                # æ£€æŸ¥å…¶ä»–é™åˆ¶
                if self._check_limits():
                    self.logger.info("âœ… è¾¾åˆ°ä¸‹è½½é™åˆ¶")
                    break
                
                # ç”Ÿæˆå…³é”®è¯
                keywords = self._generate_keywords()
                if not keywords:
                    self.logger.warning("âš ï¸  æ²¡æœ‰æ–°å…³é”®è¯ï¼Œç­‰å¾…ä¸‹ä¸€è½®...")
                    time.sleep(60)
                    continue
                
                # æœç´¢å¹¶ä¸‹è½½
                round_stats = self._search_and_download(keywords)
                
                # æ›´æ–°ç»Ÿè®¡
                self.total_files_downloaded += round_stats["files_downloaded"]
                self.total_size_downloaded += round_stats["total_size"]
                
                # æ‰“å°æ€»ç»“
                self._print_round_summary(round_stats)
                
                # æ£€æŸ¥æœ€å°‘æ–‡ä»¶æ•°
                if round_stats["files_downloaded"] < self.config["min_files_per_round"]:
                    self.logger.warning(
                        f"âš ï¸  æœ¬è½®ä¸‹è½½æ–‡ä»¶æ•°è¿‡å°‘ ({round_stats['files_downloaded']} < "
                        f"{self.config['min_files_per_round']})ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ç­–ç•¥"
                    )
                
                # æ›´æ–°è½®æ•°
                self.current_round += 1
                
                # å®šæœŸä¿å­˜çŠ¶æ€
                if self.current_round % self.config["save_state_interval"] == 0:
                    self._save_state()
                
                # æ‰“å°å…³é”®è¯ç»Ÿè®¡
                if self.current_round % 5 == 0:
                    self.keyword_manager.print_statistics()
                
                # ç­‰å¾…ä¸‹ä¸€è½®
                if self.is_running:
                    interval = self.config["round_interval"]
                    self.logger.info(f"â¸ï¸  ç­‰å¾… {interval} ç§’åå¼€å§‹ä¸‹ä¸€è½®...\n")
                    time.sleep(interval)
            
            # æœ€ç»ˆç»Ÿè®¡
            self.logger.info("\n" + "ğŸ‰ æœç´¢å®Œæˆ".center(60, "="))
            self.logger.info(f"æ€»è½®æ•°: {self.current_round}")
            self.logger.info(f"æ€»æ–‡ä»¶æ•°: {self.total_files_downloaded}")
            self.logger.info(f"æ€»å¤§å°: {self.total_size_downloaded / (1024*1024*1024):.2f} GB")
            self.keyword_manager.print_statistics()
            
        except KeyboardInterrupt:
            self.logger.info("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜çŠ¶æ€...")
            self._save_state()
            
        except Exception as e:
            self.logger.error(f"âŒ è¿è¡Œå‡ºé”™: {e}", exc_info=True)
            
        finally:
            self.is_running = False
            self._save_state()
            self._cleanup_components()
            self.logger.info("âœ… å·²æ¸…ç†èµ„æº")


def setup_logging(debug: bool = False) -> logging.Logger:
    """é…ç½®æ—¥å¿—"""
    level = logging.DEBUG if debug else logging.INFO
    
    logging.basicConfig(
        level=level,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[
            logging.StreamHandler(sys.stdout),
        ]
    )
    
    return logging.getLogger("continuous_searcher")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="DC-DC èµ„æ–™æŒç»­æœç´¢ä¸‹è½½ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        "-o", "--output",
        type=Path,
        default=Path("downloads_continuous"),
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: downloads_continuousï¼‰"
    )
    
    parser.add_argument(
        "-k", "--keywords-db",
        type=Path,
        default=Path("keywords.json"),
        help="å…³é”®è¯æ•°æ®åº“æ–‡ä»¶ï¼ˆé»˜è®¤: keywords.jsonï¼‰"
    )
    
    parser.add_argument(
        "-c", "--config",
        type=Path,
        help="é…ç½®æ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰"
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¯ç”¨è°ƒè¯•æ¨¡å¼"
    )
    
    parser.add_argument(
        "--rounds",
        type=int,
        default=0,
        help="æœ€å¤§è½®æ•°ï¼ˆ0=æ— é™ï¼Œé»˜è®¤: 0ï¼‰"
    )
    
    parser.add_argument(
        "--keywords-per-round",
        type=int,
        default=10,
        help="æ¯è½®ç”Ÿæˆå…³é”®è¯æ•°ï¼ˆé»˜è®¤: 10ï¼‰"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(args.debug)
    
    # åŠ è½½é…ç½®
    config = None
    if args.config and args.config.exists():
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"ğŸ“„ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            sys.exit(1)
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°
    if config is None:
        config = {}
    
    if args.rounds > 0:
        config["max_rounds"] = args.rounds
    
    if args.keywords_per_round > 0:
        config["keywords_per_round"] = args.keywords_per_round
    
    # åˆ›å»ºå¹¶è¿è¡Œæœç´¢å™¨
    searcher = ContinuousSearcher(
        output_dir=args.output,
        keyword_db_path=args.keywords_db,
        config=config,
        logger=logger
    )
    
    searcher.run()


if __name__ == "__main__":
    main()
