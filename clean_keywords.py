#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸…ç† explored_keywords.json ä¸­çš„è„æ•°æ®
ä½¿ç”¨ä¿®å¤åçš„ parse_keywords() å‡½æ•°é‡æ–°è¿‡æ»¤
"""

import json
from pathlib import Path
from datetime import datetime

# å¯¼å…¥ä¿®å¤åçš„ parse_keywords å‡½æ•°
from keyword_explorer import parse_keywords

def clean_explored_keywords():
    """æ¸…ç†æ¢ç´¢å…³é”®è¯æ–‡ä»¶ä¸­çš„è„æ•°æ®"""
    file_path = Path("downloads_continuous/explored_keywords.json")
    
    if not file_path.exists():
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # è¯»å–åŸå§‹æ•°æ®
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    original_keywords = data.get('keywords', [])
    print(f"ğŸ“‹ åŸå§‹å…³é”®è¯æ•°é‡: {len(original_keywords)}")
    
    # æ˜¾ç¤ºå‰5ä¸ªå’Œå5ä¸ª
    print(f"\nå‰5ä¸ª: {original_keywords[:5]}")
    print(f"å5ä¸ª: {original_keywords[-5:]}")
    
    # ä½¿ç”¨ parse_keywords æ¸…ç†æ¯ä¸€ä¸ªå…³é”®è¯
    # å› ä¸º parse_keywords æ¥å—æ•´ä¸ªå“åº”æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦é€ä¸ªéªŒè¯
    clean_keywords = []
    dirty_keywords = []
    
    for keyword in original_keywords:
        # ç›´æ¥æ£€æŸ¥é•¿åº¦ - è¶…è¿‡ 100 å­—ç¬¦çš„è‚¯å®šæœ‰é—®é¢˜
        if len(keyword) > 100:
            dirty_keywords.append(keyword)
            continue
            
        # å°†æ¯ä¸ªå…³é”®è¯å½“ä½œå•ç‹¬çš„å“åº”æ¥éªŒè¯
        result = parse_keywords(keyword)
        if result:  # å¦‚æœé€šè¿‡äº†è¿‡æ»¤
            clean_keywords.append(keyword)
        else:
            dirty_keywords.append(keyword)
    
    print(f"\nâœ… æ¸…ç†åå…³é”®è¯æ•°é‡: {len(clean_keywords)}")
    print(f"âŒ è¿‡æ»¤æ‰çš„è„æ•°æ®: {len(dirty_keywords)}")
    
    if dirty_keywords:
        print(f"\nè¢«è¿‡æ»¤çš„è„æ•°æ®:")
        for i, kw in enumerate(dirty_keywords[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            preview = kw[:100] + '...' if len(kw) > 100 else kw
            print(f"  {i}. {preview}")
        if len(dirty_keywords) > 10:
            print(f"  ... è¿˜æœ‰ {len(dirty_keywords) - 10} ä¸ª")
    
    # å¤‡ä»½åŸæ–‡ä»¶
    backup_path = file_path.with_suffix('.json.backup')
    if backup_path.exists():
        print(f"\nâš ï¸  å¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è¦†ç›–: {backup_path}")
    
    with open(backup_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"âœ… å·²å¤‡ä»½åˆ°: {backup_path}")
    
    # ä¿å­˜æ¸…ç†åçš„æ•°æ®
    cleaned_data = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'keywords': clean_keywords,
        'count': len(clean_keywords),
        'cleaned_from': len(original_keywords),
        'removed': len(dirty_keywords)
    }
    
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(cleaned_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æ¸…ç†å®Œæˆ!")
    print(f"   åŸå§‹: {len(original_keywords)} ä¸ª")
    print(f"   æ¸…ç†å: {len(clean_keywords)} ä¸ª")
    print(f"   ç§»é™¤: {len(dirty_keywords)} ä¸ª")
    print(f"   ä¿å­˜åˆ°: {file_path}")

if __name__ == '__main__':
    clean_explored_keywords()
