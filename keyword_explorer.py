#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…³é”®è¯æ¢ç´¢å™¨ - ä½¿ç”¨ Gemini ç”Ÿæˆæ–°çš„æœç´¢å…³é”®è¯å’Œå‚å•†ç½‘ç«™
"""

import json
import logging
import time
from pathlib import Path
from datetime import datetime
from chatgpt_keyword_generator import GeminiKeywordGenerator

# é…ç½®
OUTPUT_DIR = Path("downloads_continuous")
KEYWORDS_FILE = OUTPUT_DIR / "explored_keywords.json"
VENDORS_FILE = OUTPUT_DIR / "explored_vendors.json"

# Gemini æç¤ºè¯
KEYWORD_PROMPT = """You are an automotive EMC/EMI and reliability testing expert. Generate 50 search keywords for finding PDF documents related to ISO 7637 transient immunity, EMC/EMI testing, and automotive electronics reliability qualification (especially for bidirectional buck-boost converters).

Requirements:
1. Cover standards: ISO 7637-1/2/3, ISO 16750, CISPR 25, IEC 61000-4-x, IEC 62132, SAE J1113
2. Include test types: transient immunity, conducted emission, radiated emission, ESD, surge, BCI, DPI
3. Include protection design: TVS diode, EMI filter, common mode choke, ferrite bead, clamping circuit
4. Include reliability: AEC-Q100, HTOL, HTSL, thermal cycling, vibration, HALT, HASS, FMEA
5. Include applications: automotive ECU, bidirectional buck-boost, DC-DC converter EMC, 12V/24V/48V system
6. Include document types: application note, design guide, test report, compliance guide, white paper
7. Keep keywords general, suitable for web searches, avoid specific part numbers

FORMAT RULES (CRITICAL):
- Output ONLY keywords, ONE per line
- Each keyword: 2-6 words
- NO numbers, NO bullets, NO explanations, NO questions
- NO UI text like "Here are the keywords" or "Would you like"
- Just plain text keywords, each on a new line

Example output:
ISO 7637 transient immunity test
automotive EMC compliance guide
TVS diode selection automotive
CISPR 25 radiated emission measurement
load dump protection design
ESD protection CAN bus automotive
AEC-Q100 qualification test
automotive EMI filter design
bulk current injection BCI test
ISO 16750 electrical test automotive

Now generate 50 keywords following this EXACT format:"""

VENDOR_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ±½è½¦ç”µå­EMC/EMIå’Œå¯é æ€§æµ‹è¯•é¢†åŸŸçš„ä¸“å®¶ã€‚è¯·åˆ—å‡ºä»¥ä¸‹ç±»åˆ«çš„å…¨çƒä¸»è¦å‚å•†å’Œæœºæ„çš„å®˜æ–¹ç½‘ç«™åŸŸåï¼š

å·²çŸ¥å‚å•†åŒ…æ‹¬ä½†ä¸é™äºï¼š
- Littelfuse (littelfuse.com) - TVSä¿æŠ¤å™¨ä»¶
- Bourns (bourns.com) - ç¬æ€ä¿æŠ¤
- Nexperia (nexperia.com) - ESDä¿æŠ¤
- Murata (murata.com) - EMIæ»¤æ³¢å™¨
- TDK (tdk.com) - EMCå…ƒå™¨ä»¶

è¯·è¡¥å……æ›´å¤šå‚å•†ï¼ˆè‡³å°‘30ä¸ªï¼‰ï¼Œç‰¹åˆ«æ˜¯ï¼š
1. TVS/ESDä¿æŠ¤å™¨ä»¶å‚å•†ï¼ˆå¦‚Vishay, Semtech, ProTekç­‰ï¼‰
2. EMIæ»¤æ³¢å™¨/ç£æ€§å…ƒä»¶å‚å•†ï¼ˆå¦‚WÃ¼rth, Laird, Fair-Riteç­‰ï¼‰
3. æ±½è½¦çº§åŠå¯¼ä½“å‚å•†ï¼ˆå¦‚TI, Infineon, NXP, ST, onsemiç­‰ï¼‰
4. EMCæµ‹è¯•è®¾å¤‡å‚å•†ï¼ˆå¦‚Rohde & Schwarz, Keysight, TESEQç­‰ï¼‰
5. EMCè®¤è¯/æµ‹è¯•å®éªŒå®¤ï¼ˆå¦‚TÃœV, DEKRA, Bureau Veritasç­‰ï¼‰
6. è¿æ¥å™¨/çº¿æŸEMCå±è”½å‚å•†

æ ¼å¼è¦æ±‚ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š
- å¿…é¡»æ¯è¡Œä¸€ä¸ªå‚å•†
- æ ¼å¼ï¼šå‚å•†åç§°: åŸŸå
- åŸŸååªåŒ…å«ä¸»åŸŸåï¼Œä¸è¦wwwã€httpç­‰
- åªè¾“å‡ºå‚å•†åˆ—è¡¨ï¼Œä¸è¦åºå·ã€ä¸è¦è§£é‡Šã€ä¸è¦é—®é—®é¢˜

ç¤ºä¾‹æ ¼å¼ï¼š
Littelfuse: littelfuse.com
Bourns: bourns.com
Rohde Schwarz: rohde-schwarz.com
Murata: murata.com

ç°åœ¨è¯·æŒ‰ç…§ä¸Šè¿°æ ¼å¼ç”Ÿæˆå‚å•†åˆ—è¡¨ï¼š"""


def setup_logger():
    """è®¾ç½®æ—¥å¿—"""
    logger = logging.getLogger('KeywordExplorer')
    logger.setLevel(logging.INFO)
    
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    
    return logger


def parse_keywords(response_text: str) -> list:
    """è§£æ Gemini è¿”å›çš„å…³é”®è¯"""
    keywords = []
    
    # === ç¬¬ä¸€æ­¥ï¼šæ¸…ç†å’ŒéªŒè¯å“åº” ===
    # æ£€æŸ¥æ˜¯å¦åŒ…å«Gemini UIæ–‡å­—ï¼ˆä¹±ç æˆ–åŸæ–‡ï¼‰
    ui_patterns = [
        'è®¤è¯† gemini', 'geminiï¼šä½ çš„', 'ç§äºº', 'ai åŠ©ç†', 'ai assistant',
        'google gemini', 'get started', 'welcome to', 'æ¬¢è¿ä½¿ç”¨',
        'sign in', 'ç™»å½•', 'account', 'è´¦å·', 'settings', 'è®¾ç½®',
        # ä¹±ç æ¨¡å¼ï¼ˆæ£€æµ‹ç‰¹å®šå­—ç¬¦ï¼‰
        'ç’ã‚ˆç˜‘', 'é”â•ƒæ‚Š'
    ]
    
    response_lower = response_text.lower()
    if any(pattern in response_lower for pattern in ui_patterns):
        # å°è¯•æ¸…ç†è¿™äº›å†…å®¹
        for pattern in ui_patterns:
            if pattern in response_lower:
                # æŸ¥æ‰¾å¹¶ç§»é™¤åŒ…å«è¯¥æ¨¡å¼çš„æ•´è¡Œ
                lines = response_text.split('\n')
                cleaned_lines = []
                for line in lines:
                    if pattern not in line.lower():
                        cleaned_lines.append(line)
                response_text = '\n'.join(cleaned_lines)
    
    # === ç¬¬äºŒæ­¥ï¼šæŒ‰è¡Œåˆ†å‰²å’Œé¢„å¤„ç† ===
    lines = response_text.strip().split('\n')
    
    # å¯¹æ¯ä¸€è¡Œéƒ½æ£€æŸ¥é•¿åº¦ï¼Œè¶…é•¿çš„è¿›è¡Œåˆ†å‰²
    processed_lines = []
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # å¦‚æœè¿™è¡Œè¶…è¿‡100å­—ç¬¦ï¼Œå°è¯•æ™ºèƒ½åˆ†å‰²
        if len(line) > 100:
            # å°è¯•æŒ‰å¸¸è§åˆ†éš”ç¬¦åˆ†å‰²
            split_done = False
            for separator in [', ', '; ', ' / ', '  ']:
                if separator in line:
                    parts = [s.strip() for s in line.split(separator) if s.strip()]
                    # åªæœ‰å½“åˆ†å‰²åæ¯ä¸ªéƒ¨åˆ†éƒ½ä¸å¤ªé•¿æ—¶æ‰æ¥å—
                    if all(len(p) <= 100 for p in parts):
                        processed_lines.extend(parts)
                        split_done = True
                        break
            
            # å¦‚æœåˆ†éš”ç¬¦åˆ†å‰²å¤±è´¥ï¼ŒæŒ‰ç©ºæ ¼åˆ†è¯é‡ç»„ï¼ˆæ¯3è¯ä¸€ç»„ï¼‰
            if not split_done:
                words = line.split()
                i = 0
                while i < len(words):
                    # å›ºå®š3ä¸ªè¯ä¸€ç»„
                    phrase = ' '.join(words[i:i+3])
                    if len(phrase) >= 8:  # è‡³å°‘8ä¸ªå­—ç¬¦
                        processed_lines.append(phrase)
                    i += 3
                # è·³è¿‡è¿™ä¸ªè¶…é•¿è¡Œçš„åŸå§‹æ–‡æœ¬
                continue
        
        # æ­£å¸¸é•¿åº¦çš„è¡Œç›´æ¥æ·»åŠ 
        processed_lines.append(line)
    
    lines = processed_lines
    
    # === ç¬¬ä¸‰æ­¥ï¼šé€è¡Œè§£æå’Œè¿‡æ»¤ ===
    for line in lines:
        line = line.strip()
        if not line or len(line) < 5:
            continue
        
        # ç§»é™¤åºå·å’Œæ ‡è®°
        line = line.lstrip('0123456789.-*â€¢â†’ \t')
        line = line.strip('`*_?ï¼Ÿ:ï¼š')
        
        # å¼ºåˆ¶ASCIIæ£€æŸ¥ï¼ˆæŠ€æœ¯å…³é”®è¯åº”è¯¥æ˜¯è‹±æ–‡ï¼‰
        # å…è®¸å°‘é‡éASCIIå­—ç¬¦ï¼ˆå¦‚ç ´æŠ˜å·ï¼‰ï¼Œä½†ä¸»ä½“å¿…é¡»æ˜¯ASCII
        ascii_ratio = sum(1 for c in line if ord(c) < 128) / len(line)
        if ascii_ratio < 0.8:  # è‡³å°‘80%æ˜¯ASCIIå­—ç¬¦
            continue
        
        # è¿‡æ»¤é—®å¥å’Œè¯´æ˜æ–‡å­—
        skip_phrases = [
            'would you', 'do you', 'should i', 'can i', 'here is', 'here are',
            'following', 'example', 'format:', 'requirement', 'generate',
            'éœ€è¦', 'æ˜¯å¦', 'ä»¥ä¸‹', 'ç¤ºä¾‹', 'æ ¼å¼', 'ç”Ÿæˆ', 'å…³é”®è¯'
        ]
        if any(phrase in line.lower() for phrase in skip_phrases):
            continue
        
        # è¿‡æ»¤Geminiçš„è‡ªæˆ‘ä»‹ç»
        if any(word in line.lower() for word in ['gemini', 'google', 'assistant']):
            continue
        
        # æŠ€æœ¯å…³é”®è¯éªŒè¯ï¼šå¿…é¡»åŒ…å«æŠ€æœ¯ç›¸å…³è¯æ±‡
        technical_words = [
            # EMC/EMI ç›¸å…³
            'emc', 'emi', 'esd', 'transient', 'immunity', 'emission',
            'conducted', 'radiated', 'susceptibility', 'interference',
            'suppression', 'filter', 'shielding', 'coupling', 'decoupling',
            # æ ‡å‡†ç›¸å…³
            'iso', 'cispr', 'iec', 'sae', 'aec', 'jedec',
            '7637', '16750', '61000', '62132', '61967', 'j1113',
            # ä¿æŠ¤å™¨ä»¶
            'tvs', 'varistor', 'clamp', 'suppressor', 'protection',
            'choke', 'ferrite', 'bead', 'capacitor', 'inductor',
            # å¯é æ€§æµ‹è¯•
            'reliability', 'qualification', 'htol', 'htsl', 'halt', 'hass',
            'thermal', 'cycling', 'vibration', 'humidity', 'fmea',
            'mission', 'profile', 'lifetime', 'stress',
            # æ±½è½¦ç”µå­
            'automotive', 'vehicle', 'ecu', 'cmix', 'module',
            'load dump', 'cold crank', 'pulse', 'surge', 'burst',
            # é€šç”¨æŠ€æœ¯è¯
            'power', 'voltage', 'current', 'converter', 'regulator',
            'design', 'test', 'measurement', 'compliance', 'standard',
            'pcb', 'layout', 'simulation', 'spice', 'model',
            'can', 'lin', 'bus', 'connector', 'harness',
            'buck', 'boost', 'dc-dc', 'switching', 'controller',
            'bci', 'dpi', 'tem', 'stripline', 'probe',
            'datasheet', 'application', 'guide', 'reference', 'note'
        ]
        has_technical = any(word in line.lower() for word in technical_words)
        
        # æˆ–è€…åŒ…å«å¸¸è§çš„æ–‡æ¡£ç±»å‹è¯
        doc_types = ['datasheet', 'application note', 'user guide', 'reference',
                     'design guide', 'test report', 'white paper', 'compliance guide',
                     'technical note', 'selection guide']
        has_doc_type = any(dtype in line.lower() for dtype in doc_types)
        
        # å¿…é¡»æœ‰æŠ€æœ¯è¯æˆ–æ–‡æ¡£ç±»å‹
        if not (has_technical or has_doc_type):
            continue
        
        # é•¿åº¦é™åˆ¶
        if 5 < len(line) < 100:
            keywords.append(line)
    
    return keywords


def parse_vendors(response_text: str) -> dict:
    """è§£æ Gemini è¿”å›çš„å‚å•†ä¿¡æ¯"""
    vendors = {}
    lines = response_text.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        if not line or ':' not in line:
            continue
        
        # åˆ†å‰²å‚å•†åå’ŒåŸŸå
        parts = line.split(':', 1)
        if len(parts) != 2:
            continue
        
        vendor_name = parts[0].strip()
        domain = parts[1].strip()
        
        # æ¸…ç†åŸŸåï¼ˆç§»é™¤ www, httpç­‰ï¼‰
        domain = domain.replace('http://', '').replace('https://', '')
        domain = domain.replace('www.', '')
        domain = domain.split('/')[0]  # åªä¿ç•™åŸŸåéƒ¨åˆ†
        
        if domain and '.' in domain:
            vendors[vendor_name] = domain
    
    return vendors


def explore_keywords(logger, headless=False):
    """æ¢ç´¢æ–°å…³é”®è¯"""
    logger.info("="*70)
    logger.info("å¼€å§‹æ¢ç´¢æ–°å…³é”®è¯...")
    logger.info("="*70)
    
    # è¯»å–å·²ä½¿ç”¨çš„å…³é”®è¯
    used_keywords = set()
    used_file = OUTPUT_DIR / "used_keywords.json"
    if used_file.exists():
        try:
            with open(used_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                used_keywords = set(data.get('used', []))
                logger.info(f"ğŸ“‹ å·²ä½¿ç”¨ {len(used_keywords)} ä¸ªå…³é”®è¯")
        except Exception as e:
            logger.warning(f"âš ï¸  è¯»å–å·²ä½¿ç”¨å…³é”®è¯å¤±è´¥: {e}")
    
    # åŠ¨æ€æ„å»ºæç¤ºè¯
    prompt = KEYWORD_PROMPT
    if used_keywords:
        # å–æœ€è¿‘ä½¿ç”¨çš„20ä¸ªä½œä¸ºç¤ºä¾‹
        recent_used = list(used_keywords)[-20:]
        used_text = ", ".join(recent_used[:10])  # åªæ˜¾ç¤ºå‰10ä¸ª
        prompt += f"\n\nâš ï¸ é¿å…é‡å¤è¿™äº›å·²ç”¨è¿‡çš„å…³é”®è¯: {used_text}"
        prompt += "\nè¯·ç”Ÿæˆå®Œå…¨ä¸åŒçš„æ–°å…³é”®è¯ï¼"
    
    generator = GeminiKeywordGenerator(logger=logger, headless=headless, response_timeout=120)
    
    try:
        # å¯åŠ¨æµè§ˆå™¨
        generator.start()
        
        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        if not generator.check_login_status():
            logger.error("âŒ æ— æ³•ç™»å½• Gemini")
            return []
        
        # å‘é€æç¤ºè¯
        response = generator.send_prompt(prompt)
        
        if not response:
            logger.error("æœªè·å¾—å“åº”")
            return []
        
        logger.info(f"\næ”¶åˆ°å“åº”:\n{response[:500]}...\n")
        
        # è§£æå…³é”®è¯
        new_keywords = parse_keywords(response)
        logger.info(f"è§£æå¾—åˆ° {len(new_keywords)} ä¸ªæ–°å…³é”®è¯")
        
        # è¿‡æ»¤æ‰å·²ä½¿ç”¨çš„
        if used_keywords:
            original_count = len(new_keywords)
            new_keywords = [k for k in new_keywords if k not in used_keywords]
            filtered = original_count - len(new_keywords)
            if filtered > 0:
                logger.info(f"ğŸ”„ è¿‡æ»¤æ‰ {filtered} ä¸ªé‡å¤å…³é”®è¯")
        
        # è¯»å–ç°æœ‰å…³é”®è¯åˆ—è¡¨
        existing_keywords = []
        if KEYWORDS_FILE.exists():
            try:
                with open(KEYWORDS_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    raw_existing = data.get('keywords', [])
                    # è¿‡æ»¤ç°æœ‰å…³é”®è¯ï¼šåªä¿ç•™é•¿åº¦åˆç†çš„ï¼ˆ10-100å­—ç¬¦ï¼‰
                    existing_keywords = [k for k in raw_existing if 10 <= len(k) <= 100]
                    removed_old = len(raw_existing) - len(existing_keywords)
                    if removed_old > 0:
                        logger.info(f"ğŸ§¹ æ¸…ç†æ‰ {removed_old} ä¸ªæ—§çš„æ— æ•ˆå…³é”®è¯")
            except Exception as e:
                logger.warning(f"è¯»å–ç°æœ‰å…³é”®è¯å¤±è´¥: {e}")
        
        # åˆå¹¶æ–°æ—§å…³é”®è¯ï¼ˆå»é‡ï¼‰
        all_keywords = existing_keywords + new_keywords
        # ä¿æŒé¡ºåºå¹¶å»é‡
        seen = set()
        unique_keywords = []
        for k in all_keywords:
            if k not in seen:
                seen.add(k)
                unique_keywords.append(k)
        
        # ä¿å­˜ç»“æœ
        data = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'keywords': unique_keywords,
            'count': len(unique_keywords),
            'new_added': len(new_keywords)
        }
        
        with open(KEYWORDS_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… å…³é”®è¯å·²ä¿å­˜åˆ°: {KEYWORDS_FILE}")
        logger.info(f"   æ€»è®¡: {len(unique_keywords)} ä¸ª (æ–°å¢: {len(new_keywords)} ä¸ª)")
        
        return new_keywords
        
    finally:
        generator.stop()


def explore_vendors(logger, headless=False):
    """æ¢ç´¢æ–°å‚å•†"""
    logger.info("="*70)
    logger.info("å¼€å§‹æ¢ç´¢æ–°å‚å•†...")
    logger.info("="*70)
    
    generator = GeminiKeywordGenerator(logger=logger, headless=headless, response_timeout=120)
    
    try:
        # å¯åŠ¨æµè§ˆå™¨
        generator.start()
        
        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        if not generator.check_login_status():
            logger.error("âŒ æ— æ³•ç™»å½• Gemini")
            return {}
        
        # å‘é€æç¤ºè¯
        response = generator.send_prompt(VENDOR_PROMPT)
        
        if not response:
            logger.error("æœªè·å¾—å“åº”")
            return {}
        
        logger.info(f"\næ”¶åˆ°å“åº”:\n{response[:500]}...\n")
        
        # è§£æå‚å•†
        vendors = parse_vendors(response)
        logger.info(f"è§£æå¾—åˆ° {len(vendors)} ä¸ªå‚å•†")
        
        # ä¿å­˜ç»“æœ
        data = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'vendors': vendors,
            'count': len(vendors)
        }
        
        with open(VENDORS_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… å‚å•†å·²ä¿å­˜åˆ°: {VENDORS_FILE}")
        
        return vendors
        
    finally:
        generator.stop()


def main():
    """ä¸»å‡½æ•°"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    logger = setup_logger()
    
    print("="*70)
    print("å…³é”®è¯å’Œå‚å•†æ¢ç´¢å™¨")
    print("ä½¿ç”¨ Google Gemini è‡ªåŠ¨ç”Ÿæˆæœç´¢å…³é”®è¯å’Œå‚å•†åˆ—è¡¨")
    print("="*70)
    print()
    
    # é€‰æ‹©æ¨¡å¼
    print("è¯·é€‰æ‹©æ¢ç´¢æ¨¡å¼:")
    print("1. æ¢ç´¢æ–°å…³é”®è¯ (æ™ºèƒ½é¿å¼€å·²ç”¨å…³é”®è¯)")
    print("2. æ¢ç´¢æ–°å‚å•†")
    print("3. å…¨éƒ¨æ¢ç´¢")
    print("4. æŒç»­æ¢ç´¢æ¨¡å¼ (æ— é™å¾ªç¯ç”Ÿæˆæ–°å…³é”®è¯)")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-4): ").strip()
    
    # æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
    headless_input = input("æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼? (y/nï¼Œé¦–æ¬¡ä½¿ç”¨å»ºè®® n): ").strip().lower()
    headless = headless_input == 'y'
    
    if choice == '4':
        # æŒç»­æ¢ç´¢æ¨¡å¼
        print("\n" + "="*70)
        print("ğŸ”„ æŒç»­æ¢ç´¢æ¨¡å¼")
        print("å°†æ¯éš” 5 åˆ†é’Ÿè‡ªåŠ¨ç”Ÿæˆæ–°å…³é”®è¯")
        print("æŒ‰ Ctrl+C åœæ­¢")
        print("="*70)
        
        cycle_count = 0
        while True:
            try:
                cycle_count += 1
                print(f"\n{'='*70}")
                print(f"ç¬¬ {cycle_count} è½®æ¢ç´¢")
                print(f"{'='*70}")
                
                keywords = explore_keywords(logger, headless)
                print(f"\nâœ… æœ¬è½®æ–°å¢ {len(keywords)} ä¸ªå…³é”®è¯")
                
                print("\nâ³ ç­‰å¾… 5 åˆ†é’Ÿåè¿›è¡Œä¸‹ä¸€è½®æ¢ç´¢...")
                print("   (åœ¨æ­¤æœŸé—´å¯ä»¥è¿è¡Œ start_vendor_batch*.bat)")
                time.sleep(300)  # 5åˆ†é’Ÿ
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢æ¢ç´¢")
                break
            except Exception as e:
                logger.error(f"æ¢ç´¢å‡ºé”™: {e}")
                print("\nâ³ ç­‰å¾… 1 åˆ†é’Ÿåé‡è¯•...")
                time.sleep(60)
    
    elif choice in ['1', '3']:
        keywords = explore_keywords(logger, headless)
        print(f"\nâœ… æ¢ç´¢åˆ° {len(keywords)} ä¸ªæ–°å…³é”®è¯")
        if keywords:
            print("\nç¤ºä¾‹å…³é”®è¯:")
            for kw in keywords[:10]:
                print(f"  - {kw}")
        
        time.sleep(5)  # ç­‰å¾…5ç§’å†è¿›è¡Œä¸‹ä¸€ä¸ªä»»åŠ¡
    
    if choice in ['2', '3']:
        vendors = explore_vendors(logger, headless)
        print(f"\nâœ… æ¢ç´¢åˆ° {len(vendors)} ä¸ªå‚å•†")
        if vendors:
            print("\nç¤ºä¾‹å‚å•†:")
            for name, domain in list(vendors.items())[:10]:
                print(f"  - {name}: {domain}")
    
    print("\n" + "="*70)
    print("æ¢ç´¢å®Œæˆï¼")
    print(f"å…³é”®è¯æ–‡ä»¶: {KEYWORDS_FILE}")
    print(f"å‚å•†æ–‡ä»¶: {VENDORS_FILE}")
    print("="*70)


if __name__ == '__main__':
    main()
