#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AlaiDocs â€” DC-DC çŸ¥è¯†åº“ä¸€ç«™å¼ CLI
è®©æ¯ä¸ªäººéƒ½èƒ½ç”¨ä¸€å¥è¯å»ºç«‹ä¸“ä¸šçš„ DC-DC æŠ€æœ¯çŸ¥è¯†åº“ã€‚

ç”¨æ³•:
  python alaidocs.py init                  # é¦–æ¬¡åˆå§‹åŒ– (åˆ›å»ºç›®å½•+é…ç½®)
  python alaidocs.py                       # äº¤äº’æ¨¡å¼
  python alaidocs.py collect               # ä¸€é”®é‡‡é›†
  python alaidocs.py classify              # è‡ªåŠ¨ 4D åˆ†ç±»
  python alaidocs.py pack "é™å‹å˜æ¢å™¨çƒ­ç®¡ç†" # æ£€ç´¢æ‰“åŒ…
  python alaidocs.py run "GaNæ•ˆç‡ä¼˜åŒ–"     # å…¨æµç¨‹: é‡‡é›†â†’åˆ†ç±»â†’æ‰“åŒ…
  python alaidocs.py status                # ç³»ç»ŸçŠ¶æ€

ç‰ˆæœ¬: 3.1.0 (portable â€” æ— ç¡¬ç¼–ç è·¯å¾„)
"""

import argparse
import json
import logging
import os
import shutil
import sqlite3
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# ============================================================================
# è·¯å¾„çº¦å®š â€” å…¨éƒ¨ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼Œé›¶ç¡¬ç¼–ç 
# ============================================================================

PROJECT_ROOT = Path(__file__).resolve().parent

# é»˜è®¤ç›®å½•ç»“æ„ (å…¨éƒ¨å¯é€šè¿‡ alaidocs_config.json è¦†ç›–)
DEFAULT_DIRS = {
    "download_dir":   "data/downloads",       # ä¸‹è½½ç¼“å†²åŒº
    "classified_dir": "data/classified",       # 4D åˆ†ç±»å½’æ¡£
    "kb_dir":         "data/kb",               # çŸ¥è¯†åº“ (SQLite + FAISS)
    "pack_output":    "data/packed",           # NotebookLM æ‰“åŒ…è¾“å‡º
    "keywords_db":    "data/keywords.json",    # å…³é”®è¯æ•°æ®åº“
}

USER_CONFIG_FILE = PROJECT_ROOT / "alaidocs_config.json"
TEMPLATE_CONFIG  = PROJECT_ROOT / "integrated_config.json"

BANNER = r"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           AlaiDocs â€” DC-DC çŸ¥è¯†åº“è‡ªåŠ¨åŒ–ç³»ç»Ÿ                  â•‘
â•‘   Collect â†’ Classify (4D) â†’ Pack â†’ NotebookLM              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

HELP_TEXT = """
  å¯ç”¨å‘½ä»¤:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  init                 é¦–æ¬¡åˆå§‹åŒ– (åˆ›å»ºç›®å½•å’Œé…ç½®æ–‡ä»¶)
  collect              ä¸€é”®é‡‡é›† (Gemini â†’ DuckDuckGo â†’ PDF)
  classify             è‡ªåŠ¨æ•´ç† (4D åˆ†ç±»å½’æ¡£)
  pack <æŸ¥è¯¢>          æ£€ç´¢æ‰“åŒ… (é«˜ç½®ä¿¡åº¦ç­›é€‰ â†’ NotebookLM)
  run  <æŸ¥è¯¢>          å…¨æµç¨‹   (é‡‡é›† â†’ åˆ†ç±» â†’ æ‰“åŒ…)
  status               ç³»ç»ŸçŠ¶æ€æ‘˜è¦
  config               æ˜¾ç¤ºå½“å‰é…ç½®
  help                 æ˜¾ç¤ºå¸®åŠ©
  quit / exit          é€€å‡º
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  pack å‚æ•°:
    pack <æŸ¥è¯¢> [--top N] [--min-score 0.5]
    ä¾‹: pack é™å‹å˜æ¢å™¨çƒ­ç®¡ç† --top 15 --min-score 0.6
"""


# ============================================================================
# é…ç½®ç®¡ç† â€” é¦–æ¬¡è¿è¡Œè‡ªåŠ¨ç”Ÿæˆç”¨æˆ·é…ç½®
# ============================================================================

def resolve_paths(config: Dict) -> Dict[str, Path]:
    """
    ä»é…ç½®è§£ææ‰€æœ‰è·¯å¾„ (ç›¸å¯¹è·¯å¾„åŸºäº PROJECT_ROOT)ã€‚
    ä¼˜å…ˆçº§: alaidocs_config.json > integrated_config.json > å†…ç½®é»˜è®¤å€¼
    """
    paths_cfg = config.get("paths", {})
    resolved = {}
    for key, default_rel in DEFAULT_DIRS.items():
        raw = paths_cfg.get(key, default_rel)
        p = Path(raw)
        if not p.is_absolute():
            p = PROJECT_ROOT / p
        resolved[key] = p
    # æ´¾ç”Ÿè·¯å¾„
    resolved["kb_db"]    = resolved["kb_dir"] / "kb.sqlite"
    resolved["kb_faiss"] = resolved["kb_dir"] / "kb.faiss"
    return resolved


def load_config(config_path: Path = None) -> Dict:
    """
    åŠ è½½é…ç½®ã€‚åˆå¹¶é“¾:
    å†…ç½®é»˜è®¤ â†’ integrated_config.json â†’ alaidocs_config.json â†’ CLI å‚æ•°
    """
    config = {}

    # 1) integrated_config.json (é¡¹ç›®çº§æ¨¡æ¿ï¼Œå…¥ Git)
    if TEMPLATE_CONFIG.exists():
        with open(TEMPLATE_CONFIG, "r", encoding="utf-8-sig") as f:
            config = json.load(f)

    # 2) alaidocs_config.json (ç”¨æˆ·çº§è¦†ç›–ï¼Œä¸å…¥ Git)
    user_cfg_path = config_path or USER_CONFIG_FILE
    if user_cfg_path.exists():
        with open(user_cfg_path, "r", encoding="utf-8-sig") as f:
            user_cfg = json.load(f)
        _deep_merge(config, user_cfg)

    return config


def _deep_merge(base: dict, override: dict):
    for k, v in override.items():
        if k in base and isinstance(base[k], dict) and isinstance(v, dict):
            _deep_merge(base[k], v)
        else:
            base[k] = v


def ensure_initialized(paths: Dict[str, Path], logger: logging.Logger) -> bool:
    """æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–ï¼›æœªåˆå§‹åŒ–åˆ™å¼•å¯¼ç”¨æˆ·ã€‚"""
    if paths["classified_dir"].exists() and paths["kb_dir"].exists():
        return True
    logger.warning("âš ï¸  é¡¹ç›®å°šæœªåˆå§‹åŒ–ã€‚è¯·å…ˆè¿è¡Œ:")
    logger.warning("   python alaidocs.py init")
    return False


# ============================================================================
# æ—¥å¿—
# ============================================================================

def setup_logging(debug: bool = False) -> logging.Logger:
    level = logging.DEBUG if debug else logging.INFO
    root = logging.getLogger()
    for h in root.handlers[:]:
        root.removeHandler(h)

    console = logging.StreamHandler(sys.stdout)
    console.setLevel(level)
    console.setFormatter(logging.Formatter(
        "%(asctime)s â”‚ %(message)s", datefmt="%H:%M:%S"
    ))

    log_file = PROJECT_ROOT / "alaidocs.log"
    fh = logging.FileHandler(str(log_file), encoding="utf-8")
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(name)s - %(message)s"
    ))

    root.setLevel(logging.DEBUG)
    root.addHandler(console)
    root.addHandler(fh)
    return logging.getLogger("alaidocs")


# ============================================================================
# å‘½ä»¤: init
# ============================================================================

def cmd_init(config: Dict, logger: logging.Logger):
    """é¦–æ¬¡åˆå§‹åŒ–: åˆ›å»ºç›®å½•ç»“æ„ + ç”Ÿæˆç”¨æˆ·é…ç½®æ–‡ä»¶"""
    print(BANNER)
    print("  ğŸ”§ æ­£åœ¨åˆå§‹åŒ– AlaiDocs é¡¹ç›®...\n")

    paths = resolve_paths(config)

    # 1) åˆ›å»ºç›®å½•
    for name, p in paths.items():
        if name.endswith("_db") or name.endswith("_faiss") or name == "keywords_db":
            p.parent.mkdir(parents=True, exist_ok=True)
        else:
            p.mkdir(parents=True, exist_ok=True)
        print(f"  âœ… {name:20s} â†’ {p}")

    # 2) ç”Ÿæˆ alaidocs_config.json
    if not USER_CONFIG_FILE.exists():
        try:
            rel_paths = {
                k: str(v.relative_to(PROJECT_ROOT)).replace("\\", "/")
                for k, v in paths.items()
                if k in DEFAULT_DIRS
            }
        except ValueError:
            rel_paths = {k: str(v) for k, v in paths.items() if k in DEFAULT_DIRS}

        user_config = {
            "_comment": [
                "AlaiDocs ç”¨æˆ·é…ç½® â€” æ­¤æ–‡ä»¶ä¸å…¥ Git",
                "ç¼–è¾‘ paths è‡ªå®šä¹‰ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨ç»å¯¹è·¯å¾„"
            ],
            "paths": rel_paths,
        }
        with open(USER_CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(user_config, f, indent=2, ensure_ascii=False)
        print(f"\n  ğŸ“ å·²ç”Ÿæˆç”¨æˆ·é…ç½®: {USER_CONFIG_FILE.name}")
    else:
        print(f"\n  ğŸ“ ç”¨æˆ·é…ç½®å·²å­˜åœ¨: {USER_CONFIG_FILE.name}")

    # 3) åˆå§‹åŒ–ç©ºå…³é”®è¯åº“
    kw_path = paths["keywords_db"]
    if not kw_path.exists():
        with open(kw_path, "w", encoding="utf-8") as f:
            json.dump({
                "keywords": {},
                "statistics": {
                    "total_keywords_used": 0,
                    "total_searches": 0,
                    "total_files_downloaded": 0,
                    "last_updated": None,
                },
            }, f, indent=2, ensure_ascii=False)
        print(f"  ğŸ“ å·²åˆ›å»ºå…³é”®è¯åº“: {kw_path.name}")

    # 4) æ›´æ–° .gitignore
    gitignore = PROJECT_ROOT / ".gitignore"
    needed = [
        "alaidocs_config.json",
        "alaidocs.log",
        "data/",
        "*.db",
        "__pycache__/",
        ".venv/",
    ]
    existing = set()
    if gitignore.exists():
        existing = set(gitignore.read_text(encoding="utf-8").splitlines())
    missing = [e for e in needed if e not in existing]
    if missing:
        with open(gitignore, "a", encoding="utf-8") as f:
            f.write("\n# AlaiDocs runtime (auto-generated)\n")
            for e in missing:
                f.write(e + "\n")
        print(f"  ğŸ“ å·²æ›´æ–° .gitignore")

    print(f"""
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… åˆå§‹åŒ–å®Œæˆï¼ä¸‹ä¸€æ­¥:

  1. å®‰è£…ä¾èµ–:
     pip install -r requirements.txt

  2. (æµè§ˆå™¨é‡‡é›†éœ€è¦) é¢å¤–å®‰è£…:
     pip install -r requirements_browser.txt

  3. å¯åŠ¨:
     python alaidocs.py              # äº¤äº’æ¨¡å¼
     python alaidocs.py collect      # å¼€å§‹é‡‡é›†
     python alaidocs.py status       # æŸ¥çœ‹çŠ¶æ€

  4. (å¯é€‰) ç¼–è¾‘ alaidocs_config.json è‡ªå®šä¹‰è·¯å¾„
     ä¾‹å¦‚æŒ‡å‘å·²æœ‰çš„ PDF ç›®å½•æˆ–çŸ¥è¯†åº“

  ç›®å½•ç»“æ„:
     data/
     â”œâ”€â”€ downloads/       # é‡‡é›†ç¼“å†²åŒº
     â”œâ”€â”€ classified/      # 4D åˆ†ç±»å½’æ¡£
     â”‚   â””â”€â”€ TI/datasheet/power_ic/buck/xxx.pdf
     â”œâ”€â”€ kb/              # çŸ¥è¯†åº“
     â”‚   â”œâ”€â”€ kb.sqlite
     â”‚   â””â”€â”€ kb.faiss
     â”œâ”€â”€ packed/          # NotebookLM æ‰“åŒ…
     â””â”€â”€ keywords.json    # å…³é”®è¯åº“
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
""")


# ============================================================================
# å‘½ä»¤: collect
# ============================================================================

def cmd_collect(config: Dict, paths: Dict[str, Path],
                logger: logging.Logger, rounds: int = 0):
    """ä¸€é”®é‡‡é›†: Gemini å…³é”®è¯ â†’ DuckDuckGo æœç´¢ â†’ PDF ä¸‹è½½"""
    logger.info("ğŸš€ å¯åŠ¨ä¸€é”®é‡‡é›†ç³»ç»Ÿ")
    logger.info(f"   ä¸‹è½½ç›®å½•: {paths['download_dir']}")
    logger.info(f"   å…³é”®è¯åº“: {paths['keywords_db']}")

    try:
        from integrated_searcher import IntegratedSearcher
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        logger.error("   pip install -r requirements.txt")
        logger.error("   pip install -r requirements_browser.txt")
        return False

    searcher_config = config.copy()
    if rounds > 0:
        searcher_config["max_rounds"] = rounds

    searcher = IntegratedSearcher(
        output_dir=paths["download_dir"],
        keyword_db_path=paths["keywords_db"],
        use_browser=True,
        config=searcher_config,
        logger=logger,
    )
    searcher.run()

    stats = searcher._count_downloaded_files()
    logger.info(f"\nğŸ“Š é‡‡é›†å®Œæˆ: {stats['files_downloaded']} ä¸ªæ–‡ä»¶, "
                f"{stats['total_size'] / (1024**3):.2f} GB")
    return True


# ============================================================================
# å‘½ä»¤: classify
# ============================================================================

def cmd_classify(config: Dict, paths: Dict[str, Path],
                 logger: logging.Logger, once: bool = True) -> Dict:
    """è‡ªåŠ¨æ•´ç†: 4D åˆ†ç±» (å‚å•†/ç±»å‹/ä¸»é¢˜/æ‹“æ‰‘)"""
    logger.info("ğŸ—‚ï¸  å¯åŠ¨ 4D è‡ªåŠ¨åˆ†ç±»ç³»ç»Ÿ")
    logger.info(f"   æºç›®å½•:   {paths['download_dir']}")
    logger.info(f"   ç›®æ ‡ç›®å½•: {paths['classified_dir']}")

    source_dir = paths["download_dir"]
    target_dir = paths["classified_dir"]
    stats = {"total": 0, "success": 0, "failed": 0, "skipped": 0}

    if not source_dir.exists():
        logger.warning(f"âš ï¸  æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return stats

    target_dir.mkdir(parents=True, exist_ok=True)

    try:
        from pdf_classifier import PDFClassifier, ProcessedFilesDB, is_file_stable
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return stats

    db_path = PROJECT_ROOT / "classified_files.db"
    db = ProcessedFilesDB(db_path)
    metadata_file = target_dir / "metadata.jsonl"
    head_pages = config.get("classifier", {}).get("head_pages", 3)
    min_stable = config.get("classifier", {}).get("min_stable_seconds", 15)

    classifier = PDFClassifier(
        source_dir=source_dir,
        target_dir=target_dir,
        db=db,
        metadata_file=metadata_file,
        head_pages=head_pages,
        mode="move",
        dry_run=False,
    )

    files = classifier.scan_new_files()
    logger.info(f"ğŸ“‹ å‘ç° {len(files)} ä¸ªå¾…åˆ†ç±» PDF")
    stats["total"] = len(files)

    for i, pdf_file in enumerate(files, 1):
        try:
            if not is_file_stable(pdf_file, checks=2, min_stable_seconds=min_stable):
                stats["skipped"] += 1
                continue
            result = classifier.process_file(pdf_file)
            if result:
                stats["success"] += 1
            else:
                stats["skipped"] += 1
            if i % 10 == 0 or i == len(files):
                logger.info(f"  è¿›åº¦: {i}/{len(files)} "
                            f"(âœ…{stats['success']} âŒ{stats['failed']} â­ï¸{stats['skipped']})")
        except Exception as e:
            stats["failed"] += 1
            logger.error(f"  âŒ {pdf_file.name}: {e}")

    db.close()
    logger.info(f"\nğŸ“Š åˆ†ç±»å®Œæˆ: æ€»è®¡ {stats['total']} | æˆåŠŸ {stats['success']} | "
                f"å¤±è´¥ {stats['failed']} | è·³è¿‡ {stats['skipped']}")
    return stats


# ============================================================================
# å‘½ä»¤: pack â€” é«˜ç½®ä¿¡åº¦ç­›é€‰
# ============================================================================

def cmd_pack(query: str, config: Dict, paths: Dict[str, Path],
             logger: logging.Logger, max_docs: int = 20,
             min_score: float = 0.0, auto_confirm: bool = False
             ) -> Optional[Path]:
    """å¯¹è¯æ‰“åŒ…: æ··åˆæ£€ç´¢ â†’ é«˜ç½®ä¿¡åº¦ç­›é€‰ â†’ NotebookLM Source"""
    logger.info(f"ğŸ” æ£€ç´¢æ‰“åŒ…: \"{query}\"")
    logger.info(f"   å‚æ•°: top={max_docs}, min_score={min_score}")

    kb_path    = paths["kb_db"]
    faiss_path = paths["kb_faiss"]
    base_dir   = paths["classified_dir"]
    base_output = paths["pack_output"]

    if not kb_path.exists():
        logger.error(f"âŒ çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_path}")
        logger.error("   è¯·å…ˆè¿è¡Œ collect + classify æ„å»ºçŸ¥è¯†åº“ï¼Œ")
        logger.error("   æˆ–ç¼–è¾‘ alaidocs_config.json æŒ‡å‘å·²æœ‰çŸ¥è¯†åº“ã€‚")
        return None

    try:
        from smart_pack import (
            hybrid_search, select_diverse_docs, pack_files,
            detect_language, make_slug,
        )
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        logger.error("   pip install sentence-transformers faiss-cpu deep-translator")
        return None

    results = hybrid_search(query, kb_path, faiss_path, top_k=100)
    if not results:
        logger.warning("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return None

    logger.info(f"âœ… æ£€ç´¢åˆ° {len(results)} ä¸ªå€™é€‰æ–‡æ¡£")

    if min_score > 0:
        before = len(results)
        results = [r for r in results if r["score"] >= min_score]
        logger.info(f"ğŸ¯ ç½®ä¿¡åº¦ç­›é€‰ (â‰¥{min_score}): {before} â†’ {len(results)}")

    if not results:
        logger.warning(f"âš ï¸  æ²¡æœ‰æ–‡æ¡£è¾¾åˆ°ç½®ä¿¡åº¦ {min_score}")
        return None

    selected = select_diverse_docs(results, max_docs=max_docs)

    # åˆ†å±‚å±•ç¤º
    high = [d for d in selected if d["score"] > 0.7]
    mid  = [d for d in selected if 0.4 <= d["score"] <= 0.7]
    low  = [d for d in selected if d["score"] < 0.4]
    icons = {"fts5": "ğŸ”¤", "faiss": "ğŸ§ ", "hybrid": "âš¡"}
    idx = 1

    print(f"\n{'â•'*70}")
    print(f"  ğŸ“¦ æ™ºèƒ½é€‰æ‹©äº† {len(selected)} ä¸ªæ–‡æ¡£")
    print(f"{'â•'*70}")

    for label, group in [("ğŸ”¥ é«˜ç›¸å…³åº¦", high), ("ğŸ“Œ ä¸­ç­‰ç›¸å…³åº¦", mid), ("ğŸ’¡ å‚è€ƒæ–‡æ¡£", low)]:
        if group:
            print(f"\n  {label} ({len(group)} ä¸ª):")
            for doc in group:
                ic = icons.get(doc["method"], "?")
                print(f"  {idx:2d}. [{doc['score']:.3f}] {ic} "
                      f"{doc['vendor']}/{doc['doc_type']} â€” {doc.get('title', '')[:55]}")
                idx += 1

    vendors_hit = sorted(set(d["vendor"] for d in selected))
    types_hit   = sorted(set(d["doc_type"] for d in selected))
    avg_score   = sum(d["score"] for d in selected) / len(selected)

    print(f"\n{'â”€'*70}")
    print(f"  ğŸ“Š è¦†ç›– {len(vendors_hit)} å‚å•†: {', '.join(vendors_hit)}")
    print(f"  ğŸ“Š æ–‡æ¡£ç±»å‹: {', '.join(types_hit)}")
    print(f"  ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {avg_score:.3f}")
    print(f"  ğŸ“Š å›¾ä¾‹: ğŸ”¤=å…³é”®è¯  ğŸ§ =è¯­ä¹‰  âš¡=æ··åˆéªŒè¯")
    print(f"{'â•'*70}")

    if not auto_confirm:
        confirm = input(f"\n  æ‰“åŒ…è¿™ {len(selected)} ä¸ªæ–‡ä»¶? (Y/n): ").strip().lower()
        if confirm and confirm != "y":
            logger.info("âŒ å·²å–æ¶ˆ")
            return None

    lang = detect_language(query)
    date_str = datetime.now().strftime("%Y-%m-%d")
    slug = make_slug(query)
    output_dir = base_output / date_str / slug
    if output_dir.exists():
        shutil.rmtree(output_dir)

    logger.info("ğŸ“¦ æ‰“åŒ…ä¸­...")
    packed = pack_files(selected, base_dir, output_dir)
    _write_manifest(output_dir, query, selected, packed, lang)

    logger.info(f"\nâœ… æˆåŠŸæ‰“åŒ… {len(packed)} ä¸ªæ–‡ä»¶")
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print(f"\n  ğŸ’¡ ä¸‹ä¸€æ­¥: æ‰“å¼€ NotebookLM â†’ ä¸Šä¼  {output_dir.absolute()} ä¸­æ‰€æœ‰æ–‡ä»¶")
    return output_dir


def _write_manifest(output_dir, query, selected, packed, lang):
    manifest = output_dir / "manifest.txt"
    with open(manifest, "w", encoding="utf-8") as f:
        f.write(f"AlaiDocs æ™ºèƒ½æ‰“åŒ…æ¸…å•\n{'='*70}\n")
        f.write(f"æŸ¥è¯¢: {query}\n")
        f.write(f"è¯­è¨€: {'ä¸­æ–‡' if lang == 'zh' else 'è‹±æ–‡'}\n")
        f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ–¹æ³•: FTS5 å…³é”®è¯ + FAISS è¯­ä¹‰ (åŠ æƒèåˆ)\n")
        f.write(f"æ•°é‡: {len(packed)}\n{'='*70}\n\n")
        for label, pred in [("é«˜ç›¸å…³åº¦", lambda s: s > 0.7),
                            ("ä¸­ç­‰ç›¸å…³åº¦", lambda s: 0.4 <= s <= 0.7),
                            ("å‚è€ƒæ–‡æ¡£", lambda s: s < 0.4)]:
            group = [(i, d) for i, d in enumerate(selected, 1) if pred(d["score"])]
            if group:
                f.write(f"ã€{label}ã€‘\n\n")
                for i, doc in group:
                    f.write(f"{i:2d}. {doc.get('title', 'N/A')}\n")
                    f.write(f"    å‚å•†: {doc['vendor']} | ç±»å‹: {doc['doc_type']}\n")
                    f.write(f"    ç½®ä¿¡åº¦: {doc['score']:.3f} | æ–¹æ³•: {doc['method']}\n\n")
        f.write(f"{'='*70}\n")


# ============================================================================
# å‘½ä»¤: run (å…¨æµç¨‹)
# ============================================================================

def cmd_run(query: str, config: Dict, paths: Dict[str, Path],
            logger: logging.Logger, rounds: int = 1,
            max_docs: int = 20, min_score: float = 0.0):
    """å…¨æµç¨‹: é‡‡é›† â†’ åˆ†ç±» â†’ æ‰“åŒ…"""
    header = f" ğŸš€ å…¨æµç¨‹: \"{query}\""
    logger.info(f"\nâ•”{'â•'*68}â•—")
    logger.info(f"â•‘{header:<68s}â•‘")
    logger.info(f"â•š{'â•'*68}â•\n")

    logger.info(f"{'â”'*70}")
    logger.info("  Phase 1/3: é‡‡é›† (Gemini â†’ DuckDuckGo â†’ PDF)")
    logger.info(f"{'â”'*70}")
    cmd_collect(config, paths, logger, rounds=rounds)

    logger.info(f"\n{'â”'*70}")
    logger.info("  Phase 2/3: 4D åˆ†ç±» (å‚å•†/ç±»å‹/ä¸»é¢˜/æ‹“æ‰‘)")
    logger.info(f"{'â”'*70}")
    cls = cmd_classify(config, paths, logger, once=True)

    logger.info(f"\n{'â”'*70}")
    logger.info("  Phase 3/3: é«˜ç½®ä¿¡åº¦æ£€ç´¢æ‰“åŒ…")
    logger.info(f"{'â”'*70}")
    output = cmd_pack(query, config, paths, logger,
                      max_docs=max_docs, min_score=min_score, auto_confirm=True)

    print(f"\nâ•”{'â•'*68}â•—")
    print(f"  âœ… å…¨æµç¨‹å®Œæˆ | åˆ†ç±»: {cls.get('success', 0)} ä¸ª")
    if output:
        print(f"  ğŸ“ æ‰“åŒ…: {output.absolute()}")
    print(f"â•š{'â•'*68}â•")


# ============================================================================
# å‘½ä»¤: status / config
# ============================================================================

def cmd_status(config: Dict, paths: Dict[str, Path], logger: logging.Logger):
    print(f"\n{'â•'*70}")
    print("  AlaiDocs ç³»ç»ŸçŠ¶æ€")
    print(f"{'â•'*70}\n")

    # ä¸‹è½½
    dl = paths["download_dir"]
    if dl.exists():
        pdfs = list(dl.rglob("*.pdf"))
        size = sum(f.stat().st_size for f in pdfs if f.is_file())
        vendors = sorted(d.name for d in dl.iterdir()
                         if d.is_dir() and not d.name.startswith(("_", ".")))
        print(f"  ğŸ“¥ ä¸‹è½½ç¼“å†²åŒº: {dl}")
        print(f"     PDF: {len(pdfs)} | {size/(1024**2):.1f} MB")
        print(f"     å‚å•†: {', '.join(vendors) if vendors else '(ç©º)'}")
    else:
        print(f"  ğŸ“¥ ä¸‹è½½ç¼“å†²åŒº: (æœªåˆ›å»º) â€” è¿è¡Œ init")

    # åˆ†ç±»
    cls = paths["classified_dir"]
    if cls.exists():
        cp = list(cls.rglob("*.pdf"))
        cs = sum(f.stat().st_size for f in cp if f.is_file())
        cv = sorted(d.name for d in cls.iterdir()
                    if d.is_dir() and not d.name.startswith(("_", ".", "T")))
        print(f"\n  ğŸ—‚ï¸  åˆ†ç±»å½’æ¡£: {cls}")
        print(f"     PDF: {len(cp)} | {cs/(1024**3):.2f} GB")
        print(f"     å‚å•† ({len(cv)}): {', '.join(cv[:12])}"
              f"{'...' if len(cv) > 12 else ''}")
    else:
        print(f"\n  ğŸ—‚ï¸  åˆ†ç±»å½’æ¡£: (æœªåˆ›å»º)")

    # çŸ¥è¯†åº“
    kb = paths["kb_db"]
    if kb.exists():
        try:
            conn = sqlite3.connect(f"file:{kb}?mode=ro", uri=True)
            c = conn.cursor()
            c.execute("SELECT COUNT(*) FROM documents")
            nd = c.fetchone()[0]
            c.execute("SELECT COUNT(*) FROM chunks")
            nc = c.fetchone()[0]
            conn.close()
            print(f"\n  ğŸ“š çŸ¥è¯†åº“: {kb}")
            print(f"     æ–‡æ¡£: {nd} | åˆ†å—: {nc}")
        except Exception as e:
            print(f"\n  ğŸ“š çŸ¥è¯†åº“: è¯»å–å¤±è´¥ ({e})")
    else:
        print(f"\n  ğŸ“š çŸ¥è¯†åº“: (æœªåˆ›å»º)")

    fi = paths["kb_faiss"]
    if fi.exists():
        print(f"  ğŸ§  FAISS: {fi.stat().st_size/(1024**2):.1f} MB")
    else:
        print(f"  ğŸ§  FAISS: (æœªåˆ›å»º)")

    kw = paths["keywords_db"]
    if kw.exists():
        try:
            kd = json.loads(kw.read_text(encoding="utf-8"))
            print(f"\n  ğŸ”‘ å…³é”®è¯: {len(kd.get('keywords', {}))} ä¸ª, "
                  f"{kd.get('statistics', {}).get('total_searches', 0)} æ¬¡æœç´¢")
        except Exception:
            print(f"\n  ğŸ”‘ å…³é”®è¯åº“: è§£æå¤±è´¥")
    else:
        print(f"\n  ğŸ”‘ å…³é”®è¯åº“: (æœªåˆ›å»º)")

    po = paths["pack_output"]
    if po.exists() and po.is_dir():
        recent = sorted(d.name for d in po.iterdir() if d.is_dir())[-5:]
        print(f"\n  ğŸ“¦ æ‰“åŒ…è¾“å‡º: {po}")
        if recent:
            print(f"     æœ€è¿‘: {', '.join(recent)}")

    dl_cfg = config.get("downloader", {})
    print(f"\n  âš™ï¸  é…ç½®:")
    print(f"     æ¯è½®å…³é”®è¯:   {dl_cfg.get('keywords_per_round', 10)}")
    print(f"     æ¯è¯ç»“æœ:     {dl_cfg.get('results_per_keyword', 20)}")
    print(f"     æ–‡ä»¶ä¸Šé™:     {dl_cfg.get('total_files_limit', 10000)}")
    print(f"     å®¹é‡ä¸Šé™:     {dl_cfg.get('total_size_limit_gb', 100)} GB")
    print(f"     ç™½åå•åŸŸå:   {len(dl_cfg.get('domain_whitelist', []))} ä¸ª")
    print(f"\n{'â•'*70}\n")


def cmd_config(config: Dict):
    print(f"\n{'â•'*70}")
    print("  å½“å‰é…ç½®")
    print(f"{'â•'*70}\n")
    print(json.dumps(config, indent=2, ensure_ascii=False))
    print(f"\n{'â•'*70}\n")


# ============================================================================
# äº¤äº’æ¨¡å¼ (REPL)
# ============================================================================

def interactive_mode(config: Dict, paths: Dict[str, Path],
                     logger: logging.Logger):
    print(BANNER)
    print(HELP_TEXT)

    while True:
        try:
            raw = input("\n  alaidocs> ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n  ğŸ‘‹ å†è§!")
            break

        if not raw:
            continue

        parts = raw.split()
        cmd = parts[0].lower()
        rest = parts[1:]

        if cmd in ("quit", "exit", "q"):
            print("  ğŸ‘‹ å†è§!")
            break
        elif cmd in ("help", "h", "?"):
            print(HELP_TEXT)
        elif cmd == "init":
            cmd_init(config, logger)
            # é‡æ–°åŠ è½½è·¯å¾„
            paths.update(resolve_paths(load_config()))
        elif cmd == "collect":
            rounds = int(rest[0]) if rest and rest[0].isdigit() else 0
            cmd_collect(config, paths, logger, rounds=rounds)
        elif cmd == "classify":
            cmd_classify(config, paths, logger, once=True)
        elif cmd == "pack":
            q, top_n, ms = _parse_pack_args(rest)
            if q:
                cmd_pack(q, config, paths, logger, max_docs=top_n, min_score=ms)
            else:
                print("  âš ï¸  ç”¨æ³•: pack <æŸ¥è¯¢> [--top N] [--min-score 0.5]")
        elif cmd == "run":
            q, top_n, ms, rds = _parse_run_args(rest)
            if q:
                cmd_run(q, config, paths, logger, rounds=rds,
                        max_docs=top_n, min_score=ms)
            else:
                print("  âš ï¸  ç”¨æ³•: run <æŸ¥è¯¢> [--top N] [--min-score 0.5] [--rounds N]")
        elif cmd == "status":
            cmd_status(config, paths, logger)
        elif cmd == "config":
            cmd_config(config)
        elif "æ›´æ–°èµ„æ–™" in raw or "å¼€å§‹é‡‡é›†" in raw:
            cmd_collect(config, paths, logger)
        elif any(k in raw for k in ("æ£€ç´¢", "æœç´¢", "æŸ¥æ‰¾")):
            topic = raw
            for prefix in ("å¸®æˆ‘æ£€ç´¢", "æ£€ç´¢", "æœç´¢", "æŸ¥æ‰¾"):
                if raw.startswith(prefix):
                    topic = raw[len(prefix):].strip()
                    break
            if topic:
                cmd_pack(topic, config, paths, logger)
        elif "æ•´ç†" in raw or "åˆ†ç±»" in raw:
            cmd_classify(config, paths, logger, once=True)
        elif "çŠ¶æ€" in raw:
            cmd_status(config, paths, logger)
        else:
            print(f"  ğŸ’¡ å°† \"{raw}\" ä½œä¸ºæ£€ç´¢æŸ¥è¯¢...")
            cmd_pack(raw, config, paths, logger)


def _parse_pack_args(rest):
    parts, top, ms, i = [], 20, 0.0, 0
    while i < len(rest):
        if rest[i] == "--top" and i + 1 < len(rest):
            top = int(rest[i+1]); i += 2
        elif rest[i] == "--min-score" and i + 1 < len(rest):
            ms = float(rest[i+1]); i += 2
        else:
            parts.append(rest[i]); i += 1
    return " ".join(parts), top, ms


def _parse_run_args(rest):
    parts, top, ms, rds, i = [], 20, 0.0, 1, 0
    while i < len(rest):
        if rest[i] == "--top" and i + 1 < len(rest):
            top = int(rest[i+1]); i += 2
        elif rest[i] == "--min-score" and i + 1 < len(rest):
            ms = float(rest[i+1]); i += 2
        elif rest[i] == "--rounds" and i + 1 < len(rest):
            rds = int(rest[i+1]); i += 2
        else:
            parts.append(rest[i]); i += 1
    return " ".join(parts), top, ms, rds


# ============================================================================
# CLI å…¥å£
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="AlaiDocs â€” DC-DC çŸ¥è¯†åº“ä¸€ç«™å¼ CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python alaidocs.py init                           # é¦–æ¬¡åˆå§‹åŒ–
  python alaidocs.py                                # äº¤äº’æ¨¡å¼
  python alaidocs.py collect                        # ä¸€é”®é‡‡é›†
  python alaidocs.py classify                       # è‡ªåŠ¨åˆ†ç±»
  python alaidocs.py pack "é™å‹å˜æ¢å™¨çƒ­ç®¡ç†"         # æ£€ç´¢æ‰“åŒ…
  python alaidocs.py pack "GaN efficiency" --top 15 # æ‰“åŒ… 15 ç¯‡
  python alaidocs.py run "LLCè°æŒ¯å˜æ¢å™¨" --rounds 2 # å…¨æµç¨‹
  python alaidocs.py status                         # ç³»ç»ŸçŠ¶æ€
        """,
    )

    sub = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")

    sub.add_parser("init", help="é¦–æ¬¡åˆå§‹åŒ–")

    sp = sub.add_parser("collect", help="ä¸€é”®é‡‡é›†")
    sp.add_argument("--rounds", type=int, default=0)

    sub.add_parser("classify", help="è‡ªåŠ¨ 4D åˆ†ç±»")

    sp = sub.add_parser("pack", help="æ£€ç´¢æ‰“åŒ…")
    sp.add_argument("query")
    sp.add_argument("--top", type=int, default=20)
    sp.add_argument("--min-score", type=float, default=0.0)
    sp.add_argument("-y", "--yes", action="store_true")

    sp = sub.add_parser("run", help="å…¨æµç¨‹")
    sp.add_argument("query")
    sp.add_argument("--rounds", type=int, default=1)
    sp.add_argument("--top", type=int, default=20)
    sp.add_argument("--min-score", type=float, default=0.0)

    sub.add_parser("status", help="ç³»ç»ŸçŠ¶æ€")
    sub.add_parser("config", help="æ˜¾ç¤ºé…ç½®")

    parser.add_argument("-c", "--config", type=Path, default=None)
    parser.add_argument("--debug", action="store_true")

    args = parser.parse_args()
    logger = setup_logging(args.debug)
    config = load_config(args.config)
    paths = resolve_paths(config)

    if args.command is None:
        interactive_mode(config, paths, logger)
    elif args.command == "init":
        cmd_init(config, logger)
    elif args.command == "collect":
        if not ensure_initialized(paths, logger):
            return
        cmd_collect(config, paths, logger, rounds=args.rounds)
    elif args.command == "classify":
        if not ensure_initialized(paths, logger):
            return
        cmd_classify(config, paths, logger, once=True)
    elif args.command == "pack":
        if not ensure_initialized(paths, logger):
            return
        cmd_pack(args.query, config, paths, logger,
                 max_docs=args.top, min_score=args.min_score,
                 auto_confirm=args.yes)
    elif args.command == "run":
        if not ensure_initialized(paths, logger):
            return
        cmd_run(args.query, config, paths, logger,
                rounds=args.rounds, max_docs=args.top,
                min_score=args.min_score)
    elif args.command == "status":
        cmd_status(config, paths, logger)
    elif args.command == "config":
        cmd_config(config)


if __name__ == "__main__":
    main()
