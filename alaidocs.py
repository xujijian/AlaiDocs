#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AlaiDocs â€” DC-DC çŸ¥è¯†åº“ä¸€ç«™å¼ CLI
è®©æ¯ä¸ªäººéƒ½èƒ½ç”¨ä¸€å¥è¯å»ºç«‹ä¸“ä¸šçš„ DC-DC æŠ€æœ¯çŸ¥è¯†åº“ã€‚

ç”¨æ³•:
  python alaidocs.py init                  # é¦–æ¬¡åˆå§‹åŒ– (åˆ›å»ºç›®å½•+é…ç½®)
  python alaidocs.py                       # äº¤äº’æ¨¡å¼
  python alaidocs.py collect "buck converter"  # é‡‡é›† PDF (DDG æœç´¢+ä¸‹è½½)
  python alaidocs.py classify              # è‡ªåŠ¨ 4D åˆ†ç±»
  python alaidocs.py build-kb              # æ„å»ºçŸ¥è¯†åº“
  python alaidocs.py pack "é™å‹å˜æ¢å™¨çƒ­ç®¡ç†" # æ£€ç´¢æ‰“åŒ…
  python alaidocs.py run "GaNæ•ˆç‡ä¼˜åŒ–"     # å…¨æµç¨‹: é‡‡é›†â†’åˆ†ç±»â†’å»ºåº“â†’æ‰“åŒ…
  python alaidocs.py status                # ç³»ç»ŸçŠ¶æ€

ç‰ˆæœ¬: 3.1.0 (portable â€” æ— ç¡¬ç¼–ç è·¯å¾„)
"""

import argparse
import json
import logging
import os
import shutil
import sqlite3
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# Windows ç»ˆç«¯ UTF-8 ç¼–ç  (emoji æ”¯æŒ)
if sys.platform == "win32":
    os.environ.setdefault("PYTHONIOENCODING", "utf-8")
    try:
        sys.stdout.reconfigure(encoding="utf-8", errors="replace")
        sys.stderr.reconfigure(encoding="utf-8", errors="replace")
    except Exception:
        pass

# ============================================================================
# è·¯å¾„çº¦å®š â€” å…¨éƒ¨ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼Œé›¶ç¡¬ç¼–ç 
# ============================================================================

PROJECT_ROOT = Path(__file__).resolve().parent

# é»˜è®¤ç›®å½•ç»“æ„ (å…¨éƒ¨å¯é€šè¿‡ alaidocs_config.json è¦†ç›–)
DEFAULT_DIRS = {
    "download_dir":   "data/downloads",       # ä¸‹è½½ç¼“å†²åŒº
    "classified_dir": "data/classified",       # 4D åˆ†ç±»å½’æ¡£
    "kb_dir":         "data/kb",               # çŸ¥è¯†åº“ (SQLite + FAISS)
    "pack_output":    "data/packed",           # NotebookLM æ‰“åŒ…è¾“å‡º
    "keywords_db":    "data/keywords.json",    # å…³é”®è¯æ•°æ®åº“
}

USER_CONFIG_FILE = PROJECT_ROOT / "alaidocs_config.json"
TEMPLATE_CONFIG  = PROJECT_ROOT / "integrated_config.json"

BANNER = r"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           AlaiDocs â€” DC-DC çŸ¥è¯†åº“è‡ªåŠ¨åŒ–ç³»ç»Ÿ                  â•‘
â•‘   Collect â†’ Classify (4D) â†’ Pack â†’ NotebookLM              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

HELP_TEXT = """
  å¯ç”¨å‘½ä»¤:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  init                 é¦–æ¬¡åˆå§‹åŒ– (åˆ›å»ºç›®å½•å’Œé…ç½®æ–‡ä»¶)
  collect <ä¸»é¢˜>       é‡‡é›† PDF (å…³é”®è¯æ‰©å±• â†’ DDG æœç´¢ â†’ ä¸‹è½½)
  classify             è‡ªåŠ¨æ•´ç† (4D åˆ†ç±»å½’æ¡£)
  build-kb             æ„å»º/æ›´æ–°çŸ¥è¯†åº“ (ä»åˆ†ç±»ç›®å½•)
  pack <æŸ¥è¯¢>          æ£€ç´¢æ‰“åŒ… (é«˜ç½®ä¿¡åº¦ç­›é€‰ â†’ NotebookLM)
  run  <æŸ¥è¯¢>          å…¨æµç¨‹   (é‡‡é›† â†’ åˆ†ç±» â†’ å»ºåº“ â†’ æ‰“åŒ…)
  status               ç³»ç»ŸçŠ¶æ€æ‘˜è¦
  config               æ˜¾ç¤ºå½“å‰é…ç½®
  help                 æ˜¾ç¤ºå¸®åŠ©
  quit / exit          é€€å‡º
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  ç¤ºä¾‹:
    collect buck converter         é‡‡é›† buck converter ç›¸å…³ PDF
    run "LLC resonant" --top 10   å…¨æµç¨‹ï¼Œæ‰“åŒ…å‰ 10 ç¯‡
    pack é™å‹å˜æ¢å™¨çƒ­ç®¡ç†            æ£€ç´¢æ‰“åŒ…
"""


# ============================================================================
# é…ç½®ç®¡ç† â€” é¦–æ¬¡è¿è¡Œè‡ªåŠ¨ç”Ÿæˆç”¨æˆ·é…ç½®
# ============================================================================

def resolve_paths(config: Dict) -> Dict[str, Path]:
    """
    ä»é…ç½®è§£ææ‰€æœ‰è·¯å¾„ (ç›¸å¯¹è·¯å¾„åŸºäº PROJECT_ROOT)ã€‚
    ä¼˜å…ˆçº§: alaidocs_config.json > integrated_config.json > å†…ç½®é»˜è®¤å€¼
    """
    paths_cfg = config.get("paths", {})
    resolved = {}
    for key, default_rel in DEFAULT_DIRS.items():
        raw = paths_cfg.get(key, default_rel)
        p = Path(raw)
        if not p.is_absolute():
            p = PROJECT_ROOT / p
        resolved[key] = p
    # æ´¾ç”Ÿè·¯å¾„
    resolved["kb_db"]    = resolved["kb_dir"] / "kb.sqlite"
    resolved["kb_faiss"] = resolved["kb_dir"] / "kb.faiss"
    return resolved


def load_config(config_path: Path = None) -> Dict:
    """
    åŠ è½½é…ç½®ã€‚åˆå¹¶é“¾:
    å†…ç½®é»˜è®¤ â†’ integrated_config.json â†’ alaidocs_config.json â†’ CLI å‚æ•°
    """
    config = {}

    # 1) integrated_config.json (é¡¹ç›®çº§æ¨¡æ¿ï¼Œå…¥ Git)
    if TEMPLATE_CONFIG.exists():
        with open(TEMPLATE_CONFIG, "r", encoding="utf-8-sig") as f:
            config = json.load(f)

    # 2) alaidocs_config.json (ç”¨æˆ·çº§è¦†ç›–ï¼Œä¸å…¥ Git)
    user_cfg_path = config_path or USER_CONFIG_FILE
    if user_cfg_path.exists():
        with open(user_cfg_path, "r", encoding="utf-8-sig") as f:
            user_cfg = json.load(f)
        _deep_merge(config, user_cfg)

    return config


def _deep_merge(base: dict, override: dict):
    for k, v in override.items():
        if k in base and isinstance(base[k], dict) and isinstance(v, dict):
            _deep_merge(base[k], v)
        else:
            base[k] = v


def ensure_initialized(paths: Dict[str, Path], logger: logging.Logger) -> bool:
    """æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–ï¼›æœªåˆå§‹åŒ–åˆ™å¼•å¯¼ç”¨æˆ·ã€‚"""
    if paths["classified_dir"].exists() and paths["kb_dir"].exists():
        return True
    logger.warning("âš ï¸  é¡¹ç›®å°šæœªåˆå§‹åŒ–ã€‚è¯·å…ˆè¿è¡Œ:")
    logger.warning("   python alaidocs.py init")
    return False


# ============================================================================
# æ—¥å¿—
# ============================================================================

def setup_logging(debug: bool = False) -> logging.Logger:
    level = logging.DEBUG if debug else logging.INFO
    root = logging.getLogger()
    for h in root.handlers[:]:
        root.removeHandler(h)

    console = logging.StreamHandler(sys.stdout)
    console.setLevel(level)
    console.setFormatter(logging.Formatter(
        "%(asctime)s â”‚ %(message)s", datefmt="%H:%M:%S"
    ))

    log_file = PROJECT_ROOT / "alaidocs.log"
    fh = logging.FileHandler(str(log_file), encoding="utf-8")
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(name)s - %(message)s"
    ))

    root.setLevel(logging.DEBUG)
    root.addHandler(console)
    root.addHandler(fh)
    return logging.getLogger("alaidocs")


# ============================================================================
# å‘½ä»¤: init
# ============================================================================

def cmd_init(config: Dict, logger: logging.Logger):
    """é¦–æ¬¡åˆå§‹åŒ–: åˆ›å»ºç›®å½•ç»“æ„ + ç”Ÿæˆç”¨æˆ·é…ç½®æ–‡ä»¶"""
    print(BANNER)
    print("  ğŸ”§ æ­£åœ¨åˆå§‹åŒ– AlaiDocs é¡¹ç›®...\n")

    paths = resolve_paths(config)

    # 1) åˆ›å»ºç›®å½•
    for name, p in paths.items():
        if name.endswith("_db") or name.endswith("_faiss") or name == "keywords_db":
            p.parent.mkdir(parents=True, exist_ok=True)
        else:
            p.mkdir(parents=True, exist_ok=True)
        print(f"  âœ… {name:20s} â†’ {p}")

    # 2) ç”Ÿæˆ alaidocs_config.json
    if not USER_CONFIG_FILE.exists():
        try:
            rel_paths = {
                k: str(v.relative_to(PROJECT_ROOT)).replace("\\", "/")
                for k, v in paths.items()
                if k in DEFAULT_DIRS
            }
        except ValueError:
            rel_paths = {k: str(v) for k, v in paths.items() if k in DEFAULT_DIRS}

        user_config = {
            "_comment": [
                "AlaiDocs ç”¨æˆ·é…ç½® â€” æ­¤æ–‡ä»¶ä¸å…¥ Git",
                "ç¼–è¾‘ paths è‡ªå®šä¹‰ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨ç»å¯¹è·¯å¾„"
            ],
            "paths": rel_paths,
        }
        with open(USER_CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(user_config, f, indent=2, ensure_ascii=False)
        print(f"\n  ğŸ“ å·²ç”Ÿæˆç”¨æˆ·é…ç½®: {USER_CONFIG_FILE.name}")
    else:
        print(f"\n  ğŸ“ ç”¨æˆ·é…ç½®å·²å­˜åœ¨: {USER_CONFIG_FILE.name}")

    # 3) åˆå§‹åŒ–ç©ºå…³é”®è¯åº“
    kw_path = paths["keywords_db"]
    if not kw_path.exists():
        with open(kw_path, "w", encoding="utf-8") as f:
            json.dump({
                "keywords": {},
                "statistics": {
                    "total_keywords_used": 0,
                    "total_searches": 0,
                    "total_files_downloaded": 0,
                    "last_updated": None,
                },
            }, f, indent=2, ensure_ascii=False)
        print(f"  ğŸ“ å·²åˆ›å»ºå…³é”®è¯åº“: {kw_path.name}")

    # 4) æ›´æ–° .gitignore
    gitignore = PROJECT_ROOT / ".gitignore"
    needed = [
        "alaidocs_config.json",
        "alaidocs.log",
        "data/",
        "*.db",
        "__pycache__/",
        ".venv/",
    ]
    existing = set()
    if gitignore.exists():
        existing = set(gitignore.read_text(encoding="utf-8").splitlines())
    missing = [e for e in needed if e not in existing]
    if missing:
        with open(gitignore, "a", encoding="utf-8") as f:
            f.write("\n# AlaiDocs runtime (auto-generated)\n")
            for e in missing:
                f.write(e + "\n")
        print(f"  ğŸ“ å·²æ›´æ–° .gitignore")

    print(f"""
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… åˆå§‹åŒ–å®Œæˆï¼ä¸‹ä¸€æ­¥:

  1. å®‰è£…ä¾èµ–:
     pip install -r requirements.txt

  2. (æµè§ˆå™¨é‡‡é›†éœ€è¦) é¢å¤–å®‰è£…:
     pip install -r requirements_browser.txt

  3. å¯åŠ¨:
     python alaidocs.py              # äº¤äº’æ¨¡å¼
     python alaidocs.py collect      # å¼€å§‹é‡‡é›†
     python alaidocs.py status       # æŸ¥çœ‹çŠ¶æ€

  4. (å¯é€‰) ç¼–è¾‘ alaidocs_config.json è‡ªå®šä¹‰è·¯å¾„
     ä¾‹å¦‚æŒ‡å‘å·²æœ‰çš„ PDF ç›®å½•æˆ–çŸ¥è¯†åº“

  ç›®å½•ç»“æ„:
     data/
     â”œâ”€â”€ downloads/       # é‡‡é›†ç¼“å†²åŒº
     â”œâ”€â”€ classified/      # 4D åˆ†ç±»å½’æ¡£
     â”‚   â””â”€â”€ TI/datasheet/power_ic/buck/xxx.pdf
     â”œâ”€â”€ kb/              # çŸ¥è¯†åº“
     â”‚   â”œâ”€â”€ kb.sqlite
     â”‚   â””â”€â”€ kb.faiss
     â”œâ”€â”€ packed/          # NotebookLM æ‰“åŒ…
     â””â”€â”€ keywords.json    # å…³é”®è¯åº“
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
""")


# ============================================================================
# å‘½ä»¤: collect â€” æ™ºèƒ½é‡‡é›†
# ============================================================================

def cmd_collect(config: Dict, paths: Dict[str, Path],
                logger: logging.Logger, query: str = "",
                rounds: int = 0, use_gemini: bool = False) -> Dict:
    """
    æ™ºèƒ½é‡‡é›†:  å…³é”®è¯æ‰©å±• â†’ DuckDuckGo æœç´¢ â†’ PDF ä¸‹è½½

    ä¸‰ç§æ¨¡å¼:
      1. query æŒ‡å®š  (é»˜è®¤)  â†’ æœ¬åœ°å…³é”®è¯æ‰©å±• + DDG API   (æ— éœ€æµè§ˆå™¨)
      2. --gemini             â†’ Gemini ç”Ÿæˆè¯ + DDG Chrome (éœ€è¦ Chrome)
      3. æ—  query + æ—  gemini â†’ ä½¿ç”¨é…ç½® focus_areas ç›´æœ
    """
    stats = {"downloaded": 0, "failed": 0, "skipped": 0, "keywords": 0}

    download_dir = paths["download_dir"]
    download_dir.mkdir(parents=True, exist_ok=True)

    logger.info("ğŸš€ å¯åŠ¨æ™ºèƒ½é‡‡é›†ç³»ç»Ÿ")
    logger.info(f"   ä¸‹è½½ç›®å½•: {download_dir}")

    # â”€â”€ å†³å®šé‡‡é›†æ¨¡å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if use_gemini:
        return _collect_gemini_mode(config, paths, logger, query, rounds)

    # â”€â”€ å…³é”®è¯ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if query:
        from keyword_expander import expand_query
        keywords = expand_query(query, max_keywords=50)
        logger.info(f"ğŸ”‘ ä» \"{query}\" æ‰©å±•å‡º {len(keywords)} ä¸ªæœç´¢å…³é”®è¯")
    else:
        focus = config.get("downloader", {}).get("focus_areas", [])
        if focus:
            keywords = [f"{fa} pdf" for fa in focus]
            logger.info(f"ğŸ”‘ ä½¿ç”¨é…ç½® focus_areas: {len(keywords)} ä¸ªå…³é”®è¯")
        else:
            logger.error("âŒ è¯·æä¾›æŸ¥è¯¢ä¸»é¢˜: collect <æŸ¥è¯¢>")
            logger.error("   ä¾‹: python alaidocs.py collect \"buck converter\"")
            return stats

    stats["keywords"] = len(keywords)
    for i, kw in enumerate(keywords[:10], 1):
        logger.info(f"   {i:2d}. {kw}")
    if len(keywords) > 10:
        logger.info(f"   ... å…± {len(keywords)} ä¸ª")

    # â”€â”€ DDG æœç´¢ + ä¸‹è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        from ddg_fetcher import (DDGSearcher, Downloader, is_likely_pdf_url,
                                  safe_filename, VENDOR_DOMAINS)
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥ ddg_fetcher å¤±è´¥: {e}")
        logger.error("   pip install -r requirements.txt")
        return stats

    searcher  = DDGSearcher(logger, sleep_interval=3.0)
    downloader = Downloader(logger, timeout=30)
    seen_urls: set = set()
    results_per_kw = config.get("downloader", {}).get("results_per_keyword", 20)

    for ki, keyword in enumerate(keywords, 1):
        logger.info(f"\n[{ki}/{len(keywords)}] ğŸ” {keyword}")
        try:
            hits = searcher.search(keyword, max_results=results_per_kw)
        except Exception as e:
            logger.warning(f"   âš ï¸ æœç´¢å¤±è´¥: {e}")
            continue
        if not hits:
            logger.info("   (æ— ç»“æœ)")
            continue

        pdf_urls = []
        for h in hits:
            url = h["url"]
            if url in seen_urls:
                continue
            seen_urls.add(url)
            # filetype:pdf æœç´¢çš„ç»“æœå¤§å¤šæ˜¯ PDFï¼›is_likely_pdf_url åšé¢å¤–æ ¡éªŒ
            if is_likely_pdf_url(url) or "filetype:pdf" in keyword.lower():
                pdf_urls.append((h["title"], url))

        logger.info(f"   PDF é“¾æ¥: {len(pdf_urls)} / {len(hits)}")

        for title, url in pdf_urls:
            vendor = _infer_vendor(url, VENDOR_DOMAINS)
            vendor_dir = download_dir / (vendor or "other")
            vendor_dir.mkdir(parents=True, exist_ok=True)

            fname = safe_filename(title or Path(url).stem, max_length=120)
            if not fname.lower().endswith(".pdf"):
                fname += ".pdf"
            fpath = vendor_dir / fname
            if fpath.exists():
                stats["skipped"] += 1
                continue

            ok, err = downloader.download_file(url, fpath, "pdf")
            if ok:
                stats["downloaded"] += 1
            else:
                stats["failed"] += 1
                logger.debug(f"   âŒ {err}: {url[:70]}")

        # é˜²è¿‡çƒ­: æ¯ 10 ä¸ªå…³é”®è¯ä¼‘æ¯ä¸€ä¸‹
        if ki % 10 == 0:
            logger.info(f"   â¸ è¿›åº¦: {stats['downloaded']} ä¸‹è½½ / {stats['failed']} å¤±è´¥")
            time.sleep(5)

    logger.info(f"\n{'â”'*60}")
    logger.info(f"ğŸ“Š é‡‡é›†å®Œæˆ")
    logger.info(f"   æœç´¢è¯:  {stats['keywords']}")
    logger.info(f"   ä¸‹è½½:    {stats['downloaded']}")
    logger.info(f"   å¤±è´¥:    {stats['failed']}")
    logger.info(f"   è·³è¿‡:    {stats['skipped']}")
    logger.info(f"   ç›®å½•:    {download_dir}")
    logger.info(f"{'â”'*60}")
    return stats


def _infer_vendor(url: str, vendor_domains: Dict) -> Optional[str]:
    """ä» URL åŸŸåæ¨æ–­å‚å•†å"""
    from urllib.parse import urlparse
    try:
        domain = urlparse(url).netloc.lower()
        for vendor, domains in vendor_domains.items():
            if any(d in domain for d in domains):
                return vendor
    except Exception:
        pass
    return None


def _collect_gemini_mode(config: Dict, paths: Dict[str, Path],
                         logger: logging.Logger,
                         query: str = "", rounds: int = 0) -> Dict:
    """Gemini Chrome æ¨¡å¼é‡‡é›† (åŸå§‹æ¨¡å¼ï¼Œéœ€è¦æµè§ˆå™¨ç™»å½•)"""
    logger.info("ğŸŒ Gemini æ¨¡å¼: ä½¿ç”¨ Chrome æµè§ˆå™¨")
    try:
        from integrated_searcher import IntegratedSearcher
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        logger.error("   pip install -r requirements_browser.txt")
        return {"downloaded": 0}

    searcher_config = config.copy()
    if rounds > 0:
        searcher_config["max_rounds"] = rounds
    # å¦‚æœæœ‰ queryï¼Œæ³¨å…¥åˆ° focus_areas å¤´éƒ¨
    if query:
        fa = searcher_config.get("downloader", {}).get("focus_areas", [])
        searcher_config.setdefault("downloader", {})["focus_areas"] = [query] + fa

    searcher = IntegratedSearcher(
        output_dir=paths["download_dir"],
        keyword_db_path=paths["keywords_db"],
        use_browser=True,
        config=searcher_config,
        logger=logger,
    )
    searcher.run()
    s = searcher._count_downloaded_files()
    return {"downloaded": s["files_downloaded"], "failed": 0, "skipped": 0}


# ============================================================================
# å‘½ä»¤: classify
# ============================================================================

def cmd_classify(config: Dict, paths: Dict[str, Path],
                 logger: logging.Logger, once: bool = True) -> Dict:
    """è‡ªåŠ¨æ•´ç†: 4D åˆ†ç±» (å‚å•†/ç±»å‹/ä¸»é¢˜/æ‹“æ‰‘)"""
    logger.info("ğŸ—‚ï¸  å¯åŠ¨ 4D è‡ªåŠ¨åˆ†ç±»ç³»ç»Ÿ")
    logger.info(f"   æºç›®å½•:   {paths['download_dir']}")
    logger.info(f"   ç›®æ ‡ç›®å½•: {paths['classified_dir']}")

    source_dir = paths["download_dir"]
    target_dir = paths["classified_dir"]
    stats = {"total": 0, "success": 0, "failed": 0, "skipped": 0}

    if not source_dir.exists():
        logger.warning(f"âš ï¸  æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return stats

    target_dir.mkdir(parents=True, exist_ok=True)

    try:
        from pdf_classifier import PDFClassifier, ProcessedFilesDB, is_file_stable
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return stats

    db_path = PROJECT_ROOT / "classified_files.db"
    db = ProcessedFilesDB(db_path)
    metadata_file = target_dir / "metadata.jsonl"
    head_pages = config.get("classifier", {}).get("head_pages", 3)
    min_stable = config.get("classifier", {}).get("min_stable_seconds", 15)

    classifier = PDFClassifier(
        source_dir=source_dir,
        target_dir=target_dir,
        db=db,
        metadata_file=metadata_file,
        head_pages=head_pages,
        mode="move",
        dry_run=False,
    )

    files = classifier.scan_new_files()
    logger.info(f"ğŸ“‹ å‘ç° {len(files)} ä¸ªå¾…åˆ†ç±» PDF")
    stats["total"] = len(files)

    for i, pdf_file in enumerate(files, 1):
        try:
            if not is_file_stable(pdf_file, checks=2, min_stable_seconds=min_stable):
                stats["skipped"] += 1
                continue
            result = classifier.process_file(pdf_file)
            if result:
                stats["success"] += 1
            else:
                stats["skipped"] += 1
            if i % 10 == 0 or i == len(files):
                logger.info(f"  è¿›åº¦: {i}/{len(files)} "
                            f"(âœ…{stats['success']} âŒ{stats['failed']} â­ï¸{stats['skipped']})")
        except Exception as e:
            stats["failed"] += 1
            logger.error(f"  âŒ {pdf_file.name}: {e}")

    db.close()
    logger.info(f"\nğŸ“Š åˆ†ç±»å®Œæˆ: æ€»è®¡ {stats['total']} | æˆåŠŸ {stats['success']} | "
                f"å¤±è´¥ {stats['failed']} | è·³è¿‡ {stats['skipped']}")
    return stats


# ============================================================================
# å‘½ä»¤: build-kb â€” ä»åˆ†ç±»ç›®å½•æ„å»ºçŸ¥è¯†åº“
# ============================================================================

def cmd_build_kb(config: Dict, paths: Dict[str, Path],
                 logger: logging.Logger, rebuild: bool = False,
                 repair: bool = False) -> Dict:
    """ä»åˆ†ç±»å¥½çš„ PDF æ„å»º / å¢é‡æ›´æ–°çŸ¥è¯†åº“ (SQLite FTS5 + FAISS)"""

    # --repair: å¿«é€Ÿä¿®å¤æŸåçš„ FTS5 ç´¢å¼•
    if repair:
        logger.info("ğŸ”§ ä¿®å¤ FTS5 ç´¢å¼•")
        try:
            from kb_builder import repair_fts
        except ImportError as e:
            logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            return {}
        return repair_fts(paths["kb_dir"])

    logger.info("ğŸ“š æ„å»ºçŸ¥è¯†åº“")
    logger.info(f"   æºç›®å½•: {paths['classified_dir']}")
    logger.info(f"   çŸ¥è¯†åº“: {paths['kb_dir']}")

    classified_dir = paths["classified_dir"]
    if not classified_dir.exists():
        logger.warning(f"âš ï¸  åˆ†ç±»ç›®å½•ä¸å­˜åœ¨: {classified_dir}")
        logger.warning("   è¯·å…ˆè¿è¡Œ classify å¯¹ PDF è¿›è¡Œåˆ†ç±»")
        return {}

    pdf_count = len(list(classified_dir.rglob("*.pdf")))
    if pdf_count == 0:
        logger.warning("âš ï¸  åˆ†ç±»ç›®å½•ä¸­æ²¡æœ‰ PDF æ–‡ä»¶")
        return {}

    try:
        from kb_builder import build_kb
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        logger.error("   pip install pypdf")
        return {}

    def progress(cur, total, name):
        if cur % 20 == 0 or cur == total:
            logger.info(f"  [{cur}/{total}] {name}")

    stats = build_kb(
        classified_dir=classified_dir,
        kb_dir=paths["kb_dir"],
        rebuild=rebuild,
        build_faiss=True,
        progress_callback=progress,
    )

    logger.info(f"\nğŸ“Š çŸ¥è¯†åº“æ„å»ºå®Œæˆ:")
    logger.info(f"   æ–°å¢æ–‡æ¡£: {stats.get('docs_added', 0)}")
    logger.info(f"   æ–°å¢åˆ†å—: {stats.get('chunks_added', 0)}")
    logger.info(f"   FAISSå‘é‡: {stats.get('faiss_vectors', 0)}")
    logger.info(f"   è·³è¿‡: {stats.get('skipped', 0)} | é”™è¯¯: {stats.get('errors', 0)}")
    return stats


# ============================================================================
# å‘½ä»¤: pack â€” é«˜ç½®ä¿¡åº¦ç­›é€‰
# ============================================================================

def cmd_pack(query: str, config: Dict, paths: Dict[str, Path],
             logger: logging.Logger, max_docs: int = 20,
             min_score: float = 0.0, auto_confirm: bool = False
             ) -> Optional[Path]:
    """å¯¹è¯æ‰“åŒ…: æ··åˆæ£€ç´¢ â†’ é«˜ç½®ä¿¡åº¦ç­›é€‰ â†’ NotebookLM Source"""
    logger.info(f"ğŸ” æ£€ç´¢æ‰“åŒ…: \"{query}\"")
    logger.info(f"   å‚æ•°: top={max_docs}, min_score={min_score}")

    kb_path    = paths["kb_db"]
    faiss_path = paths["kb_faiss"]
    base_dir   = paths["classified_dir"]
    base_output = paths["pack_output"]

    if not kb_path.exists():
        # è‡ªåŠ¨ä»åˆ†ç±»ç›®å½•æ„å»ºçŸ¥è¯†åº“
        classified_dir = paths["classified_dir"]
        if classified_dir.exists() and list(classified_dir.rglob("*.pdf")):
            logger.info("ğŸ“š çŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œä»åˆ†ç±»ç›®å½•è‡ªåŠ¨æ„å»º...")
            cmd_build_kb(config, paths, logger)
            if not kb_path.exists():
                logger.error("âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥")
                return None
        else:
            logger.error(f"âŒ çŸ¥è¯†åº“ä¸å­˜åœ¨ä¸”æ— åˆ†ç±»æ–‡ä»¶: {kb_path}")
            logger.error("   è¯·å…ˆè¿è¡Œ: collect â†’ classify â†’ build-kb")
            return None

    try:
        from smart_pack import (
            hybrid_search, select_diverse_docs, pack_files,
            detect_language, make_slug,
        )
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        logger.error("   pip install sentence-transformers faiss-cpu deep-translator")
        return None

    results = hybrid_search(query, kb_path, faiss_path, top_k=100)

    # è‡ªåŠ¨æ£€æµ‹ FTS5 æŸå â†’ ä¿®å¤åé‡è¯•
    if not results or all(r.get("method") != "fts5" for r in results):
        fts_broken = False
        try:
            import sqlite3 as _sql
            _c = _sql.connect(f"file:{kb_path}?mode=ro", uri=True)
            # å’Œ smart_pack.py ä¸€æ¨¡ä¸€æ ·çš„ 3-table JOIN + ORDER BY bm25
            _c.execute("""
                SELECT chunks.doc_id, d.path, bm25(chunks_fts)
                FROM chunks_fts
                JOIN chunks ON chunks_fts.chunk_id = chunks.chunk_id
                JOIN documents d ON chunks.doc_id = d.doc_id
                WHERE chunks_fts MATCH '"test"'
                ORDER BY bm25(chunks_fts) ASC
                LIMIT 5
            """).fetchall()
        except (_sql.DatabaseError, _sql.OperationalError):
            fts_broken = True
        finally:
            try:
                _c.close()
            except Exception:
                pass

        if fts_broken:
            logger.warning("âš ï¸  æ£€æµ‹åˆ° FTS5 ç´¢å¼•æŸåï¼Œè‡ªåŠ¨ä¿®å¤ä¸­...")
            cmd_build_kb(config, paths, logger, repair=True)
            results = hybrid_search(query, kb_path, faiss_path, top_k=100)

    if not results:
        logger.warning("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return None

    logger.info(f"âœ… æ£€ç´¢åˆ° {len(results)} ä¸ªå€™é€‰æ–‡æ¡£")

    if min_score > 0:
        before = len(results)
        results = [r for r in results if r["score"] >= min_score]
        logger.info(f"ğŸ¯ ç½®ä¿¡åº¦ç­›é€‰ (â‰¥{min_score}): {before} â†’ {len(results)}")

    if not results:
        logger.warning(f"âš ï¸  æ²¡æœ‰æ–‡æ¡£è¾¾åˆ°ç½®ä¿¡åº¦ {min_score}")
        return None

    selected = select_diverse_docs(results, max_docs=max_docs)

    # åˆ†å±‚å±•ç¤º
    high = [d for d in selected if d["score"] > 0.7]
    mid  = [d for d in selected if 0.4 <= d["score"] <= 0.7]
    low  = [d for d in selected if d["score"] < 0.4]
    icons = {"fts5": "ğŸ”¤", "faiss": "ğŸ§ ", "hybrid": "âš¡"}
    idx = 1

    print(f"\n{'â•'*70}")
    print(f"  ğŸ“¦ æ™ºèƒ½é€‰æ‹©äº† {len(selected)} ä¸ªæ–‡æ¡£")
    print(f"{'â•'*70}")

    for label, group in [("ğŸ”¥ é«˜ç›¸å…³åº¦", high), ("ğŸ“Œ ä¸­ç­‰ç›¸å…³åº¦", mid), ("ğŸ’¡ å‚è€ƒæ–‡æ¡£", low)]:
        if group:
            print(f"\n  {label} ({len(group)} ä¸ª):")
            for doc in group:
                ic = icons.get(doc["method"], "?")
                print(f"  {idx:2d}. [{doc['score']:.3f}] {ic} "
                      f"{doc['vendor']}/{doc['doc_type']} â€” {doc.get('title', '')[:55]}")
                idx += 1

    vendors_hit = sorted(set(d["vendor"] for d in selected))
    types_hit   = sorted(set(d["doc_type"] for d in selected))
    avg_score   = sum(d["score"] for d in selected) / len(selected)

    print(f"\n{'â”€'*70}")
    print(f"  ğŸ“Š è¦†ç›– {len(vendors_hit)} å‚å•†: {', '.join(vendors_hit)}")
    print(f"  ğŸ“Š æ–‡æ¡£ç±»å‹: {', '.join(types_hit)}")
    print(f"  ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {avg_score:.3f}")
    print(f"  ğŸ“Š å›¾ä¾‹: ğŸ”¤=å…³é”®è¯  ğŸ§ =è¯­ä¹‰  âš¡=æ··åˆéªŒè¯")
    print(f"{'â•'*70}")

    if not auto_confirm:
        confirm = input(f"\n  æ‰“åŒ…è¿™ {len(selected)} ä¸ªæ–‡ä»¶? (Y/n): ").strip().lower()
        if confirm and confirm != "y":
            logger.info("âŒ å·²å–æ¶ˆ")
            return None

    lang = detect_language(query)
    date_str = datetime.now().strftime("%Y-%m-%d")
    slug = make_slug(query)
    output_dir = base_output / date_str / slug
    if output_dir.exists():
        try:
            shutil.rmtree(output_dir)
        except PermissionError:
            # Windows æ–‡ä»¶é”å®šæ—¶è¿½åŠ æ—¶é—´æˆ³
            slug2 = f"{slug}_{datetime.now().strftime('%H%M%S')}"
            output_dir = base_output / date_str / slug2
            logger.info(f"âš ï¸  æ—§ç›®å½•è¢«å ç”¨ï¼Œä½¿ç”¨æ–°ç›®å½•: {slug2}")

    logger.info("ğŸ“¦ æ‰“åŒ…ä¸­...")
    packed = pack_files(selected, base_dir, output_dir)
    _write_manifest(output_dir, query, selected, packed, lang)

    logger.info(f"\nâœ… æˆåŠŸæ‰“åŒ… {len(packed)} ä¸ªæ–‡ä»¶")
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print(f"\n  ğŸ’¡ ä¸‹ä¸€æ­¥: æ‰“å¼€ NotebookLM â†’ ä¸Šä¼  {output_dir.absolute()} ä¸­æ‰€æœ‰æ–‡ä»¶")
    return output_dir


def _write_manifest(output_dir, query, selected, packed, lang):
    manifest = output_dir / "manifest.txt"
    with open(manifest, "w", encoding="utf-8") as f:
        f.write(f"AlaiDocs æ™ºèƒ½æ‰“åŒ…æ¸…å•\n{'='*70}\n")
        f.write(f"æŸ¥è¯¢: {query}\n")
        f.write(f"è¯­è¨€: {'ä¸­æ–‡' if lang == 'zh' else 'è‹±æ–‡'}\n")
        f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ–¹æ³•: FTS5 å…³é”®è¯ + FAISS è¯­ä¹‰ (åŠ æƒèåˆ)\n")
        f.write(f"æ•°é‡: {len(packed)}\n{'='*70}\n\n")
        for label, pred in [("é«˜ç›¸å…³åº¦", lambda s: s > 0.7),
                            ("ä¸­ç­‰ç›¸å…³åº¦", lambda s: 0.4 <= s <= 0.7),
                            ("å‚è€ƒæ–‡æ¡£", lambda s: s < 0.4)]:
            group = [(i, d) for i, d in enumerate(selected, 1) if pred(d["score"])]
            if group:
                f.write(f"ã€{label}ã€‘\n\n")
                for i, doc in group:
                    f.write(f"{i:2d}. {doc.get('title', 'N/A')}\n")
                    f.write(f"    å‚å•†: {doc['vendor']} | ç±»å‹: {doc['doc_type']}\n")
                    f.write(f"    ç½®ä¿¡åº¦: {doc['score']:.3f} | æ–¹æ³•: {doc['method']}\n\n")
        f.write(f"{'='*70}\n")


# ============================================================================
# å‘½ä»¤: run (å…¨æµç¨‹)
# ============================================================================

def cmd_run(query: str, config: Dict, paths: Dict[str, Path],
            logger: logging.Logger, rounds: int = 1,
            max_docs: int = 20, min_score: float = 0.0,
            use_gemini: bool = False):
    """å…¨æµç¨‹: é‡‡é›† â†’ åˆ†ç±» â†’ å»ºåº“ â†’ æ‰“åŒ… (ä¸€æ¡å‘½ä»¤)"""
    header = f" ğŸš€ å…¨æµç¨‹: \"{query}\""
    logger.info(f"\nâ•”{'â•'*68}â•—")
    logger.info(f"â•‘{header:<68s}â•‘")
    logger.info(f"â•š{'â•'*68}â•\n")

    t0 = time.time()

    # â”€â”€ Phase 1: é‡‡é›† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"{'â”'*70}")
    if use_gemini:
        logger.info("  Phase 1/4: é‡‡é›† (Gemini â†’ DuckDuckGo â†’ PDF)")
    else:
        logger.info("  Phase 1/4: é‡‡é›† (å…³é”®è¯æ‰©å±• â†’ DuckDuckGo â†’ PDF)")
    logger.info(f"{'â”'*70}")
    col = cmd_collect(config, paths, logger,
                      query=query, rounds=rounds, use_gemini=use_gemini)

    # â”€â”€ Phase 2: åˆ†ç±» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\n{'â”'*70}")
    logger.info("  Phase 2/4: 4D åˆ†ç±» (å‚å•†/ç±»å‹/ä¸»é¢˜/æ‹“æ‰‘)")
    logger.info(f"{'â”'*70}")
    cls = cmd_classify(config, paths, logger, once=True)

    # â”€â”€ Phase 3: å»ºåº“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\n{'â”'*70}")
    logger.info("  Phase 3/4: æ„å»ºçŸ¥è¯†åº“ (SQLite FTS5 + FAISS)")
    logger.info(f"{'â”'*70}")
    cmd_build_kb(config, paths, logger)

    # â”€â”€ Phase 4: æ‰“åŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\n{'â”'*70}")
    logger.info("  Phase 4/4: æ··åˆæ£€ç´¢æ‰“åŒ…")
    logger.info(f"{'â”'*70}")
    output = cmd_pack(query, config, paths, logger,
                      max_docs=max_docs, min_score=min_score, auto_confirm=True)

    elapsed = time.time() - t0
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)

    print(f"\nâ•”{'â•'*68}â•—")
    print(f"  âœ… å…¨æµç¨‹å®Œæˆ ({minutes}åˆ†{seconds}ç§’)")
    print(f"  ğŸ“¥ é‡‡é›†: {col.get('downloaded', 0)} ä¸ª PDF")
    print(f"  ğŸ—‚ï¸  åˆ†ç±»: {cls.get('success', 0)} ä¸ª")
    if output:
        print(f"  ğŸ“ æ‰“åŒ…: {output.absolute()}")
    print(f"â•š{'â•'*68}â•")


# ============================================================================
# å‘½ä»¤: status / config
# ============================================================================

def cmd_status(config: Dict, paths: Dict[str, Path], logger: logging.Logger):
    print(f"\n{'â•'*70}")
    print("  AlaiDocs ç³»ç»ŸçŠ¶æ€")
    print(f"{'â•'*70}\n")

    # ä¸‹è½½
    dl = paths["download_dir"]
    if dl.exists():
        pdfs = list(dl.rglob("*.pdf"))
        size = sum(f.stat().st_size for f in pdfs if f.is_file())
        vendors = sorted(d.name for d in dl.iterdir()
                         if d.is_dir() and not d.name.startswith(("_", ".")))
        print(f"  ğŸ“¥ ä¸‹è½½ç¼“å†²åŒº: {dl}")
        print(f"     PDF: {len(pdfs)} | {size/(1024**2):.1f} MB")
        print(f"     å‚å•†: {', '.join(vendors) if vendors else '(ç©º)'}")
    else:
        print(f"  ğŸ“¥ ä¸‹è½½ç¼“å†²åŒº: (æœªåˆ›å»º) â€” è¿è¡Œ init")

    # åˆ†ç±»
    cls = paths["classified_dir"]
    if cls.exists():
        cp = list(cls.rglob("*.pdf"))
        cs = sum(f.stat().st_size for f in cp if f.is_file())
        cv = sorted(d.name for d in cls.iterdir()
                    if d.is_dir() and not d.name.startswith(("_", ".", "T")))
        print(f"\n  ğŸ—‚ï¸  åˆ†ç±»å½’æ¡£: {cls}")
        print(f"     PDF: {len(cp)} | {cs/(1024**3):.2f} GB")
        print(f"     å‚å•† ({len(cv)}): {', '.join(cv[:12])}"
              f"{'...' if len(cv) > 12 else ''}")
    else:
        print(f"\n  ğŸ—‚ï¸  åˆ†ç±»å½’æ¡£: (æœªåˆ›å»º)")

    # çŸ¥è¯†åº“
    kb = paths["kb_db"]
    if kb.exists():
        try:
            conn = sqlite3.connect(f"file:{kb}?mode=ro", uri=True)
            c = conn.cursor()
            c.execute("SELECT COUNT(*) FROM documents")
            nd = c.fetchone()[0]
            c.execute("SELECT COUNT(*) FROM chunks")
            nc = c.fetchone()[0]
            conn.close()
            print(f"\n  ğŸ“š çŸ¥è¯†åº“: {kb}")
            print(f"     æ–‡æ¡£: {nd} | åˆ†å—: {nc}")
        except Exception as e:
            print(f"\n  ğŸ“š çŸ¥è¯†åº“: è¯»å–å¤±è´¥ ({e})")
    else:
        print(f"\n  ğŸ“š çŸ¥è¯†åº“: (æœªåˆ›å»º)")

    fi = paths["kb_faiss"]
    if fi.exists():
        print(f"  ğŸ§  FAISS: {fi.stat().st_size/(1024**2):.1f} MB")
    else:
        print(f"  ğŸ§  FAISS: (æœªåˆ›å»º)")

    kw = paths["keywords_db"]
    if kw.exists():
        try:
            kd = json.loads(kw.read_text(encoding="utf-8"))
            print(f"\n  ğŸ”‘ å…³é”®è¯: {len(kd.get('keywords', {}))} ä¸ª, "
                  f"{kd.get('statistics', {}).get('total_searches', 0)} æ¬¡æœç´¢")
        except Exception:
            print(f"\n  ğŸ”‘ å…³é”®è¯åº“: è§£æå¤±è´¥")
    else:
        print(f"\n  ğŸ”‘ å…³é”®è¯åº“: (æœªåˆ›å»º)")

    po = paths["pack_output"]
    if po.exists() and po.is_dir():
        recent = sorted(d.name for d in po.iterdir() if d.is_dir())[-5:]
        print(f"\n  ğŸ“¦ æ‰“åŒ…è¾“å‡º: {po}")
        if recent:
            print(f"     æœ€è¿‘: {', '.join(recent)}")

    dl_cfg = config.get("downloader", {})
    print(f"\n  âš™ï¸  é…ç½®:")
    print(f"     æ¯è½®å…³é”®è¯:   {dl_cfg.get('keywords_per_round', 10)}")
    print(f"     æ¯è¯ç»“æœ:     {dl_cfg.get('results_per_keyword', 20)}")
    print(f"     æ–‡ä»¶ä¸Šé™:     {dl_cfg.get('total_files_limit', 10000)}")
    print(f"     å®¹é‡ä¸Šé™:     {dl_cfg.get('total_size_limit_gb', 100)} GB")
    print(f"     ç™½åå•åŸŸå:   {len(dl_cfg.get('domain_whitelist', []))} ä¸ª")
    print(f"\n{'â•'*70}\n")


def cmd_config(config: Dict):
    print(f"\n{'â•'*70}")
    print("  å½“å‰é…ç½®")
    print(f"{'â•'*70}\n")
    print(json.dumps(config, indent=2, ensure_ascii=False))
    print(f"\n{'â•'*70}\n")


# ============================================================================
# äº¤äº’æ¨¡å¼ (REPL)
# ============================================================================

def interactive_mode(config: Dict, paths: Dict[str, Path],
                     logger: logging.Logger):
    print(BANNER)
    print(HELP_TEXT)

    while True:
        try:
            raw = input("\n  alaidocs> ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n  ğŸ‘‹ å†è§!")
            break

        if not raw:
            continue

        parts = raw.split()
        cmd = parts[0].lower()
        rest = parts[1:]

        if cmd in ("quit", "exit", "q"):
            print("  ğŸ‘‹ å†è§!")
            break
        elif cmd in ("help", "h", "?"):
            print(HELP_TEXT)
        elif cmd == "init":
            cmd_init(config, logger)
            # é‡æ–°åŠ è½½è·¯å¾„
            paths.update(resolve_paths(load_config()))
        elif cmd == "collect":
            q = " ".join(rest) if rest else ""
            cmd_collect(config, paths, logger, query=q)
        elif cmd == "classify":
            cmd_classify(config, paths, logger, once=True)
        elif cmd in ("build-kb", "buildkb", "kb"):
            rebuild = "--rebuild" in rest
            cmd_build_kb(config, paths, logger, rebuild=rebuild)
        elif cmd == "pack":
            q, top_n, ms = _parse_pack_args(rest)
            if q:
                cmd_pack(q, config, paths, logger, max_docs=top_n, min_score=ms)
            else:
                print("  âš ï¸  ç”¨æ³•: pack <æŸ¥è¯¢> [--top N] [--min-score 0.5]")
        elif cmd == "run":
            q, top_n, ms, rds = _parse_run_args(rest)
            if q:
                cmd_run(q, config, paths, logger, rounds=rds,
                        max_docs=top_n, min_score=ms)
            else:
                print("  âš ï¸  ç”¨æ³•: run <æŸ¥è¯¢ä¸»é¢˜> [--top N] [--min-score 0.5] [--rounds N]")
                print("  ä¾‹: run \"buck converter efficiency\"")
        elif cmd == "status":
            cmd_status(config, paths, logger)
        elif cmd == "config":
            cmd_config(config)
        elif "æ›´æ–°èµ„æ–™" in raw or "å¼€å§‹é‡‡é›†" in raw:
            cmd_collect(config, paths, logger)
        elif any(k in raw for k in ("æ£€ç´¢", "æœç´¢", "æŸ¥æ‰¾")):
            topic = raw
            for prefix in ("å¸®æˆ‘æ£€ç´¢", "æ£€ç´¢", "æœç´¢", "æŸ¥æ‰¾"):
                if raw.startswith(prefix):
                    topic = raw[len(prefix):].strip()
                    break
            if topic:
                cmd_pack(topic, config, paths, logger)
        elif "æ•´ç†" in raw or "åˆ†ç±»" in raw:
            cmd_classify(config, paths, logger, once=True)
        elif "çŠ¶æ€" in raw:
            cmd_status(config, paths, logger)
        else:
            print(f"  ğŸ’¡ å°† \"{raw}\" ä½œä¸ºæ£€ç´¢æŸ¥è¯¢...")
            cmd_pack(raw, config, paths, logger)


def _parse_pack_args(rest):
    parts, top, ms, i = [], 20, 0.0, 0
    while i < len(rest):
        if rest[i] == "--top" and i + 1 < len(rest):
            top = int(rest[i+1]); i += 2
        elif rest[i] == "--min-score" and i + 1 < len(rest):
            ms = float(rest[i+1]); i += 2
        else:
            parts.append(rest[i]); i += 1
    return " ".join(parts), top, ms


def _parse_run_args(rest):
    parts, top, ms, rds, i = [], 20, 0.0, 1, 0
    while i < len(rest):
        if rest[i] == "--top" and i + 1 < len(rest):
            top = int(rest[i+1]); i += 2
        elif rest[i] == "--min-score" and i + 1 < len(rest):
            ms = float(rest[i+1]); i += 2
        elif rest[i] == "--rounds" and i + 1 < len(rest):
            rds = int(rest[i+1]); i += 2
        else:
            parts.append(rest[i]); i += 1
    return " ".join(parts), top, ms, rds


# ============================================================================
# CLI å…¥å£
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="AlaiDocs â€” DC-DC çŸ¥è¯†åº“ä¸€ç«™å¼ CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python alaidocs.py init                              # é¦–æ¬¡åˆå§‹åŒ–
  python alaidocs.py                                   # äº¤äº’æ¨¡å¼
  python alaidocs.py collect "buck converter"           # é‡‡é›† PDF
  python alaidocs.py classify                          # è‡ªåŠ¨åˆ†ç±»
  python alaidocs.py build-kb                          # æ„å»ºçŸ¥è¯†åº“
  python alaidocs.py pack "é™å‹å˜æ¢å™¨çƒ­ç®¡ç†"             # æ£€ç´¢æ‰“åŒ…
  python alaidocs.py run "LLC resonant converter"      # å…¨æµç¨‹ (ä¸€æ¡å‘½ä»¤)
  python alaidocs.py run "GaNæ•ˆç‡" --gemini --top 15   # å…¨æµç¨‹ + Gemini å¢å¼º
  python alaidocs.py status                            # ç³»ç»ŸçŠ¶æ€
        """,
    )

    sub = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")

    sub.add_parser("init", help="é¦–æ¬¡åˆå§‹åŒ–")

    sp = sub.add_parser("collect", help="æ™ºèƒ½é‡‡é›† (DDG æœç´¢ + PDF ä¸‹è½½)")
    sp.add_argument("query", nargs="?", default="", help="æœç´¢ä¸»é¢˜ (å¦‚ \"buck converter\")")
    sp.add_argument("--rounds", type=int, default=0)
    sp.add_argument("--gemini", action="store_true",
                    help="ä½¿ç”¨ Gemini Chrome ç”Ÿæˆå…³é”®è¯ (éœ€ç™»å½•)")

    sub.add_parser("classify", help="è‡ªåŠ¨ 4D åˆ†ç±»")

    sp = sub.add_parser("build-kb", help="æ„å»º/æ›´æ–°çŸ¥è¯†åº“")
    sp.add_argument("--rebuild", action="store_true", help="æ¸…ç©ºé‡å»ºï¼ˆé»˜è®¤å¢é‡ï¼‰")
    sp.add_argument("--repair", action="store_true", help="ä¿®å¤æŸåçš„ FTS5 ç´¢å¼•")

    sp = sub.add_parser("pack", help="æ£€ç´¢æ‰“åŒ…")
    sp.add_argument("query")
    sp.add_argument("--top", type=int, default=20)
    sp.add_argument("--min-score", type=float, default=0.0)
    sp.add_argument("-y", "--yes", action="store_true")

    sp = sub.add_parser("run", help="å…¨æµç¨‹ (é‡‡é›†â†’åˆ†ç±»â†’å»ºåº“â†’æ‰“åŒ…)")
    sp.add_argument("query")
    sp.add_argument("--rounds", type=int, default=1)
    sp.add_argument("--top", type=int, default=20)
    sp.add_argument("--min-score", type=float, default=0.0)
    sp.add_argument("--gemini", action="store_true",
                    help="ä½¿ç”¨ Gemini Chrome ç”Ÿæˆå…³é”®è¯")

    sub.add_parser("status", help="ç³»ç»ŸçŠ¶æ€")
    sub.add_parser("config", help="æ˜¾ç¤ºé…ç½®")

    parser.add_argument("-c", "--config", type=Path, default=None)
    parser.add_argument("--debug", action="store_true")

    args = parser.parse_args()
    logger = setup_logging(args.debug)
    config = load_config(args.config)
    paths = resolve_paths(config)

    if args.command is None:
        interactive_mode(config, paths, logger)
    elif args.command == "init":
        cmd_init(config, logger)
    elif args.command == "collect":
        if not ensure_initialized(paths, logger):
            return
        cmd_collect(config, paths, logger,
                    query=args.query, rounds=args.rounds,
                    use_gemini=args.gemini)
    elif args.command == "classify":
        if not ensure_initialized(paths, logger):
            return
        cmd_classify(config, paths, logger, once=True)
    elif args.command == "build-kb":
        if not ensure_initialized(paths, logger):
            return
        cmd_build_kb(config, paths, logger, rebuild=args.rebuild, repair=args.repair)
    elif args.command == "pack":
        if not ensure_initialized(paths, logger):
            return
        cmd_pack(args.query, config, paths, logger,
                 max_docs=args.top, min_score=args.min_score,
                 auto_confirm=args.yes)
    elif args.command == "run":
        if not ensure_initialized(paths, logger):
            return
        cmd_run(args.query, config, paths, logger,
                rounds=args.rounds, max_docs=args.top,
                min_score=args.min_score,
                use_gemini=args.gemini)
    elif args.command == "status":
        cmd_status(config, paths, logger)
    elif args.command == "config":
        cmd_config(config)


if __name__ == "__main__":
    main()
