#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AlaiDocs â€” DC-DC çŸ¥è¯†åº“ä¸€ç«™å¼ CLI
è®©æ¯ä¸ªäººéƒ½èƒ½ç”¨ä¸€å¥è¯å»ºç«‹ä¸“ä¸šçš„ DC-DC æŠ€æœ¯çŸ¥è¯†åº“ã€‚

ç”¨æ³•:
  python alaidocs.py init                  # é¦–æ¬¡åˆå§‹åŒ– (åˆ›å»ºç›®å½•+é…ç½®)
  python alaidocs.py                       # äº¤äº’æ¨¡å¼
  python alaidocs.py collect "buck converter"  # é‡‡é›† PDF (DDG æœç´¢+ä¸‹è½½)
  python alaidocs.py classify              # è‡ªåŠ¨ 4D åˆ†ç±»
  python alaidocs.py build-kb              # æ„å»ºçŸ¥è¯†åº“
  python alaidocs.py pack "é™å‹å˜æ¢å™¨çƒ­ç®¡ç†" # æ£€ç´¢æ‰“åŒ…
  python alaidocs.py run "GaNæ•ˆç‡ä¼˜åŒ–"     # å…¨æµç¨‹: é‡‡é›†â†’åˆ†ç±»â†’å»ºåº“â†’æ‰“åŒ…
  python alaidocs.py status                # ç³»ç»ŸçŠ¶æ€

ç‰ˆæœ¬: 3.1.0 (portable â€” æ— ç¡¬ç¼–ç è·¯å¾„)
"""

import argparse
import json
import logging
import os
import random
import re as _re
import shutil
import sqlite3
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urlparse as _urlparse

# Windows ç»ˆç«¯ UTF-8 ç¼–ç  (emoji æ”¯æŒ)
if sys.platform == "win32":
    os.environ.setdefault("PYTHONIOENCODING", "utf-8")
    try:
        sys.stdout.reconfigure(encoding="utf-8", errors="replace")
        sys.stderr.reconfigure(encoding="utf-8", errors="replace")
    except Exception:
        pass

# ============================================================================
# è·¯å¾„çº¦å®š â€” å…¨éƒ¨ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼Œé›¶ç¡¬ç¼–ç 
# ============================================================================

PROJECT_ROOT = Path(__file__).resolve().parent

# é»˜è®¤ç›®å½•ç»“æ„ (å…¨éƒ¨å¯é€šè¿‡ alaidocs_config.json è¦†ç›–)
DEFAULT_DIRS = {
    "download_dir":   "data/downloads",       # ä¸‹è½½ç¼“å†²åŒº
    "classified_dir": "data/classified",       # 4D åˆ†ç±»å½’æ¡£
    "kb_dir":         "data/kb",               # çŸ¥è¯†åº“ (SQLite + FAISS)
    "pack_output":    "data/packed",           # NotebookLM æ‰“åŒ…è¾“å‡º
    "keywords_db":    "data/keywords.json",    # å…³é”®è¯æ•°æ®åº“
}

USER_CONFIG_FILE = PROJECT_ROOT / "alaidocs_config.json"
TEMPLATE_CONFIG  = PROJECT_ROOT / "integrated_config.json"

BANNER = r"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           AlaiDocs â€” DC-DC çŸ¥è¯†åº“è‡ªåŠ¨åŒ–ç³»ç»Ÿ                  â•‘
â•‘   Collect â†’ Classify (4D) â†’ Pack â†’ NotebookLM              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

HELP_TEXT = """
  å¯ç”¨å‘½ä»¤:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  init                 é¦–æ¬¡åˆå§‹åŒ– (åˆ›å»ºç›®å½•å’Œé…ç½®æ–‡ä»¶)
  collect <ä¸»é¢˜>       é‡‡é›† PDF (å‚å•†è½®æ¢ + DDG æœç´¢ + ä¸‹è½½)
  classify             è‡ªåŠ¨æ•´ç† (4D åˆ†ç±»å½’æ¡£)
  build-kb             æ„å»º/æ›´æ–°çŸ¥è¯†åº“ (ä»åˆ†ç±»ç›®å½•)
  pack <æŸ¥è¯¢>          æ£€ç´¢æ‰“åŒ… (é«˜ç½®ä¿¡åº¦ç­›é€‰ â†’ NotebookLM)
  run  <æŸ¥è¯¢>          å…¨æµç¨‹   (é‡‡é›† â†’ åˆ†ç±» â†’ å»ºåº“ â†’ æ‰“åŒ…)
  status               ç³»ç»ŸçŠ¶æ€æ‘˜è¦
  config               æ˜¾ç¤ºå½“å‰é…ç½®
  help                 æ˜¾ç¤ºå¸®åŠ©
  quit / exit          é€€å‡º
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  collect ç­–ç•¥ (å­¦ä¹ è‡ª vendor_downloader):
    site:ti.com <keyword> filetype:pdf   ç²¾å‡†æœç´¢å‚å•†å®˜ç½‘
    å‚å•†è½®æ¢: æ¯ä¸ªå…³é”®è¯è½®æµæœç´¢å„å‚å•†ï¼Œé¿å…è¿ç»­è¯·æ±‚åŒä¸€ç«™ç‚¹
    requests ä¸‹è½½: ç›´æ¥é«˜é€Ÿä¸‹è½½ PDF

  collect ç”¨æ³•:
    collect "buck converter"                 æœç´¢æ‰€æœ‰å‚å•†
    collect "buck converter" --vendors TI,ST ä»…æœç´¢ TI å’Œ ST
    collect --gemini                         Gemini AI ç”Ÿæˆè¯ + å‚å•†æœç´¢
    collect --gemini --rounds 3              Gemini æŒç»­é‡‡é›† 3 è½®

  ç¤ºä¾‹:
    collect buck converter                   é‡‡é›† buck ç›¸å…³ PDF
    collect "GaN efficiency" --vendors TI    ä»…æœ TI
    run "LLC resonant" --top 10              å…¨æµç¨‹ï¼Œæ‰“åŒ…å‰ 10 ç¯‡
    pack é™å‹å˜æ¢å™¨çƒ­ç®¡ç†                     æ£€ç´¢æ‰“åŒ…
"""


# ============================================================================
# é…ç½®ç®¡ç† â€” é¦–æ¬¡è¿è¡Œè‡ªåŠ¨ç”Ÿæˆç”¨æˆ·é…ç½®
# ============================================================================

def resolve_paths(config: Dict) -> Dict[str, Path]:
    """
    ä»é…ç½®è§£ææ‰€æœ‰è·¯å¾„ (ç›¸å¯¹è·¯å¾„åŸºäº PROJECT_ROOT)ã€‚
    ä¼˜å…ˆçº§: alaidocs_config.json > integrated_config.json > å†…ç½®é»˜è®¤å€¼
    """
    paths_cfg = config.get("paths", {})
    resolved = {}
    for key, default_rel in DEFAULT_DIRS.items():
        raw = paths_cfg.get(key, default_rel)
        p = Path(raw)
        if not p.is_absolute():
            p = PROJECT_ROOT / p
        resolved[key] = p
    # æ´¾ç”Ÿè·¯å¾„
    resolved["kb_db"]    = resolved["kb_dir"] / "kb.sqlite"
    resolved["kb_faiss"] = resolved["kb_dir"] / "kb.faiss"
    return resolved


def load_config(config_path: Path = None) -> Dict:
    """
    åŠ è½½é…ç½®ã€‚åˆå¹¶é“¾:
    å†…ç½®é»˜è®¤ â†’ integrated_config.json â†’ alaidocs_config.json â†’ CLI å‚æ•°
    """
    config = {}

    # 1) integrated_config.json (é¡¹ç›®çº§æ¨¡æ¿ï¼Œå…¥ Git)
    if TEMPLATE_CONFIG.exists():
        with open(TEMPLATE_CONFIG, "r", encoding="utf-8-sig") as f:
            config = json.load(f)

    # 2) alaidocs_config.json (ç”¨æˆ·çº§è¦†ç›–ï¼Œä¸å…¥ Git)
    user_cfg_path = config_path or USER_CONFIG_FILE
    if user_cfg_path.exists():
        with open(user_cfg_path, "r", encoding="utf-8-sig") as f:
            user_cfg = json.load(f)
        _deep_merge(config, user_cfg)

    return config


def _deep_merge(base: dict, override: dict):
    for k, v in override.items():
        if k in base and isinstance(base[k], dict) and isinstance(v, dict):
            _deep_merge(base[k], v)
        else:
            base[k] = v


def ensure_initialized(paths: Dict[str, Path], logger: logging.Logger) -> bool:
    """æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–ï¼›æœªåˆå§‹åŒ–åˆ™å¼•å¯¼ç”¨æˆ·ã€‚"""
    if paths["classified_dir"].exists() and paths["kb_dir"].exists():
        return True
    logger.warning("âš ï¸  é¡¹ç›®å°šæœªåˆå§‹åŒ–ã€‚è¯·å…ˆè¿è¡Œ:")
    logger.warning("   python alaidocs.py init")
    return False


# ============================================================================
# æ—¥å¿—
# ============================================================================

def setup_logging(debug: bool = False) -> logging.Logger:
    level = logging.DEBUG if debug else logging.INFO
    root = logging.getLogger()
    for h in root.handlers[:]:
        root.removeHandler(h)

    console = logging.StreamHandler(sys.stdout)
    console.setLevel(level)
    console.setFormatter(logging.Formatter(
        "%(asctime)s â”‚ %(message)s", datefmt="%H:%M:%S"
    ))

    log_file = PROJECT_ROOT / "alaidocs.log"
    fh = logging.FileHandler(str(log_file), encoding="utf-8")
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(name)s - %(message)s"
    ))

    root.setLevel(logging.DEBUG)
    root.addHandler(console)
    root.addHandler(fh)
    return logging.getLogger("alaidocs")


# ============================================================================
# å‘½ä»¤: init
# ============================================================================

def cmd_init(config: Dict, logger: logging.Logger):
    """é¦–æ¬¡åˆå§‹åŒ–: åˆ›å»ºç›®å½•ç»“æ„ + ç”Ÿæˆç”¨æˆ·é…ç½®æ–‡ä»¶"""
    print(BANNER)
    print("  ğŸ”§ æ­£åœ¨åˆå§‹åŒ– AlaiDocs é¡¹ç›®...\n")

    paths = resolve_paths(config)

    # 1) åˆ›å»ºç›®å½•
    for name, p in paths.items():
        if name.endswith("_db") or name.endswith("_faiss") or name == "keywords_db":
            p.parent.mkdir(parents=True, exist_ok=True)
        else:
            p.mkdir(parents=True, exist_ok=True)
        print(f"  âœ… {name:20s} â†’ {p}")

    # 2) ç”Ÿæˆ alaidocs_config.json
    if not USER_CONFIG_FILE.exists():
        try:
            rel_paths = {
                k: str(v.relative_to(PROJECT_ROOT)).replace("\\", "/")
                for k, v in paths.items()
                if k in DEFAULT_DIRS
            }
        except ValueError:
            rel_paths = {k: str(v) for k, v in paths.items() if k in DEFAULT_DIRS}

        user_config = {
            "_comment": [
                "AlaiDocs ç”¨æˆ·é…ç½® â€” æ­¤æ–‡ä»¶ä¸å…¥ Git",
                "ç¼–è¾‘ paths è‡ªå®šä¹‰ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨ç»å¯¹è·¯å¾„"
            ],
            "paths": rel_paths,
        }
        with open(USER_CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(user_config, f, indent=2, ensure_ascii=False)
        print(f"\n  ğŸ“ å·²ç”Ÿæˆç”¨æˆ·é…ç½®: {USER_CONFIG_FILE.name}")
    else:
        print(f"\n  ğŸ“ ç”¨æˆ·é…ç½®å·²å­˜åœ¨: {USER_CONFIG_FILE.name}")

    # 3) åˆå§‹åŒ–ç©ºå…³é”®è¯åº“
    kw_path = paths["keywords_db"]
    if not kw_path.exists():
        with open(kw_path, "w", encoding="utf-8") as f:
            json.dump({
                "keywords": {},
                "statistics": {
                    "total_keywords_used": 0,
                    "total_searches": 0,
                    "total_files_downloaded": 0,
                    "last_updated": None,
                },
            }, f, indent=2, ensure_ascii=False)
        print(f"  ğŸ“ å·²åˆ›å»ºå…³é”®è¯åº“: {kw_path.name}")

    # 4) æ›´æ–° .gitignore
    gitignore = PROJECT_ROOT / ".gitignore"
    needed = [
        "alaidocs_config.json",
        "alaidocs.log",
        "data/",
        "*.db",
        "__pycache__/",
        ".venv/",
    ]
    existing = set()
    if gitignore.exists():
        existing = set(gitignore.read_text(encoding="utf-8").splitlines())
    missing = [e for e in needed if e not in existing]
    if missing:
        with open(gitignore, "a", encoding="utf-8") as f:
            f.write("\n# AlaiDocs runtime (auto-generated)\n")
            for e in missing:
                f.write(e + "\n")
        print(f"  ğŸ“ å·²æ›´æ–° .gitignore")

    print(f"""
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… åˆå§‹åŒ–å®Œæˆï¼ä¸‹ä¸€æ­¥:

  1. å®‰è£…ä¾èµ–:
     pip install -r requirements.txt

  2. (æµè§ˆå™¨é‡‡é›†éœ€è¦) é¢å¤–å®‰è£…:
     pip install -r requirements_browser.txt

  3. å¯åŠ¨:
     python alaidocs.py              # äº¤äº’æ¨¡å¼
     python alaidocs.py collect      # å¼€å§‹é‡‡é›†
     python alaidocs.py status       # æŸ¥çœ‹çŠ¶æ€

  4. (å¯é€‰) ç¼–è¾‘ alaidocs_config.json è‡ªå®šä¹‰è·¯å¾„
     ä¾‹å¦‚æŒ‡å‘å·²æœ‰çš„ PDF ç›®å½•æˆ–çŸ¥è¯†åº“

  ç›®å½•ç»“æ„:
     data/
     â”œâ”€â”€ downloads/       # é‡‡é›†ç¼“å†²åŒº
     â”œâ”€â”€ classified/      # 4D åˆ†ç±»å½’æ¡£
     â”‚   â””â”€â”€ TI/datasheet/power_ic/buck/xxx.pdf
     â”œâ”€â”€ kb/              # çŸ¥è¯†åº“
     â”‚   â”œâ”€â”€ kb.sqlite
     â”‚   â””â”€â”€ kb.faiss
     â”œâ”€â”€ packed/          # NotebookLM æ‰“åŒ…
     â””â”€â”€ keywords.json    # å…³é”®è¯åº“
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
""")


# ============================================================================
# å‘½ä»¤: collect â€” æ™ºèƒ½é‡‡é›† (å­¦ä¹ è‡ª vendor_downloader.py)
# ============================================================================

# å‚å•†å®˜ç½‘åŸŸå â€” site:æœç´¢é™å®šç¬¦ï¼Œè´¨é‡æ›´é«˜
VENDOR_SITES = {
    # ç»¼åˆå¤§å‚
    "TI":        "ti.com",
    "ADI":       "analog.com",
    "ST":        "st.com",
    "Infineon":  "infineon.com",
    "onsemi":    "onsemi.com",
    "NXP":       "nxp.com",
    "Microchip": "microchip.com",
    "Renesas":   "renesas.com",
    "ROHM":      "rohm.com",
    "Vishay":    "vishay.com",
    # ç”µæºä¸“ç²¾å‚å•†
    "MPS":       "monolithicpower.com",
    "Diodes":    "diodes.com",
    "PI":        "power.com",
    "Navitas":   "navitassemi.com",
    "Richtek":   "richtek.com",
    "AOS":       "aosmd.com",
    "Vicor":     "vicorpower.com",
}

# ä¸‰ä¸ªæ‰¹æ¬¡ (å¯å¹¶è¡Œå¼€ä¸‰ä¸ªç»ˆç«¯)
VENDOR_BATCH1 = ["TI", "ADI", "ST", "Infineon", "onsemi"]
VENDOR_BATCH2 = ["NXP", "Microchip", "Renesas", "MPS", "ROHM"]
VENDOR_BATCH3 = ["Vishay", "Diodes", "PI", "Navitas", "Richtek", "AOS", "Vicor"]


def cmd_collect(config: Dict, paths: Dict[str, Path],
                logger: logging.Logger, query: str = "",
                rounds: int = 0, use_gemini: bool = False,
                vendors: str = "") -> Dict:
    """
    æ™ºèƒ½é‡‡é›† â€” å­¦ä¹ è‡ª vendor_downloader.py çš„æˆç†Ÿç­–ç•¥:

      site:ti.com <keyword> filetype:pdf   ç²¾å‡†æœç´¢å‚å•†å®˜ç½‘
      å‚å•†è½®æ¢: æ¯ä¸ªå…³é”®è¯è½®æµæœç´¢å„å‚å•†ï¼Œé¿å…è¿ç»­è¯·æ±‚åŒä¸€ç«™ç‚¹
      requests ä¸‹è½½: ç›´æ¥é«˜é€Ÿä¸‹è½½ PDF

    æ¨¡å¼:
      1. collect <ä¸»é¢˜>           â†’ æœ¬åœ°å…³é”®è¯æ‰©å±• + å‚å•†è½®æ¢æœç´¢
      2. collect <ä¸»é¢˜> --gemini  â†’ Gemini AI ç”Ÿæˆè¯ + å‚å•†è½®æ¢æœç´¢
      3. collect --vendors TI,ST  â†’ ä»…æœç´¢æŒ‡å®šå‚å•†
    """
    stats = {"downloaded": 0, "failed": 0, "skipped": 0, "keywords": 0}

    download_dir = paths["download_dir"]
    download_dir.mkdir(parents=True, exist_ok=True)

    logger.info("ğŸš€ å¯åŠ¨æ™ºèƒ½é‡‡é›†ç³»ç»Ÿ")
    logger.info(f"   ä¸‹è½½ç›®å½•: {download_dir}")
    logger.info("   ç­–ç•¥: site:vendor.com + å‚å•†è½®æ¢ + requests ä¸‹è½½")

    # â”€â”€ Gemini æ¨¡å¼: å…³é”®è¯ç”± Gemini AI ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if use_gemini:
        return _collect_gemini_mode(config, paths, logger, query, rounds)

    # â”€â”€ é€‰æ‹©å‚å•† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if vendors:
        vendor_names = [v.strip() for v in vendors.split(",")]
        vendor_map = {k.upper(): k for k in VENDOR_SITES}
        selected = {}
        for v in vendor_names:
            key = vendor_map.get(v.upper())
            if key:
                selected[key] = VENDOR_SITES[key]
            else:
                logger.warning(f"   âš ï¸  æœªçŸ¥å‚å•†: {v} (è·³è¿‡)")
        if not selected:
            logger.error(f"âŒ æ— æœ‰æ•ˆå‚å•†ã€‚å¯ç”¨: {', '.join(VENDOR_SITES.keys())}")
            return stats
        vendors_to_use = selected
    else:
        vendors_to_use = VENDOR_SITES
    logger.info(f"   å‚å•†: {len(vendors_to_use)} ä¸ª â€” {', '.join(vendors_to_use.keys())}")

    # â”€â”€ å…³é”®è¯ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if query:
        from keyword_expander import expand_query
        keywords = expand_query(query, max_keywords=20)
        logger.info(f"ğŸ”‘ ä» \"{query}\" æ‰©å±•å‡º {len(keywords)} ä¸ªæœç´¢å…³é”®è¯")
    else:
        focus = config.get("downloader", {}).get("focus_areas", [])
        if focus:
            keywords = focus.copy()
            logger.info(f"ğŸ”‘ ä½¿ç”¨é…ç½® focus_areas: {len(keywords)} ä¸ªå…³é”®è¯")
        else:
            logger.error("âŒ è¯·æä¾›æŸ¥è¯¢ä¸»é¢˜: collect <æŸ¥è¯¢>")
            return stats

    stats["keywords"] = len(keywords)
    for i, kw in enumerate(keywords[:10], 1):
        logger.info(f"   {i:2d}. {kw}")
    if len(keywords) > 10:
        logger.info(f"   ... å…± {len(keywords)} ä¸ª")

    # â”€â”€ å‚å•†è½®æ¢æœç´¢ + requests ä¸‹è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    return _vendor_rotate_search(keywords, vendors_to_use, download_dir,
                                 config, logger, stats)


def _vendor_rotate_search(keywords: List[str], vendors: Dict[str, str],
                          download_dir: Path, config: Dict,
                          logger: logging.Logger, stats: Dict) -> Dict:
    """
    å‚å•†è½®æ¢æœç´¢ç­–ç•¥ (å­¦ä¹ è‡ª vendor_downloader.py):
      å¤–å±‚å¾ªç¯: å…³é”®è¯
      å†…å±‚å¾ªç¯: è½®æµæ¯ä¸ªå‚å•†
      æŸ¥è¯¢:     site:vendor.com <keyword> filetype:pdf
      ä¸‹è½½:     requests (é«˜é€Ÿ, ä¸éœ€è¦ Chrome)
    """
    import requests

    try:
        from ddgs import DDGS
    except ImportError:
        try:
            from duckduckgo_search import DDGS
        except ImportError:
            logger.error("âŒ éœ€è¦ DDG æœç´¢åº“: pip install ddgs")
            return stats

    dl_cfg = config.get("downloader", {})
    max_results = dl_cfg.get("results_per_keyword", 20)
    seen_urls: set = set()

    logger.info(f"\n{'='*60}")
    logger.info(f"å¼€å§‹å‚å•†è½®æ¢æœç´¢ â€” {len(keywords)} å…³é”®è¯ Ã— {len(vendors)} å‚å•†")
    logger.info(f"{'='*60}\n")

    for ki, keyword in enumerate(keywords, 1):
        logger.info(f"\n{'â”€'*60}")
        logger.info(f"å…³é”®è¯ [{ki}/{len(keywords)}]: {keyword}")
        logger.info(f"{'â”€'*60}")

        # å†…å±‚: è½®æµæ¯ä¸ªå‚å•†
        for vendor, site in vendors.items():
            query = f"site:{site} {keyword} filetype:pdf"
            logger.info(f"  {vendor} ({site})")

            # DDG æœç´¢ (æŠ‘åˆ¶ SearXNG å†…éƒ¨æ—¥å¿—)
            try:
                _prev = logging.getLogger().level
                logging.getLogger().setLevel(logging.WARNING)
                ddgs = DDGS()
                results = list(ddgs.text(query, max_results=max_results))
                logging.getLogger().setLevel(_prev)
            except Exception as e:
                logging.getLogger().setLevel(logging.INFO)
                logger.warning(f"    âš ï¸  æœç´¢å¤±è´¥: {e}")
                time.sleep(random.uniform(1, 2))
                continue

            if not results:
                logger.info(f"    (æ— ç»“æœ)")
                time.sleep(random.uniform(0.5, 1))
                continue

            # ç­›é€‰ PDF URL
            pdf_items = []
            for r in results:
                url = r.get("href", r.get("url", ""))
                if not url or url in seen_urls:
                    continue
                seen_urls.add(url)
                if _is_pdf_url(url) and site in url:
                    pdf_items.append((r.get("title", "Untitled"), url))

            if not pdf_items:
                logger.info(f"    æ‰¾åˆ° {len(results)} ä¸ªç»“æœ, 0 ä¸ª PDF")
                time.sleep(random.uniform(0.5, 1))
                continue

            logger.info(f"    æ‰¾åˆ° {len(pdf_items)} ä¸ª PDF, å¼€å§‹ä¸‹è½½...")
            vendor_dir = download_dir / vendor.lower()
            vendor_dir.mkdir(parents=True, exist_ok=True)

            downloaded_this = 0
            for title, url in pdf_items:
                fname = _safe_filename(title)
                if not fname.lower().endswith(".pdf"):
                    fname += ".pdf"
                fpath = vendor_dir / fname

                if fpath.exists():
                    stats["skipped"] += 1
                    continue

                ok = _download_pdf(url, fpath, logger)
                if ok:
                    stats["downloaded"] += 1
                    downloaded_this += 1
                else:
                    stats["failed"] += 1

                time.sleep(random.uniform(0.3, 0.5))

            if downloaded_this > 0:
                logger.info(f"    ğŸ“¥ ä¸‹è½½: {downloaded_this} ä¸ª")

            # æ¯ä¸ªå‚å•†åçŸ­æš‚å»¶è¿Ÿ
            time.sleep(random.uniform(1, 2))

        # æ¯ä¸ªå…³é”®è¯åæ˜¾ç¤ºè¿›åº¦
        logger.info(f"  ğŸ“Š ç´¯è®¡: {stats['downloaded']} ä¸‹è½½ / {stats['failed']} å¤±è´¥ / {stats['skipped']} è·³è¿‡")
        time.sleep(random.uniform(2, 3))

    logger.info(f"\n{'â”'*60}")
    logger.info(f"ğŸ“Š é‡‡é›†å®Œæˆ")
    logger.info(f"   å…³é”®è¯:  {stats['keywords']}")
    logger.info(f"   å‚å•†:    {len(vendors)}")
    logger.info(f"   ä¸‹è½½:    {stats['downloaded']}")
    logger.info(f"   å¤±è´¥:    {stats['failed']}")
    logger.info(f"   è·³è¿‡:    {stats['skipped']}")
    logger.info(f"   ç›®å½•:    {download_dir}")
    logger.info(f"{'â”'*60}")
    return stats


def _is_pdf_url(url: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦æ˜¯ PDF URL"""
    u = url.lower()
    if u.endswith(".pdf"):
        return True
    pdf_hints = ["/pdf/", "/lit/", "/datasheet/", "/ds/",
                 "filetype=pdf", "type=pdf", ".pdf?",
                 "/media/", "/technical-documentation/",
                 "/application-note/", "/an/"]
    return any(h in u for h in pdf_hints)


def _safe_filename(title: str, max_length: int = 120) -> str:
    """æ¸…ç†æ–‡ä»¶å"""
    name = _re.sub(r'[<>:"/\\|?*]', '', title)
    name = name.strip(". ")
    if len(name) > max_length:
        name = name[:max_length]
    return name or "untitled"


def _download_pdf(url: str, fpath: Path, logger: logging.Logger) -> bool:
    """requests ä¸‹è½½ PDF â€” å¸¦ SSL é‡è¯•"""
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry

    session = requests.Session()
    retries = Retry(total=2, backoff_factor=0.5,
                    status_forcelist=[500, 502, 503, 504])
    session.mount("https://", HTTPAdapter(max_retries=retries))

    headers = {
        "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/120.0.0.0 Safari/537.36"),
        "Accept": "application/pdf,*/*",
    }
    try:
        resp = session.get(url, headers=headers, timeout=30, stream=True)
        resp.raise_for_status()

        ct = resp.headers.get("Content-Type", "").lower()
        if "pdf" not in ct and "octet-stream" not in ct:
            logger.debug(f"    è·³è¿‡ (Content-Type: {ct}): {url}")
            return False

        with open(fpath, "wb") as f:
            total = 0
            for chunk in resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    total += len(chunk)

        # å¤ªå°çš„æ–‡ä»¶å¯èƒ½ä¸æ˜¯æœ‰æ•ˆ PDF
        if total < 1024:
            fpath.unlink(missing_ok=True)
            logger.debug(f"    è·³è¿‡ (å¤ªå° {total}B): {url}")
            return False

        logger.info(f"    âœ… {fpath.name} ({total // 1024}KB)")
        return True
    except Exception as e:
        fpath.unlink(missing_ok=True)
        logger.debug(f"    âŒ ä¸‹è½½å¤±è´¥: {e.__class__.__name__}: {e}")
        return False


def _infer_vendor(url: str, vendor_domains: Dict) -> Optional[str]:
    """ä» URL åŸŸåæ¨æ–­å‚å•†å"""
    from urllib.parse import urlparse
    try:
        domain = urlparse(url).netloc.lower()
        for vendor, domains in vendor_domains.items():
            if any(d in domain for d in domains):
                return vendor
    except Exception:
        pass
    return None


def _collect_gemini_mode(config: Dict, paths: Dict[str, Path],
                         logger: logging.Logger,
                         query: str = "", rounds: int = 0) -> Dict:
    """
    Gemini Chrome å®Œæ•´æ¨¡å¼:
      Chrome Gemini AI ç”Ÿæˆå…³é”®è¯ â†’ Chrome DuckDuckGo æœç´¢ â†’ PDF ä¸‹è½½

    æµç¨‹:
      1. å¯åŠ¨ Chrome æµè§ˆå™¨ (é¦–æ¬¡éœ€æ‰‹åŠ¨ç™»å½• Google Gemini)
      2. Gemini æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆä¸“ä¸šæœç´¢å…³é”®è¯
      3. Chrome è®¿é—® DuckDuckGo æœç´¢æ¯ä¸ªå…³é”®è¯
      4. è‡ªåŠ¨è¯†åˆ«å¹¶ä¸‹è½½ PDF æ–‡ä»¶
      5. æŒ‰å‚å•†è‡ªåŠ¨å½’æ¡£åˆ° data/downloads/<vendor>/

    é¦–æ¬¡ä½¿ç”¨:
      pip install -r requirements_browser.txt
      è¿è¡Œæ—¶ä¼šæ‰“å¼€ Chromeï¼Œæ‰‹åŠ¨ç™»å½• gemini.google.com
    """
    logger.info("ğŸŒ å®Œæ•´æ¨¡å¼: Chrome Gemini â†’ Chrome DuckDuckGo â†’ PDF")
    logger.info("   æµç¨‹: AI å…³é”®è¯ç”Ÿæˆ â†’ æµè§ˆå™¨æœç´¢ â†’ è‡ªåŠ¨ä¸‹è½½")
    try:
        from integrated_searcher import IntegratedSearcher
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        logger.error("   éœ€è¦å®‰è£…æµè§ˆå™¨ä¾èµ–:")
        logger.error("   pip install -r requirements_browser.txt")
        logger.error("")
        logger.error("   æˆ–ä½¿ç”¨è½»é‡æ¨¡å¼ (æ— éœ€ Chrome):")
        logger.error("   python alaidocs.py collect \"buck converter\"")
        return {"downloaded": 0, "failed": 0, "skipped": 0}

    # â”€â”€ æ„å»º IntegratedSearcher éœ€è¦çš„æ‰å¹³é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # alaidocs_config.json ç”¨ "downloader" é”®ï¼Œä½† IntegratedSearcher
    # çš„ _default_config() ç”¨æ‰å¹³é”®åï¼Œéœ€è¦åšæ˜ å°„
    dl_cfg = config.get("downloader", {})
    searcher_config = {
        # Gemini è®¾ç½®
        "gemini_headless":         dl_cfg.get("headless", True),
        "gemini_response_timeout": dl_cfg.get("response_timeout", 60),
        # å…³é”®è¯è®¾ç½®
        "keywords_per_round":      dl_cfg.get("keywords_per_round", 10),
        # æœç´¢è®¾ç½®
        "results_per_keyword":     dl_cfg.get("results_per_keyword", 20),
        "max_downloads_per_keyword": dl_cfg.get("max_downloads_per_keyword", 10),
        # é™åˆ¶
        "total_size_limit_gb":     dl_cfg.get("total_size_limit_gb", 50),
        "total_files_limit":       dl_cfg.get("total_files_limit", 5000),
        # å¾ªç¯æ§åˆ¶
        "round_interval":          dl_cfg.get("round_interval_seconds", 300),
        "min_files_per_round":     dl_cfg.get("min_files_per_round", 3),
        "save_state_interval":     dl_cfg.get("save_state_interval", 5),
    }

    # focus_areas: å¦‚æœæœ‰ query å°±æ”¾åœ¨æœ€å‰é¢
    focus_areas = dl_cfg.get("focus_areas", [])
    if query:
        focus_areas = [query] + [fa for fa in focus_areas if fa != query]
    searcher_config["focus_areas"] = focus_areas

    # rounds
    if rounds > 0:
        searcher_config["max_rounds"] = rounds
    elif dl_cfg.get("max_rounds", 0) > 0:
        searcher_config["max_rounds"] = dl_cfg["max_rounds"]
    else:
        searcher_config["max_rounds"] = 1  # é»˜è®¤è·‘ 1 è½®

    logger.info(f"   å…³é”®è¯/è½®: {searcher_config['keywords_per_round']}")
    logger.info(f"   è½®æ•°ä¸Šé™:  {searcher_config['max_rounds']}")
    if focus_areas:
        logger.info(f"   ç„¦ç‚¹é¢†åŸŸ:  {', '.join(focus_areas[:3])}{'...' if len(focus_areas) > 3 else ''}")

    searcher = IntegratedSearcher(
        output_dir=paths["download_dir"],
        keyword_db_path=paths["keywords_db"],
        use_browser=True,
        config=searcher_config,
        logger=logger,
    )
    searcher.run()
    s = searcher._count_downloaded_files()
    return {"downloaded": s["files_downloaded"], "failed": 0, "skipped": 0}


# ============================================================================
# å‘½ä»¤: classify
# ============================================================================

def cmd_classify(config: Dict, paths: Dict[str, Path],
                 logger: logging.Logger, once: bool = True) -> Dict:
    """è‡ªåŠ¨æ•´ç†: 4D åˆ†ç±» (å‚å•†/ç±»å‹/ä¸»é¢˜/æ‹“æ‰‘)"""
    logger.info("ğŸ—‚ï¸  å¯åŠ¨ 4D è‡ªåŠ¨åˆ†ç±»ç³»ç»Ÿ")
    logger.info(f"   æºç›®å½•:   {paths['download_dir']}")
    logger.info(f"   ç›®æ ‡ç›®å½•: {paths['classified_dir']}")

    source_dir = paths["download_dir"]
    target_dir = paths["classified_dir"]
    stats = {"total": 0, "success": 0, "failed": 0, "skipped": 0}

    if not source_dir.exists():
        logger.warning(f"âš ï¸  æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return stats

    target_dir.mkdir(parents=True, exist_ok=True)

    try:
        from pdf_classifier import PDFClassifier, ProcessedFilesDB, is_file_stable
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return stats

    db_path = PROJECT_ROOT / "classified_files.db"
    db = ProcessedFilesDB(db_path)
    metadata_file = target_dir / "metadata.jsonl"
    head_pages = config.get("classifier", {}).get("head_pages", 3)
    min_stable = config.get("classifier", {}).get("min_stable_seconds", 15)

    classifier = PDFClassifier(
        source_dir=source_dir,
        target_dir=target_dir,
        db=db,
        metadata_file=metadata_file,
        head_pages=head_pages,
        mode="move",
        dry_run=False,
    )

    files = classifier.scan_new_files()
    logger.info(f"ğŸ“‹ å‘ç° {len(files)} ä¸ªå¾…åˆ†ç±» PDF")
    stats["total"] = len(files)

    for i, pdf_file in enumerate(files, 1):
        try:
            if not is_file_stable(pdf_file, checks=2, min_stable_seconds=min_stable):
                stats["skipped"] += 1
                continue
            result = classifier.process_file(pdf_file)
            if result:
                stats["success"] += 1
            else:
                stats["skipped"] += 1
            if i % 10 == 0 or i == len(files):
                logger.info(f"  è¿›åº¦: {i}/{len(files)} "
                            f"(âœ…{stats['success']} âŒ{stats['failed']} â­ï¸{stats['skipped']})")
        except Exception as e:
            stats["failed"] += 1
            logger.error(f"  âŒ {pdf_file.name}: {e}")

    db.close()
    logger.info(f"\nğŸ“Š åˆ†ç±»å®Œæˆ: æ€»è®¡ {stats['total']} | æˆåŠŸ {stats['success']} | "
                f"å¤±è´¥ {stats['failed']} | è·³è¿‡ {stats['skipped']}")
    return stats


# ============================================================================
# å‘½ä»¤: build-kb â€” ä»åˆ†ç±»ç›®å½•æ„å»ºçŸ¥è¯†åº“
# ============================================================================

def cmd_build_kb(config: Dict, paths: Dict[str, Path],
                 logger: logging.Logger, rebuild: bool = False,
                 repair: bool = False) -> Dict:
    """ä»åˆ†ç±»å¥½çš„ PDF æ„å»º / å¢é‡æ›´æ–°çŸ¥è¯†åº“ (SQLite FTS5 + FAISS)"""

    # --repair: å¿«é€Ÿä¿®å¤æŸåçš„ FTS5 ç´¢å¼•
    if repair:
        logger.info("ğŸ”§ ä¿®å¤ FTS5 ç´¢å¼•")
        try:
            from kb_builder import repair_fts
        except ImportError as e:
            logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            return {}
        return repair_fts(paths["kb_dir"])

    logger.info("ğŸ“š æ„å»ºçŸ¥è¯†åº“")
    logger.info(f"   æºç›®å½•: {paths['classified_dir']}")
    logger.info(f"   çŸ¥è¯†åº“: {paths['kb_dir']}")

    classified_dir = paths["classified_dir"]
    if not classified_dir.exists():
        logger.warning(f"âš ï¸  åˆ†ç±»ç›®å½•ä¸å­˜åœ¨: {classified_dir}")
        logger.warning("   è¯·å…ˆè¿è¡Œ classify å¯¹ PDF è¿›è¡Œåˆ†ç±»")
        return {}

    pdf_count = len(list(classified_dir.rglob("*.pdf")))
    if pdf_count == 0:
        logger.warning("âš ï¸  åˆ†ç±»ç›®å½•ä¸­æ²¡æœ‰ PDF æ–‡ä»¶")
        return {}

    try:
        from kb_builder import build_kb
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        logger.error("   pip install pypdf")
        return {}

    def progress(cur, total, name):
        if cur % 20 == 0 or cur == total:
            logger.info(f"  [{cur}/{total}] {name}")

    stats = build_kb(
        classified_dir=classified_dir,
        kb_dir=paths["kb_dir"],
        rebuild=rebuild,
        build_faiss=True,
        progress_callback=progress,
    )

    logger.info(f"\nğŸ“Š çŸ¥è¯†åº“æ„å»ºå®Œæˆ:")
    logger.info(f"   æ–°å¢æ–‡æ¡£: {stats.get('docs_added', 0)}")
    logger.info(f"   æ–°å¢åˆ†å—: {stats.get('chunks_added', 0)}")
    logger.info(f"   FAISSå‘é‡: {stats.get('faiss_vectors', 0)}")
    logger.info(f"   è·³è¿‡: {stats.get('skipped', 0)} | é”™è¯¯: {stats.get('errors', 0)}")
    return stats


# ============================================================================
# å‘½ä»¤: pack â€” é«˜ç½®ä¿¡åº¦ç­›é€‰
# ============================================================================

def cmd_pack(query: str, config: Dict, paths: Dict[str, Path],
             logger: logging.Logger, max_docs: int = 20,
             min_score: float = 0.0, auto_confirm: bool = False
             ) -> Optional[Path]:
    """å¯¹è¯æ‰“åŒ…: æ··åˆæ£€ç´¢ â†’ é«˜ç½®ä¿¡åº¦ç­›é€‰ â†’ NotebookLM Source"""
    logger.info(f"ğŸ” æ£€ç´¢æ‰“åŒ…: \"{query}\"")
    logger.info(f"   å‚æ•°: top={max_docs}, min_score={min_score}")

    kb_path    = paths["kb_db"]
    faiss_path = paths["kb_faiss"]
    base_dir   = paths["classified_dir"]
    base_output = paths["pack_output"]

    if not kb_path.exists():
        # è‡ªåŠ¨ä»åˆ†ç±»ç›®å½•æ„å»ºçŸ¥è¯†åº“
        classified_dir = paths["classified_dir"]
        if classified_dir.exists() and list(classified_dir.rglob("*.pdf")):
            logger.info("ğŸ“š çŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œä»åˆ†ç±»ç›®å½•è‡ªåŠ¨æ„å»º...")
            cmd_build_kb(config, paths, logger)
            if not kb_path.exists():
                logger.error("âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥")
                return None
        else:
            logger.error(f"âŒ çŸ¥è¯†åº“ä¸å­˜åœ¨ä¸”æ— åˆ†ç±»æ–‡ä»¶: {kb_path}")
            logger.error("   è¯·å…ˆè¿è¡Œ: collect â†’ classify â†’ build-kb")
            return None

    try:
        from smart_pack import (
            hybrid_search, select_diverse_docs, pack_files,
            detect_language, make_slug,
        )
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        logger.error("   pip install sentence-transformers faiss-cpu deep-translator")
        return None

    results = hybrid_search(query, kb_path, faiss_path, top_k=100)

    # è‡ªåŠ¨æ£€æµ‹ FTS5 æŸå â†’ ä¿®å¤åé‡è¯•
    if not results or all(r.get("method") != "fts5" for r in results):
        fts_broken = False
        try:
            import sqlite3 as _sql
            _c = _sql.connect(f"file:{kb_path}?mode=ro", uri=True)
            # å’Œ smart_pack.py ä¸€æ¨¡ä¸€æ ·çš„ 3-table JOIN + ORDER BY bm25
            _c.execute("""
                SELECT chunks.doc_id, d.path, bm25(chunks_fts)
                FROM chunks_fts
                JOIN chunks ON chunks_fts.chunk_id = chunks.chunk_id
                JOIN documents d ON chunks.doc_id = d.doc_id
                WHERE chunks_fts MATCH '"test"'
                ORDER BY bm25(chunks_fts) ASC
                LIMIT 5
            """).fetchall()
        except (_sql.DatabaseError, _sql.OperationalError):
            fts_broken = True
        finally:
            try:
                _c.close()
            except Exception:
                pass

        if fts_broken:
            logger.warning("âš ï¸  æ£€æµ‹åˆ° FTS5 ç´¢å¼•æŸåï¼Œè‡ªåŠ¨ä¿®å¤ä¸­...")
            cmd_build_kb(config, paths, logger, repair=True)
            results = hybrid_search(query, kb_path, faiss_path, top_k=100)

    if not results:
        logger.warning("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return None

    logger.info(f"âœ… æ£€ç´¢åˆ° {len(results)} ä¸ªå€™é€‰æ–‡æ¡£")

    if min_score > 0:
        before = len(results)
        results = [r for r in results if r["score"] >= min_score]
        logger.info(f"ğŸ¯ ç½®ä¿¡åº¦ç­›é€‰ (â‰¥{min_score}): {before} â†’ {len(results)}")

    if not results:
        logger.warning(f"âš ï¸  æ²¡æœ‰æ–‡æ¡£è¾¾åˆ°ç½®ä¿¡åº¦ {min_score}")
        return None

    selected = select_diverse_docs(results, max_docs=max_docs)

    # åˆ†å±‚å±•ç¤º
    high = [d for d in selected if d["score"] > 0.7]
    mid  = [d for d in selected if 0.4 <= d["score"] <= 0.7]
    low  = [d for d in selected if d["score"] < 0.4]
    icons = {"fts5": "ğŸ”¤", "faiss": "ğŸ§ ", "hybrid": "âš¡"}
    idx = 1

    print(f"\n{'â•'*70}")
    print(f"  ğŸ“¦ æ™ºèƒ½é€‰æ‹©äº† {len(selected)} ä¸ªæ–‡æ¡£")
    print(f"{'â•'*70}")

    for label, group in [("ğŸ”¥ é«˜ç›¸å…³åº¦", high), ("ğŸ“Œ ä¸­ç­‰ç›¸å…³åº¦", mid), ("ğŸ’¡ å‚è€ƒæ–‡æ¡£", low)]:
        if group:
            print(f"\n  {label} ({len(group)} ä¸ª):")
            for doc in group:
                ic = icons.get(doc["method"], "?")
                print(f"  {idx:2d}. [{doc['score']:.3f}] {ic} "
                      f"{doc['vendor']}/{doc['doc_type']} â€” {doc.get('title', '')[:55]}")
                idx += 1

    vendors_hit = sorted(set(d["vendor"] for d in selected))
    types_hit   = sorted(set(d["doc_type"] for d in selected))
    avg_score   = sum(d["score"] for d in selected) / len(selected)

    print(f"\n{'â”€'*70}")
    print(f"  ğŸ“Š è¦†ç›– {len(vendors_hit)} å‚å•†: {', '.join(vendors_hit)}")
    print(f"  ğŸ“Š æ–‡æ¡£ç±»å‹: {', '.join(types_hit)}")
    print(f"  ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {avg_score:.3f}")
    print(f"  ğŸ“Š å›¾ä¾‹: ğŸ”¤=å…³é”®è¯  ğŸ§ =è¯­ä¹‰  âš¡=æ··åˆéªŒè¯")
    print(f"{'â•'*70}")

    if not auto_confirm:
        confirm = input(f"\n  æ‰“åŒ…è¿™ {len(selected)} ä¸ªæ–‡ä»¶? (Y/n): ").strip().lower()
        if confirm and confirm != "y":
            logger.info("âŒ å·²å–æ¶ˆ")
            return None

    lang = detect_language(query)
    date_str = datetime.now().strftime("%Y-%m-%d")
    slug = make_slug(query)
    output_dir = base_output / date_str / slug
    if output_dir.exists():
        try:
            shutil.rmtree(output_dir)
        except PermissionError:
            # Windows æ–‡ä»¶é”å®šæ—¶è¿½åŠ æ—¶é—´æˆ³
            slug2 = f"{slug}_{datetime.now().strftime('%H%M%S')}"
            output_dir = base_output / date_str / slug2
            logger.info(f"âš ï¸  æ—§ç›®å½•è¢«å ç”¨ï¼Œä½¿ç”¨æ–°ç›®å½•: {slug2}")

    logger.info("ğŸ“¦ æ‰“åŒ…ä¸­...")
    packed = pack_files(selected, base_dir, output_dir)
    _write_manifest(output_dir, query, selected, packed, lang)

    logger.info(f"\nâœ… æˆåŠŸæ‰“åŒ… {len(packed)} ä¸ªæ–‡ä»¶")
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print(f"\n  ğŸ’¡ ä¸‹ä¸€æ­¥: æ‰“å¼€ NotebookLM â†’ ä¸Šä¼  {output_dir.absolute()} ä¸­æ‰€æœ‰æ–‡ä»¶")
    return output_dir


def _write_manifest(output_dir, query, selected, packed, lang):
    manifest = output_dir / "manifest.txt"
    with open(manifest, "w", encoding="utf-8") as f:
        f.write(f"AlaiDocs æ™ºèƒ½æ‰“åŒ…æ¸…å•\n{'='*70}\n")
        f.write(f"æŸ¥è¯¢: {query}\n")
        f.write(f"è¯­è¨€: {'ä¸­æ–‡' if lang == 'zh' else 'è‹±æ–‡'}\n")
        f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ–¹æ³•: FTS5 å…³é”®è¯ + FAISS è¯­ä¹‰ (åŠ æƒèåˆ)\n")
        f.write(f"æ•°é‡: {len(packed)}\n{'='*70}\n\n")
        for label, pred in [("é«˜ç›¸å…³åº¦", lambda s: s > 0.7),
                            ("ä¸­ç­‰ç›¸å…³åº¦", lambda s: 0.4 <= s <= 0.7),
                            ("å‚è€ƒæ–‡æ¡£", lambda s: s < 0.4)]:
            group = [(i, d) for i, d in enumerate(selected, 1) if pred(d["score"])]
            if group:
                f.write(f"ã€{label}ã€‘\n\n")
                for i, doc in group:
                    f.write(f"{i:2d}. {doc.get('title', 'N/A')}\n")
                    f.write(f"    å‚å•†: {doc['vendor']} | ç±»å‹: {doc['doc_type']}\n")
                    f.write(f"    ç½®ä¿¡åº¦: {doc['score']:.3f} | æ–¹æ³•: {doc['method']}\n\n")
        f.write(f"{'='*70}\n")


# ============================================================================
# å‘½ä»¤: run (å…¨æµç¨‹)
# ============================================================================

def cmd_run(query: str, config: Dict, paths: Dict[str, Path],
            logger: logging.Logger, rounds: int = 1,
            max_docs: int = 20, min_score: float = 0.0,
            use_gemini: bool = False):
    """å…¨æµç¨‹: é‡‡é›† â†’ åˆ†ç±» â†’ å»ºåº“ â†’ æ‰“åŒ… (ä¸€æ¡å‘½ä»¤)"""
    header = f" ğŸš€ å…¨æµç¨‹: \"{query}\""
    logger.info(f"\nâ•”{'â•'*68}â•—")
    logger.info(f"â•‘{header:<68s}â•‘")
    logger.info(f"â•š{'â•'*68}â•\n")

    t0 = time.time()

    # â”€â”€ Phase 1: é‡‡é›† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"{'â”'*70}")
    if use_gemini:
        logger.info("  Phase 1/4: é‡‡é›† (Gemini â†’ DuckDuckGo â†’ PDF)")
    else:
        logger.info("  Phase 1/4: é‡‡é›† (å…³é”®è¯æ‰©å±• â†’ DuckDuckGo â†’ PDF)")
    logger.info(f"{'â”'*70}")
    col = cmd_collect(config, paths, logger,
                      query=query, rounds=rounds, use_gemini=use_gemini)

    # â”€â”€ Phase 2: åˆ†ç±» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\n{'â”'*70}")
    logger.info("  Phase 2/4: 4D åˆ†ç±» (å‚å•†/ç±»å‹/ä¸»é¢˜/æ‹“æ‰‘)")
    logger.info(f"{'â”'*70}")
    cls = cmd_classify(config, paths, logger, once=True)

    # â”€â”€ Phase 3: å»ºåº“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\n{'â”'*70}")
    logger.info("  Phase 3/4: æ„å»ºçŸ¥è¯†åº“ (SQLite FTS5 + FAISS)")
    logger.info(f"{'â”'*70}")
    cmd_build_kb(config, paths, logger)

    # â”€â”€ Phase 4: æ‰“åŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\n{'â”'*70}")
    logger.info("  Phase 4/4: æ··åˆæ£€ç´¢æ‰“åŒ…")
    logger.info(f"{'â”'*70}")
    output = cmd_pack(query, config, paths, logger,
                      max_docs=max_docs, min_score=min_score, auto_confirm=True)

    elapsed = time.time() - t0
    minutes = int(elapsed // 60)
    seconds = int(elapsed % 60)

    print(f"\nâ•”{'â•'*68}â•—")
    print(f"  âœ… å…¨æµç¨‹å®Œæˆ ({minutes}åˆ†{seconds}ç§’)")
    print(f"  ğŸ“¥ é‡‡é›†: {col.get('downloaded', 0)} ä¸ª PDF")
    print(f"  ğŸ—‚ï¸  åˆ†ç±»: {cls.get('success', 0)} ä¸ª")
    if output:
        print(f"  ğŸ“ æ‰“åŒ…: {output.absolute()}")
    print(f"â•š{'â•'*68}â•")


# ============================================================================
# å‘½ä»¤: status / config
# ============================================================================

def cmd_status(config: Dict, paths: Dict[str, Path], logger: logging.Logger):
    print(f"\n{'â•'*70}")
    print("  AlaiDocs ç³»ç»ŸçŠ¶æ€")
    print(f"{'â•'*70}\n")

    # ä¸‹è½½
    dl = paths["download_dir"]
    if dl.exists():
        pdfs = list(dl.rglob("*.pdf"))
        size = sum(f.stat().st_size for f in pdfs if f.is_file())
        vendors = sorted(d.name for d in dl.iterdir()
                         if d.is_dir() and not d.name.startswith(("_", ".")))
        print(f"  ğŸ“¥ ä¸‹è½½ç¼“å†²åŒº: {dl}")
        print(f"     PDF: {len(pdfs)} | {size/(1024**2):.1f} MB")
        print(f"     å‚å•†: {', '.join(vendors) if vendors else '(ç©º)'}")
    else:
        print(f"  ğŸ“¥ ä¸‹è½½ç¼“å†²åŒº: (æœªåˆ›å»º) â€” è¿è¡Œ init")

    # åˆ†ç±»
    cls = paths["classified_dir"]
    if cls.exists():
        cp = list(cls.rglob("*.pdf"))
        cs = sum(f.stat().st_size for f in cp if f.is_file())
        cv = sorted(d.name for d in cls.iterdir()
                    if d.is_dir() and not d.name.startswith(("_", ".", "T")))
        print(f"\n  ğŸ—‚ï¸  åˆ†ç±»å½’æ¡£: {cls}")
        print(f"     PDF: {len(cp)} | {cs/(1024**3):.2f} GB")
        print(f"     å‚å•† ({len(cv)}): {', '.join(cv[:12])}"
              f"{'...' if len(cv) > 12 else ''}")
    else:
        print(f"\n  ğŸ—‚ï¸  åˆ†ç±»å½’æ¡£: (æœªåˆ›å»º)")

    # çŸ¥è¯†åº“
    kb = paths["kb_db"]
    if kb.exists():
        try:
            conn = sqlite3.connect(f"file:{kb}?mode=ro", uri=True)
            c = conn.cursor()
            c.execute("SELECT COUNT(*) FROM documents")
            nd = c.fetchone()[0]
            c.execute("SELECT COUNT(*) FROM chunks")
            nc = c.fetchone()[0]
            conn.close()
            print(f"\n  ğŸ“š çŸ¥è¯†åº“: {kb}")
            print(f"     æ–‡æ¡£: {nd} | åˆ†å—: {nc}")
        except Exception as e:
            print(f"\n  ğŸ“š çŸ¥è¯†åº“: è¯»å–å¤±è´¥ ({e})")
    else:
        print(f"\n  ğŸ“š çŸ¥è¯†åº“: (æœªåˆ›å»º)")

    fi = paths["kb_faiss"]
    if fi.exists():
        print(f"  ğŸ§  FAISS: {fi.stat().st_size/(1024**2):.1f} MB")
    else:
        print(f"  ğŸ§  FAISS: (æœªåˆ›å»º)")

    kw = paths["keywords_db"]
    if kw.exists():
        try:
            kd = json.loads(kw.read_text(encoding="utf-8"))
            print(f"\n  ğŸ”‘ å…³é”®è¯: {len(kd.get('keywords', {}))} ä¸ª, "
                  f"{kd.get('statistics', {}).get('total_searches', 0)} æ¬¡æœç´¢")
        except Exception:
            print(f"\n  ğŸ”‘ å…³é”®è¯åº“: è§£æå¤±è´¥")
    else:
        print(f"\n  ğŸ”‘ å…³é”®è¯åº“: (æœªåˆ›å»º)")

    po = paths["pack_output"]
    if po.exists() and po.is_dir():
        recent = sorted(d.name for d in po.iterdir() if d.is_dir())[-5:]
        print(f"\n  ğŸ“¦ æ‰“åŒ…è¾“å‡º: {po}")
        if recent:
            print(f"     æœ€è¿‘: {', '.join(recent)}")

    dl_cfg = config.get("downloader", {})
    print(f"\n  âš™ï¸  é…ç½®:")
    print(f"     æ¯è½®å…³é”®è¯:   {dl_cfg.get('keywords_per_round', 10)}")
    print(f"     æ¯è¯ç»“æœ:     {dl_cfg.get('results_per_keyword', 20)}")
    print(f"     æ–‡ä»¶ä¸Šé™:     {dl_cfg.get('total_files_limit', 10000)}")
    print(f"     å®¹é‡ä¸Šé™:     {dl_cfg.get('total_size_limit_gb', 100)} GB")
    print(f"     ç™½åå•åŸŸå:   {len(dl_cfg.get('domain_whitelist', []))} ä¸ª")
    print(f"\n{'â•'*70}\n")


def cmd_config(config: Dict):
    print(f"\n{'â•'*70}")
    print("  å½“å‰é…ç½®")
    print(f"{'â•'*70}\n")
    print(json.dumps(config, indent=2, ensure_ascii=False))
    print(f"\n{'â•'*70}\n")


# ============================================================================
# äº¤äº’æ¨¡å¼ (REPL)
# ============================================================================

def interactive_mode(config: Dict, paths: Dict[str, Path],
                     logger: logging.Logger):
    print(BANNER)
    print(HELP_TEXT)

    while True:
        try:
            raw = input("\n  alaidocs> ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n  ğŸ‘‹ å†è§!")
            break

        if not raw:
            continue

        parts = raw.split()
        cmd = parts[0].lower()
        rest = parts[1:]

        if cmd in ("quit", "exit", "q"):
            print("  ğŸ‘‹ å†è§!")
            break
        elif cmd in ("help", "h", "?"):
            print(HELP_TEXT)
        elif cmd == "init":
            cmd_init(config, logger)
            # é‡æ–°åŠ è½½è·¯å¾„
            paths.update(resolve_paths(load_config()))
        elif cmd == "collect":
            use_gem = "--gemini" in rest
            rounds_val = 0
            vendors_val = ""
            q_parts = []
            i = 0
            while i < len(rest):
                if rest[i] == "--gemini":
                    i += 1
                elif rest[i] == "--rounds" and i + 1 < len(rest):
                    rounds_val = int(rest[i+1]); i += 2
                elif rest[i] == "--vendors" and i + 1 < len(rest):
                    vendors_val = rest[i+1]; i += 2
                else:
                    q_parts.append(rest[i]); i += 1
            q = " ".join(q_parts)
            cmd_collect(config, paths, logger, query=q,
                        rounds=rounds_val, use_gemini=use_gem,
                        vendors=vendors_val)
        elif cmd == "classify":
            cmd_classify(config, paths, logger, once=True)
        elif cmd in ("build-kb", "buildkb", "kb"):
            rebuild = "--rebuild" in rest
            cmd_build_kb(config, paths, logger, rebuild=rebuild)
        elif cmd == "pack":
            q, top_n, ms = _parse_pack_args(rest)
            if q:
                cmd_pack(q, config, paths, logger, max_docs=top_n, min_score=ms)
            else:
                print("  âš ï¸  ç”¨æ³•: pack <æŸ¥è¯¢> [--top N] [--min-score 0.5]")
        elif cmd == "run":
            q, top_n, ms, rds, use_gem = _parse_run_args(rest)
            if q:
                cmd_run(q, config, paths, logger, rounds=rds,
                        max_docs=top_n, min_score=ms, use_gemini=use_gem)
            else:
                print("  âš ï¸  ç”¨æ³•: run <æŸ¥è¯¢ä¸»é¢˜> [--top N] [--gemini] [--rounds N]")
                print("  ä¾‹: run \"buck converter efficiency\"")
                print("  ä¾‹: run \"GaNæ•ˆç‡\" --gemini --top 15")
        elif cmd == "status":
            cmd_status(config, paths, logger)
        elif cmd == "config":
            cmd_config(config)
        elif "æ›´æ–°èµ„æ–™" in raw or "å¼€å§‹é‡‡é›†" in raw:
            cmd_collect(config, paths, logger)
        elif any(k in raw for k in ("æ£€ç´¢", "æœç´¢", "æŸ¥æ‰¾")):
            topic = raw
            for prefix in ("å¸®æˆ‘æ£€ç´¢", "æ£€ç´¢", "æœç´¢", "æŸ¥æ‰¾"):
                if raw.startswith(prefix):
                    topic = raw[len(prefix):].strip()
                    break
            if topic:
                cmd_pack(topic, config, paths, logger)
        elif "æ•´ç†" in raw or "åˆ†ç±»" in raw:
            cmd_classify(config, paths, logger, once=True)
        elif "çŠ¶æ€" in raw:
            cmd_status(config, paths, logger)
        else:
            print(f"  ğŸ’¡ å°† \"{raw}\" ä½œä¸ºæ£€ç´¢æŸ¥è¯¢...")
            cmd_pack(raw, config, paths, logger)


def _parse_pack_args(rest):
    parts, top, ms, i = [], 20, 0.0, 0
    while i < len(rest):
        if rest[i] == "--top" and i + 1 < len(rest):
            top = int(rest[i+1]); i += 2
        elif rest[i] == "--min-score" and i + 1 < len(rest):
            ms = float(rest[i+1]); i += 2
        else:
            parts.append(rest[i]); i += 1
    return " ".join(parts), top, ms


def _parse_run_args(rest):
    parts, top, ms, rds, gem, i = [], 20, 0.0, 1, False, 0
    while i < len(rest):
        if rest[i] == "--top" and i + 1 < len(rest):
            top = int(rest[i+1]); i += 2
        elif rest[i] == "--min-score" and i + 1 < len(rest):
            ms = float(rest[i+1]); i += 2
        elif rest[i] == "--rounds" and i + 1 < len(rest):
            rds = int(rest[i+1]); i += 2
        elif rest[i] == "--gemini":
            gem = True; i += 1
        else:
            parts.append(rest[i]); i += 1
    return " ".join(parts), top, ms, rds, gem


# ============================================================================
# CLI å…¥å£
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="AlaiDocs â€” DC-DC çŸ¥è¯†åº“ä¸€ç«™å¼ CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python alaidocs.py init                                   # é¦–æ¬¡åˆå§‹åŒ–
  python alaidocs.py                                        # äº¤äº’æ¨¡å¼
  python alaidocs.py collect "buck converter"                # é‡‡é›† PDF (å…¨å‚å•†)
  python alaidocs.py collect "buck" --vendors TI,ST,ADI     # ä»…æœ TI/ST/ADI
  python alaidocs.py collect --gemini --rounds 3             # Gemini æŒç»­é‡‡é›†
  python alaidocs.py classify                               # è‡ªåŠ¨åˆ†ç±»
  python alaidocs.py build-kb                               # æ„å»ºçŸ¥è¯†åº“
  python alaidocs.py pack "é™å‹å˜æ¢å™¨çƒ­ç®¡ç†"                  # æ£€ç´¢æ‰“åŒ…
  python alaidocs.py run "LLC resonant converter"           # å…¨æµç¨‹
  python alaidocs.py run "GaNæ•ˆç‡" --gemini --top 15        # å…¨æµç¨‹ + Gemini
  python alaidocs.py status                                 # ç³»ç»ŸçŠ¶æ€
        """,
    )

    sub = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")

    sub.add_parser("init", help="é¦–æ¬¡åˆå§‹åŒ–")

    sp = sub.add_parser("collect", help="æ™ºèƒ½é‡‡é›† (å‚å•†è½®æ¢ + DDG æœç´¢ + PDF ä¸‹è½½)")
    sp.add_argument("query", nargs="?", default="", help="æœç´¢ä¸»é¢˜ (å¦‚ \"buck converter\")")
    sp.add_argument("--rounds", type=int, default=0)
    sp.add_argument("--vendors", type=str, default="",
                    help="æŒ‡å®šå‚å•† (å¦‚ \"TI,ST,ADI\")")
    sp.add_argument("--gemini", action="store_true",
                    help="ä½¿ç”¨ Gemini Chrome ç”Ÿæˆå…³é”®è¯ (éœ€ç™»å½•)")

    sub.add_parser("classify", help="è‡ªåŠ¨ 4D åˆ†ç±»")

    sp = sub.add_parser("build-kb", help="æ„å»º/æ›´æ–°çŸ¥è¯†åº“")
    sp.add_argument("--rebuild", action="store_true", help="æ¸…ç©ºé‡å»ºï¼ˆé»˜è®¤å¢é‡ï¼‰")
    sp.add_argument("--repair", action="store_true", help="ä¿®å¤æŸåçš„ FTS5 ç´¢å¼•")

    sp = sub.add_parser("pack", help="æ£€ç´¢æ‰“åŒ…")
    sp.add_argument("query")
    sp.add_argument("--top", type=int, default=20)
    sp.add_argument("--min-score", type=float, default=0.0)
    sp.add_argument("-y", "--yes", action="store_true")

    sp = sub.add_parser("run", help="å…¨æµç¨‹ (é‡‡é›†â†’åˆ†ç±»â†’å»ºåº“â†’æ‰“åŒ…)")
    sp.add_argument("query")
    sp.add_argument("--rounds", type=int, default=1)
    sp.add_argument("--top", type=int, default=20)
    sp.add_argument("--min-score", type=float, default=0.0)
    sp.add_argument("--gemini", action="store_true",
                    help="ä½¿ç”¨ Gemini Chrome ç”Ÿæˆå…³é”®è¯")

    sub.add_parser("status", help="ç³»ç»ŸçŠ¶æ€")
    sub.add_parser("config", help="æ˜¾ç¤ºé…ç½®")

    parser.add_argument("-c", "--config", type=Path, default=None)
    parser.add_argument("--debug", action="store_true")

    args = parser.parse_args()
    logger = setup_logging(args.debug)
    config = load_config(args.config)
    paths = resolve_paths(config)

    if args.command is None:
        interactive_mode(config, paths, logger)
    elif args.command == "init":
        cmd_init(config, logger)
    elif args.command == "collect":
        if not ensure_initialized(paths, logger):
            return
        cmd_collect(config, paths, logger,
                    query=args.query, rounds=args.rounds,
                    use_gemini=args.gemini, vendors=args.vendors)
    elif args.command == "classify":
        if not ensure_initialized(paths, logger):
            return
        cmd_classify(config, paths, logger, once=True)
    elif args.command == "build-kb":
        if not ensure_initialized(paths, logger):
            return
        cmd_build_kb(config, paths, logger, rebuild=args.rebuild, repair=args.repair)
    elif args.command == "pack":
        if not ensure_initialized(paths, logger):
            return
        cmd_pack(args.query, config, paths, logger,
                 max_docs=args.top, min_score=args.min_score,
                 auto_confirm=args.yes)
    elif args.command == "run":
        if not ensure_initialized(paths, logger):
            return
        cmd_run(args.query, config, paths, logger,
                rounds=args.rounds, max_docs=args.top,
                min_score=args.min_score,
                use_gemini=args.gemini)
    elif args.command == "status":
        cmd_status(config, paths, logger)
    elif args.command == "config":
        cmd_config(config)


if __name__ == "__main__":
    main()
