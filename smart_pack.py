#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–‡ä»¶æ‰“åŒ…å™¨ - è‡ªåŠ¨æ£€ç´¢ã€å»é‡ã€æ‰“åŒ…PDFæ–‡ä»¶ä¾›NotebookLMä½¿ç”¨
"""

import sys
import sqlite3
import numpy as np
import shutil
from pathlib import Path
from typing import List, Dict, Tuple
import re
from datetime import datetime

# è‡ªåŠ¨ç¿»è¯‘
try:
    from deep_translator import GoogleTranslator
    TRANSLATOR_AVAILABLE = True
except ImportError:
    print("âš ï¸  æœªå®‰è£…ç¿»è¯‘åº“ï¼Œå°†å°è¯•ç›´æ¥æ£€ç´¢...")
    print("   å®‰è£…: pip install deep-translator")
    TRANSLATOR_AVAILABLE = False

# å‘é‡æ£€ç´¢
try:
    import faiss
    from sentence_transformers import SentenceTransformer
    FAISS_AVAILABLE = True
except ImportError:
    print("âš ï¸  FAISSæœªå®‰è£…ï¼Œä»…ä½¿ç”¨å…³é”®è¯æ£€ç´¢")
    FAISS_AVAILABLE = False

# æ¨¡å—çº§ç¼“å­˜ï¼šSentenceTransformer å•ä¾‹ï¼ˆæƒ°æ€§åŠ è½½ï¼‰
_cached_model = None

def get_sentence_transformer():
    """è·å–ç¼“å­˜çš„ SentenceTransformer æ¨¡å‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _cached_model
    if _cached_model is None:
        _cached_model = SentenceTransformer('all-MiniLM-L6-v2')
    return _cached_model


def make_slug(text: str, max_length: int = 50) -> str:
    """å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å¤¹å"""
    # åªä¿ç•™å­—æ¯æ•°å­—å’Œä¸­æ–‡
    slug = re.sub(r'[^\w\u4e00-\u9fff]+', '_', text)
    slug = slug.strip('_')
    if len(slug) > max_length:
        slug = slug[:max_length]
    return slug if slug else "query"


def detect_language(text: str) -> str:
    """æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼ˆç®€å•è§„åˆ™ï¼‰"""
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    return 'zh' if chinese_chars > len(text) * 0.3 else 'en'


def translate_to_english(text: str) -> str:
    """ç¿»è¯‘ä¸­æ–‡åˆ°è‹±æ–‡"""
    if not TRANSLATOR_AVAILABLE:
        return text
    
    try:
        translator = GoogleTranslator(source='zh-CN', target='en')
        result = translator.translate(text)
        print(f"  ğŸŒ ç¿»è¯‘: {text} â†’ {result}")
        return result
    except Exception as e:
        print(f"  âš ï¸  ç¿»è¯‘å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæ–‡")
        return text


def extract_keywords(query: str) -> List[str]:
    """æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆå«ç¼©å†™å±•å¼€å’Œé©¼å³°/è¿å†™æ‹†åˆ†ï¼‰"""
    import re
    # ç§»é™¤å¸¸è§åœç”¨è¯
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
                 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
                 'within', 'options', 'selection', 'choose', 'comparison'}

    # DC-DC é¢†åŸŸå¸¸ç”¨ç¼©å†™æ˜ å°„
    ABBREVIATIONS = {
        'bi': 'bidirectional',
        'bb': 'buck boost',
        'buckboost': 'buck boost',
        'bibuckboost': 'bidirectional buck boost',
        'dcdc': 'dc dc',
        'llc': 'llc resonant',
        'emi': 'electromagnetic interference',
        'emc': 'electromagnetic compatibility',
        'esd': 'electrostatic discharge',
        'pcb': 'printed circuit board',
        'gan': 'gallium nitride',
        'sic': 'silicon carbide',
        'mosfet': 'mosfet',
        'sepic': 'sepic',
        'smps': 'switched mode power supply',
        'pfc': 'power factor correction',
        'mppt': 'maximum power point tracking',
        'bms': 'battery management system',
    }

    # å…ˆå°è¯•æ•´ä¸²ç¼©å†™åŒ¹é…
    query_lower = query.lower().strip()
    if query_lower in ABBREVIATIONS:
        expanded = ABBREVIATIONS[query_lower]
        words = re.findall(r'\w+', expanded)
        return [w for w in words if len(w) > 1]

    # é©¼å³° / è¿å†™æ‹†åˆ†: "buckBoost" â†’ "buck Boost", "bibuckboost" â†’ å°è¯•å­ä¸²åŒ¹é…
    expanded_query = re.sub(r'([a-z])([A-Z])', r'\1 \2', query)  # camelCase
    # æŒ‰å·²çŸ¥æŠ€æœ¯è¯æ‹†åˆ†è¿å†™ (è´ªå¿ƒåŒ¹é…)
    KNOWN_TOKENS = sorted([
        'bidirectional', 'buck', 'boost', 'converter', 'inverting',
        'synchronous', 'resonant', 'isolated', 'flyback', 'forward',
        'half', 'bridge', 'full', 'phase', 'shifted', 'charge', 'pump',
        'sepic', 'cuk', 'zeta', 'llc', 'dab', 'controller', 'regulator',
        'driver', 'gate', 'mosfet', 'gan', 'sic', 'efficiency', 'thermal',
        'emi', 'emc', 'esd', 'pcb', 'layout', 'datasheet', 'design',
        'power', 'voltage', 'current', 'output', 'input', 'switching',
        'frequency', 'loop', 'compensation', 'feedback', 'control',
    ], key=len, reverse=True)

    def split_compound(word):
        """è´ªå¿ƒæ‹†åˆ†è¿å†™è¯: 'bibuckboost' â†’ ['bi','buck','boost']"""
        result = []
        w = word.lower()
        while w:
            matched = False
            for token in KNOWN_TOKENS:
                if w.startswith(token):
                    result.append(token)
                    w = w[len(token):]
                    matched = True
                    break
            if not matched:
                # æ²¡åŒ¹é…åˆ°å·²çŸ¥è¯ï¼Œå–æ•´ä¸ªå‰©ä½™
                if w:
                    result.append(w)
                break
        return result

    # åˆ†è¯
    words = re.findall(r'\w+', expanded_query.lower())

    # å¯¹æ¯ä¸ªè¯å°è¯•æ‹†åˆ† + ç¼©å†™å±•å¼€
    all_keywords = []
    for w in words:
        if w in stopwords or len(w) < 2:
            continue
        # ç¼©å†™å±•å¼€
        if w in ABBREVIATIONS:
            expanded_words = re.findall(r'\w+', ABBREVIATIONS[w])
            all_keywords.extend(expanded_words)
        # é•¿è¿å†™è¯æ‹†åˆ†
        elif len(w) > 8:
            parts = split_compound(w)
            if len(parts) > 1:
                # æ‹†åˆ†æˆåŠŸï¼Œå±•å¼€ç¼©å†™
                for p in parts:
                    if p in ABBREVIATIONS:
                        all_keywords.extend(re.findall(r'\w+', ABBREVIATIONS[p]))
                    elif len(p) > 1 and p not in stopwords:
                        all_keywords.append(p)
            else:
                all_keywords.append(w)
        elif len(w) > 2:
            all_keywords.append(w)

    # å»é‡ä¿åº
    seen = set()
    unique = []
    for kw in all_keywords:
        if kw not in seen:
            seen.add(kw)
            unique.append(kw)

    return unique if unique else [query_lower]


def search_fts5(query: str, kb_path: Path, limit: int = 100) -> List[Dict]:
    """å…¨æ–‡æœç´¢ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰"""
    # åªè¯»æ¨¡å¼æ‰“å¼€ï¼Œé¿å…å†²çª
    conn = sqlite3.connect(f'file:{kb_path}?mode=ro', uri=True)
    cursor = conn.cursor()
    
    # æå–å…³é”®è¯å¹¶æ„å»ºORæŸ¥è¯¢
    keywords = extract_keywords(query)
    if not keywords:
        keywords = [query]
    
    # FTS5 ORæŸ¥è¯¢ï¼ˆç”¨åŒå¼•å·åŒ…è£¹å…³é”®è¯ï¼Œé¿å…ç‰¹æ®Šç¬¦å·å’Œä¿ç•™å­—é—®é¢˜ï¼‰
    fts_query = ' OR '.join([f'"{kw}"' for kw in keywords])
    
    try:
        # ä½¿ç”¨ bm25() å¾—åˆ†ï¼ˆè¶Šå°è¶Šç›¸å…³ï¼‰ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè¶Šå¤§è¶Šç›¸å…³ï¼‰
        cursor.execute("""
            SELECT chunks.doc_id, chunks.chunk_id, chunks.text, chunks.page_start,
                   d.path, d.vendor, d.doc_type, d.title,
                   bm25(chunks_fts) as bm25_score
            FROM chunks_fts
            JOIN chunks ON chunks_fts.chunk_id = chunks.chunk_id
            JOIN documents d ON chunks.doc_id = d.doc_id
            WHERE chunks_fts MATCH ?
            ORDER BY bm25(chunks_fts) ASC
            LIMIT ?
        """, (fts_query, limit))
        
        # è·å–æ‰€æœ‰ç»“æœå¹¶æå– bm25 å€¼
        rows = cursor.fetchall()
        if not rows:
            conn.close()
            return []
        
        # æ‰¾åˆ°æœ€å° bm25 å€¼ï¼ˆæœ€ç›¸å…³ï¼Œé€šå¸¸æœ€è´Ÿï¼‰
        bm25_vals = [row[8] if row[8] is not None else 0.0 for row in rows]
        min_bm25 = min(bm25_vals)
        
        results = []
        for row in rows:
            bm25_value = row[8] if row[8] is not None else 0.0
            # ç›¸å¯¹å½’ä¸€åŒ–ï¼šæœ€ç›¸å…³çš„ bm25ï¼ˆæœ€å°å€¼ï¼‰â†’ adj=0 â†’ score=1.0
            # è¶Šä¸ç›¸å…³çš„ bm25 â†’ adj è¶Šå¤§ â†’ score è¶Šæ¥è¿‘ 0
            # è¿™æ ·ç¡®ä¿ FTS æ’åºå’Œåˆ†æ•°å•è°ƒä¸€è‡´
            adj = bm25_value - min_bm25
            fts_score = 1.0 / (1.0 + adj)
            
            results.append({
                'doc_id': row[0],
                'chunk_id': row[1],
                'content': row[2],  # textåˆ—æ˜ å°„ä¸ºcontent
                'page': row[3],     # page_startåˆ—æ˜ å°„ä¸ºpage
                'path': row[4],
                'vendor': row[5],
                'doc_type': row[6],
                'title': row[7],
                'score': fts_score,  # ç›¸å¯¹å½’ä¸€åŒ–çš„ç›¸ä¼¼åº¦åˆ†æ•°
                'method': 'fts5'
            })
    except sqlite3.OperationalError as e:
        print(f"  âš ï¸  FTS5æŸ¥è¯¢å¤±è´¥: {e}")
        results = []
    except sqlite3.DatabaseError as e:
        print(f"  âš ï¸  æ•°æ®åº“é”™è¯¯: {e}")
        print(f"  ğŸ’¡ æç¤º: è¯·åœæ­¢ kb_watcher åé‡è¯•")
        results = []
    
    conn.close()
    return results


def search_faiss(query: str, kb_path: Path, faiss_path: Path, limit: int = 100) -> List[Dict]:
    """å‘é‡æœç´¢ï¼ˆè¯­ä¹‰ç›¸ä¼¼ï¼‰
    
    è¯´æ˜ï¼šè‡ªåŠ¨æ£€æµ‹ç´¢å¼•ç±»å‹ï¼ˆL2 vs IP/ä½™å¼¦ï¼‰ï¼Œå¹¶é‡‡ç”¨ç›¸åº”çš„è·ç¦»æ˜ å°„ç­–ç•¥ï¼š
    - L2 è·ç¦»ï¼šä½¿ç”¨ 1/(1+dist) æ˜ å°„åˆ°ç›¸ä¼¼åº¦ï¼Œç¡®ä¿åˆ†æ•°åœ¨ (0,1] èŒƒå›´
    - IP/ä½™å¼¦è·ç¦»ï¼šå¯¹ query åš L2 normalizeï¼Œç›´æ¥ä½¿ç”¨ dist ä½œä¸ºç›¸ä¼¼åº¦å¹¶ clamp åˆ° [0,1]
    åˆ†æ•°è¶Šå¤§è¡¨ç¤ºè¶Šç›¸å…³ã€‚
    """
    if not FAISS_AVAILABLE:
        return []
    
    try:
        # åŠ è½½FAISSç´¢å¼•
        index = faiss.read_index(str(faiss_path))
        
        # æ£€æµ‹ç´¢å¼•ç±»å‹ï¼ˆL2 vs IP/ä½™å¼¦ï¼‰
        is_inner_product = False
        try:
            # å°è¯•è¯»å– metric_type (FAISS >= 1.6.0)
            if hasattr(index, 'metric_type'):
                is_inner_product = (index.metric_type == faiss.METRIC_INNER_PRODUCT)
        except:
            # é™çº§ï¼šæ£€æŸ¥ç´¢å¼•ç±»åï¼ˆä¸å¤ªå¯é ä½†èƒ½è¦†ç›–å¸¸è§æƒ…å†µï¼‰
            index_class = type(index).__name__
            is_inner_product = 'IP' in index_class or 'InnerProduct' in index_class
        
        # ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹
        model = get_sentence_transformer()
        
        # æŸ¥è¯¢å‘é‡
        query_vec = model.encode([query])[0].astype('float32')
        
        # å¦‚æœæ˜¯ IP/ä½™å¼¦ç´¢å¼•ï¼Œå¯¹ query åš L2 å½’ä¸€åŒ–
        if is_inner_product:
            norm = np.linalg.norm(query_vec)
            if norm > 0:
                query_vec = query_vec / norm
        
        query_vec = query_vec.reshape(1, -1)
        
        # æœç´¢
        distances, indices = index.search(query_vec, limit)
        
        # è·å–chunkä¿¡æ¯ï¼ˆåªè¯»æ¨¡å¼ï¼‰
        conn = sqlite3.connect(f'file:{kb_path}?mode=ro', uri=True)
        cursor = conn.cursor()
        
        results = []
        for idx, (dist, vec_id) in enumerate(zip(distances[0], indices[0])):
            if vec_id < 0:
                continue
            
            # æ ¹æ®vector_idæ‰¾åˆ°chunk
            cursor.execute("""
                SELECT e.chunk_id, c.doc_id, c.text, c.page_start,
                       d.path, d.vendor, d.doc_type, d.title
                FROM embeddings e
                JOIN chunks c ON e.chunk_id = c.chunk_id
                JOIN documents d ON c.doc_id = d.doc_id
                WHERE e.vector_id = ?
            """, (int(vec_id),))
            
            row = cursor.fetchone()
            if row:
                # æ ¹æ®ç´¢å¼•ç±»å‹æ˜ å°„åˆ†æ•°
                if is_inner_product:
                    # IP/ä½™å¼¦ï¼šdist æœ¬èº«å°±æ˜¯ç›¸ä¼¼åº¦ï¼ˆå·²åšå½’ä¸€åŒ–ï¼‰ï¼Œclamp åˆ° [0,1]
                    faiss_score = float(max(0.0, min(1.0, dist)))
                else:
                    # L2 è·ç¦»æ˜ å°„ä¸ºç›¸ä¼¼åº¦ï¼š1 / (1 + dist)
                    # ç¡®ä¿åˆ†æ•°åœ¨ (0, 1] èŒƒå›´ï¼Œdist è¶Šå°ï¼ˆè¶Šç›¸ä¼¼ï¼‰score è¶Šå¤§
                    faiss_score = 1.0 / (1.0 + float(dist))
                
                results.append({
                    'chunk_id': row[0],
                    'doc_id': row[1],
                    'content': row[2],  # textåˆ—æ˜ å°„ä¸ºcontent
                    'page': row[3],     # page_startåˆ—æ˜ å°„ä¸ºpage
                    'path': row[4],
                    'vendor': row[5],
                    'doc_type': row[6],
                    'title': row[7],
                    'score': faiss_score,  # ä½¿ç”¨ 1/(1+dist) æ˜ å°„çš„ç›¸ä¼¼åº¦
                    'method': 'faiss'
                })
        
        conn.close()
    except sqlite3.DatabaseError as e:
        print(f"  âš ï¸  æ•°æ®åº“é”™è¯¯: {e}")
        print(f"  ğŸ’¡ æç¤º: è¯·åœæ­¢ kb_watcher åé‡è¯•")
        results = []
    except Exception as e:
        print(f"  âš ï¸  FAISSæŸ¥è¯¢å¤±è´¥: {e}")
        results = []
    
    return results


def hybrid_search(query: str, kb_path: Path, faiss_path: Path = None,
                  top_k: int = 100) -> List[Dict]:
    """æ··åˆæœç´¢ï¼šFTS5 + FAISS"""
    # è¯­è¨€æ£€æµ‹å’Œç¿»è¯‘
    lang = detect_language(query)
    print(f"\nğŸ” æ£€æµ‹è¯­è¨€: {'ä¸­æ–‡' if lang == 'zh' else 'è‹±æ–‡'}")
    
    if lang == 'zh' and TRANSLATOR_AVAILABLE:
        en_query = translate_to_english(query)
    else:
        en_query = query
    
    all_results = []
    
    # FTS5æœç´¢
    print(f"ğŸ“š å…³é”®è¯æœç´¢ (FTS5)...")
    fts_results = search_fts5(en_query, kb_path, limit=top_k)
    if fts_results:
        print(f"  âœ… æ‰¾åˆ° {len(fts_results)} ä¸ªç»“æœ")
        all_results.extend(fts_results)
    else:
        print(f"  âš ï¸  æœªæ‰¾åˆ°ç»“æœ")
    
    # FAISSæœç´¢
    if FAISS_AVAILABLE and faiss_path and faiss_path.exists():
        print(f"ğŸ§  è¯­ä¹‰æœç´¢ (FAISS)...")
        faiss_results = search_faiss(en_query, kb_path, faiss_path, limit=top_k)
        if faiss_results:
            print(f"  âœ… æ‰¾åˆ° {len(faiss_results)} ä¸ªç»“æœ")
            all_results.extend(faiss_results)
        else:
            print(f"  âš ï¸  æœªæ‰¾åˆ°ç»“æœ")
    
    if not all_results:
        return []
    
    # === çœŸæ­£çš„åŠ æƒèåˆé€»è¾‘ ===
    # 1. æŒ‰ doc_id åˆ†ç»„ï¼Œæ”¶é›†æ¥è‡ª FTS å’Œ FAISS çš„æ‰€æœ‰åˆ†æ•°
    doc_scores = {}  # doc_id -> {'fts': [scores], 'faiss': [scores], 'best_chunk': result}
    
    for result in all_results:
        doc_id = result['doc_id']
        method = result['method']
        score = result['score']
        
        if doc_id not in doc_scores:
            doc_scores[doc_id] = {'fts': [], 'faiss': [], 'best_chunk': result}
        
        # æ”¶é›†åˆ†æ•°
        if method == 'fts5':
            doc_scores[doc_id]['fts'].append(score)
        elif method == 'faiss':
            doc_scores[doc_id]['faiss'].append(score)
        
        # æ›´æ–°æœ€ä½³ chunkï¼ˆç”¨äºè¿”å›ï¼‰
        if score > doc_scores[doc_id]['best_chunk']['score']:
            doc_scores[doc_id]['best_chunk'] = result
    
    # 2. è®¡ç®—æ¯ä¸ª doc çš„èåˆåˆ†æ•°
    FTS_WEIGHT = 0.6
    FAISS_WEIGHT = 0.4
    DUAL_HIT_BONUS = 0.05  # åŒå‘½ä¸­å¥–åŠ±
    
    final_results = []
    for doc_id, data in doc_scores.items():
        fts_scores = data['fts']
        faiss_scores = data['faiss']
        best_chunk = data['best_chunk']
        
        # èšåˆå„é€šé“åˆ†æ•°ï¼ˆå–æœ€å¤§å€¼ï¼‰
        fts_max = max(fts_scores) if fts_scores else 0.0
        faiss_max = max(faiss_scores) if faiss_scores else 0.0
        
        # åŠ æƒèåˆ
        if fts_scores and faiss_scores:
            # åŒå‘½ä¸­ï¼šåŠ æƒèåˆ + å° bonus
            final_score = FTS_WEIGHT * fts_max + FAISS_WEIGHT * faiss_max + DUAL_HIT_BONUS
            final_score = min(final_score, 1.0)  # clamp åˆ° 1.0
            method = 'hybrid'
        elif fts_scores:
            # ä»… FTS
            final_score = fts_max
            method = 'fts5'
        else:
            # ä»… FAISS
            final_score = faiss_max
            method = 'faiss'
        
        # æ›´æ–° best_chunk çš„åˆ†æ•°å’Œæ–¹æ³•
        best_chunk['score'] = final_score
        best_chunk['method'] = method
        final_results.append(best_chunk)
    
    # 3. æ’åºå¹¶è¿”å›
    final_results.sort(key=lambda x: x['score'], reverse=True)
    print(f"\nğŸ“Š åˆå¹¶ç»“æœ: {len(final_results)} ä¸ªæ–‡æ¡£")
    return final_results


def select_diverse_docs(results: List[Dict], max_docs: int = 20) -> List[Dict]:
    """æ™ºèƒ½é€‰æ‹©æ–‡æ¡£ï¼šå»é‡ã€å¤šæ ·æ€§ã€è´¨é‡ä¼˜å…ˆ"""
    if not results:
        return []
    
    selected = []
    doc_ids_seen = set()
    
    # ç¬¬ä¸€è½®ï¼šé€‰æ‹©é«˜åˆ†æ–‡æ¡£ï¼ˆscore > 0.7ï¼‰
    high_score_docs = [r for r in results if r['score'] > 0.7]
    for r in high_score_docs[:max_docs // 2]:
        if r['doc_id'] not in doc_ids_seen:
            selected.append(r)
            doc_ids_seen.add(r['doc_id'])
    
    # ç¬¬äºŒè½®ï¼šæŒ‰å‚å•†å’Œæ–‡æ¡£ç±»å‹åˆ†ç»„ï¼Œç¡®ä¿å¤šæ ·æ€§
    by_category = {}
    for r in results:
        if r['doc_id'] in doc_ids_seen:
            continue
        key = f"{r['vendor']}/{r['doc_type']}"
        if key not in by_category:
            by_category[key] = []
        by_category[key].append(r)
    
    # è½®è¯¢é€‰æ‹©ï¼ˆæ¯ä¸ªç±»åˆ«é€‰ä¸€ä¸ªï¼‰
    categories = sorted(by_category.keys())
    round_idx = 0
    
    while len(selected) < max_docs and categories:
        for cat in categories[:]:
            if len(by_category[cat]) > 0:
                r = by_category[cat].pop(0)
                if r['doc_id'] not in doc_ids_seen:
                    selected.append(r)
                    doc_ids_seen.add(r['doc_id'])
                    if len(selected) >= max_docs:
                        break
            else:
                categories.remove(cat)
        round_idx += 1
        if round_idx > 10:  # é˜²æ­¢æ­»å¾ªç¯
            break
    
    # ç¬¬ä¸‰è½®ï¼šå¦‚æœè¿˜ä¸å¤Ÿï¼ŒæŒ‰åˆ†æ•°è¡¥å……
    if len(selected) < max_docs:
        for r in results:
            if r['doc_id'] not in doc_ids_seen:
                selected.append(r)
                doc_ids_seen.add(r['doc_id'])
                if len(selected) >= max_docs:
                    break
    
    return selected[:max_docs]


def pack_files(selected: List[Dict], base_dir: Path, output_dir: Path) -> List[Path]:
    """æ‰“åŒ…æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    packed = []
    for i, doc in enumerate(selected, 1):
        src = base_dir / doc['path']
        if not src.exists():
            print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {doc['path']}")
            continue
        
        # ä¿ç•™åŸæ–‡ä»¶åï¼ŒåŠ åºå·é¿å…å†²çª
        dst_name = f"{i:02d}_{src.name}"
        dst = output_dir / dst_name
        
        shutil.copy2(src, dst)
        packed.append(dst)
    
    return packed


def main():
    print("="*70)
    print("æ™ºèƒ½æ–‡ä»¶æ‰“åŒ…å™¨ - NotebookLMåŠ©æ‰‹")
    print("="*70)
    
    # é…ç½®
    kb_path = Path(r"D:\E-BOOK\axis-SQLite\kb.sqlite")
    faiss_path = Path(r"D:\E-BOOK\axis-SQLite\kb.faiss")
    base_dir = Path(r"D:\E-BOOK\axis-dcdc-pdf")
    base_output_dir = Path(r"D:\E-BOOK\_to_notebooklm")
    
    if not kb_path.exists():
        print(f"âŒ çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_path}")
        return
    
    # æŸ¥è¯¢
    print("\nè¯·è¾“å…¥æŸ¥è¯¢ï¼ˆæ”¯æŒä¸­æ–‡/è‹±æ–‡ï¼‰:")
    query = input("> ").strip()
    
    if not query:
        print("âŒ æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼šæ—¥æœŸ / æŸ¥è¯¢slug
    date_str = datetime.now().strftime("%Y-%m-%d")
    slug = make_slug(query)
    output_dir = base_output_dir / date_str / slug
    
    print(f"\n{'='*70}")
    print(f"æŸ¥è¯¢: {query}")
    print(f"è¾“å‡º: {output_dir}")
    print(f"{'='*70}")
    
    # æœç´¢ï¼ˆhybrid_search å†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç†è¯­è¨€æ£€æµ‹å’Œç¿»è¯‘ï¼‰
    results = hybrid_search(query, kb_path, faiss_path, top_k=100)
    
    if not results:
        print("\nâŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return
    
    print(f"\nâœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
    
    # æ™ºèƒ½é€‰æ‹©
    max_docs = 20
    selected = select_diverse_docs(results, max_docs=max_docs)
    
    print(f"\nğŸ“¦ æ™ºèƒ½é€‰æ‹©äº† {len(selected)} ä¸ªæ–‡æ¡£:")
    print(f"{'='*70}")
    
    # æŒ‰åˆ†æ•°åˆ†ç»„æ˜¾ç¤º
    high_score = [d for d in selected if d['score'] > 0.7]
    med_score = [d for d in selected if 0.4 <= d['score'] <= 0.7]
    low_score = [d for d in selected if d['score'] < 0.4]
    
    if high_score:
        print(f"\nğŸ”¥ é«˜ç›¸å…³åº¦ ({len(high_score)} ä¸ª):")
        for i, doc in enumerate(high_score, 1):
            score_str = f"{doc['score']:.3f}"
            method_icon = "ğŸ”¤" if doc['method'] == 'fts5' else "ğŸ§ " if doc['method'] == 'faiss' else "âš¡"
            print(f"{i:2d}. [{score_str}] {method_icon} {doc['vendor']}/{doc['doc_type']}")
            print(f"    {doc['title'][:65]}...")
    
    if med_score:
        print(f"\nğŸ“Œ ä¸­ç­‰ç›¸å…³åº¦ ({len(med_score)} ä¸ª):")
        for i, doc in enumerate(med_score, len(high_score) + 1):
            score_str = f"{doc['score']:.3f}"
            method_icon = "ğŸ”¤" if doc['method'] == 'fts5' else "ğŸ§ " if doc['method'] == 'faiss' else "âš¡"
            print(f"{i:2d}. [{score_str}] {method_icon} {doc['vendor']}/{doc['doc_type']}")
            print(f"    {doc['title'][:65]}...")
    
    if low_score:
        print(f"\nğŸ’¡ å‚è€ƒæ–‡æ¡£ ({len(low_score)} ä¸ª):")
        for i, doc in enumerate(low_score, len(high_score) + len(med_score) + 1):
            score_str = f"{doc['score']:.3f}"
            print(f"{i:2d}. [{score_str}] {doc['vendor']}/{doc['doc_type']} - {doc['title'][:50]}...")
    
    print(f"\n{'='*70}")
    print("å›¾ä¾‹: ğŸ”¤=å…³é”®è¯åŒ¹é… ğŸ§ =è¯­ä¹‰ç›¸ä¼¼ âš¡=æ··åˆéªŒè¯")
    
    # ç¡®è®¤æ‰“åŒ…
    print(f"\n{'='*70}")
    confirm = input(f"æ‰“åŒ…è¿™ {len(selected)} ä¸ªæ–‡ä»¶? (y/n): ").strip().lower()
    
    if confirm != 'y':
        print("âŒ å–æ¶ˆæ‰“åŒ…")
        return
    
    # æ¸…ç©ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if output_dir.exists():
        print(f"âš ï¸  ç›®å½•å·²å­˜åœ¨ï¼Œå°†æ¸…ç©º: {output_dir}")
        shutil.rmtree(output_dir)
    
    # æ‰“åŒ…
    print(f"\nğŸ“¦ æ‰“åŒ…ä¸­...")
    packed = pack_files(selected, base_dir, output_dir)
    
    print(f"\nâœ… æˆåŠŸæ‰“åŒ… {len(packed)} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"   1. æ‰“å¼€ NotebookLM")
    print(f"   2. ä¸Šä¼  {output_dir.absolute()} ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶")
    print(f"   3. å¼€å§‹åˆ†æï¼")
    
    # ç”Ÿæˆæ–‡ä»¶æ¸…å•
    manifest = output_dir / "manifest.txt"
    lang = detect_language(query)  # é‡æ–°æ£€æµ‹ç”¨äº manifest
    with open(manifest, 'w', encoding='utf-8') as f:
        f.write(f"æŸ¥è¯¢: {query}\n")
        f.write(f"è¯­è¨€: {'ä¸­æ–‡' if lang == 'zh' else 'è‹±æ–‡'}\n")
        f.write(f"æ‰“åŒ…æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ£€ç´¢æ–¹æ³•: FTS5å…³é”®è¯ + FAISSè¯­ä¹‰ç›¸ä¼¼\n\n")
        f.write(f"æ–‡ä»¶æ¸…å• ({len(packed)} ä¸ª):\n")
        f.write("="*70 + "\n\n")
        
        # æŒ‰ç›¸å…³åº¦åˆ†ç»„
        high_score = [(i, doc) for i, doc in enumerate(selected, 1) if doc['score'] > 0.7]
        med_score = [(i, doc) for i, doc in enumerate(selected, 1) if 0.4 <= doc['score'] <= 0.7]
        low_score = [(i, doc) for i, doc in enumerate(selected, 1) if doc['score'] < 0.4]
        
        if high_score:
            f.write("ã€é«˜ç›¸å…³åº¦æ–‡æ¡£ã€‘\n\n")
            for i, doc in high_score:
                f.write(f"{i:2d}. {doc['title']}\n")
                f.write(f"    å‚å•†: {doc['vendor']} | ç±»å‹: {doc['doc_type']}\n")
                f.write(f"    ç›¸å…³åº¦: {doc['score']:.3f} | æ–¹æ³•: {doc['method']}\n")
                f.write(f"    è·¯å¾„: {doc['path']}\n\n")
        
        if med_score:
            f.write("\nã€ä¸­ç­‰ç›¸å…³åº¦æ–‡æ¡£ã€‘\n\n")
            for i, doc in med_score:
                f.write(f"{i:2d}. {doc['title']}\n")
                f.write(f"    å‚å•†: {doc['vendor']} | ç±»å‹: {doc['doc_type']}\n")
                f.write(f"    ç›¸å…³åº¦: {doc['score']:.3f}\n\n")
        
        if low_score:
            f.write("\nã€å‚è€ƒæ–‡æ¡£ã€‘\n\n")
            for i, doc in low_score:
                f.write(f"{i:2d}. {doc['title']}\n")
                f.write(f"    {doc['vendor']}/{doc['doc_type']} - ç›¸å…³åº¦: {doc['score']:.3f}\n\n")
        
        f.write("\n" + "="*70 + "\n")
        f.write("\nNotebookLM ä½¿ç”¨å»ºè®®:\n")
        f.write("1. ä¼˜å…ˆé˜…è¯»é«˜ç›¸å…³åº¦æ–‡æ¡£\n")
        f.write("2. å…³æ³¨æ··åˆéªŒè¯(âš¡)çš„æ–‡æ¡£ï¼Œè¿™äº›åŒæ—¶æ»¡è¶³å…³é”®è¯å’Œè¯­ä¹‰åŒ¹é…\n")
        f.write("3. ä½¿ç”¨åŸå§‹æŸ¥è¯¢ä½œä¸ºåˆå§‹é—®é¢˜\n")
        f.write("4. åœ¨NotebookLMä¸­å¯ä»¥è¿›ä¸€æ­¥ç»†åŒ–é—®é¢˜\n")
    
    print(f"ğŸ“‹ æ–‡ä»¶æ¸…å•: {manifest}")


if __name__ == "__main__":
    main()
