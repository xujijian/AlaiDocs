#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†åº“æ„å»ºå™¨ â€” ä»åˆ†ç±»å¥½çš„ PDF æ„å»º SQLite FTS5 + FAISS å‘é‡ç´¢å¼•

ç”¨æ³•:
  # ä½œä¸ºæ¨¡å—è¢« alaidocs.py è°ƒç”¨
  from kb_builder import build_kb
  stats = build_kb(classified_dir, kb_dir)

  # ä¹Ÿå¯ç‹¬ç«‹è¿è¡Œ
  python kb_builder.py --source data/classified --output data/kb
"""

import logging
import re
import sqlite3
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np

logger = logging.getLogger("kb_builder")

# PDF æ–‡æœ¬æå–
try:
    from pypdf import PdfReader
    PDF_AVAILABLE = True
except ImportError:
    try:
        from PyPDF2 import PdfReader
        PDF_AVAILABLE = True
    except ImportError:
        PDF_AVAILABLE = False

# FAISS + SentenceTransformer (å¯é€‰)
try:
    import faiss
    from sentence_transformers import SentenceTransformer
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¸¸é‡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CHUNK_SIZE = 500        # æ¯å—å­—ç¬¦æ•°
CHUNK_OVERLAP = 50      # å—é—´é‡å å­—ç¬¦
MAX_PAGES = 50          # æ¯ç¯‡ PDF æœ€å¤§æŠ½å–é¡µæ•°
EMBED_MODEL = "all-MiniLM-L6-v2"
BATCH_SIZE = 64         # åµŒå…¥å‘é‡æ‰¹é‡ç¼–ç å¤§å°

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PDF æ–‡æœ¬æå–
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_full_text(filepath: Path, max_pages: int = MAX_PAGES) -> Tuple[str, int]:
    """æå– PDF å…¨æ–‡"""
    if not PDF_AVAILABLE:
        logger.warning("pypdf / PyPDF2 æœªå®‰è£…ï¼Œè·³è¿‡æ–‡æœ¬æå–")
        return "", 0
    try:
        reader = PdfReader(str(filepath))
        page_count = len(reader.pages)
        pages_to_read = min(max_pages, page_count)
        texts = []
        for i in range(pages_to_read):
            page_text = reader.pages[i].extract_text()
            if page_text:
                # æ¸…ç† Unicode surrogates (æŸäº› PDF å­—ä½“æ˜ å°„ä¼šäº§ç”Ÿ)
                page_text = page_text.encode("utf-8", errors="replace").decode("utf-8")
                texts.append(page_text)
        return "\n".join(texts), page_count
    except Exception as e:
        logger.debug(f"PDF æå–å¤±è´¥ {filepath.name}: {e}")
        return "", 0


def extract_title(text: str, filename: str) -> str:
    """ä»æ–‡æœ¬é¦–è¡Œæˆ–æ–‡ä»¶åçŒœæµ‹æ–‡æ¡£æ ‡é¢˜"""
    # ä¼˜å…ˆç”¨æ–‡æœ¬ç¬¬ä¸€è¡Œï¼ˆå»æ‰åƒåœ¾å­—ç¬¦åï¼‰
    if text:
        for line in text.split("\n"):
            line = line.strip()
            # è·³è¿‡å¤ªçŸ­æˆ–çº¯æ•°å­—/æ—¥æœŸè¡Œ
            if len(line) > 5 and not re.match(r'^[\d\s/\-\.]+$', line):
                return line[:200]
    # å›é€€ï¼šç”¨æ–‡ä»¶å
    return Path(filename).stem.replace("_", " ").replace("-", " ")[:200]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ–‡æœ¬åˆ†å—
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def chunk_text(text: str, chunk_size: int = CHUNK_SIZE,
               overlap: int = CHUNK_OVERLAP) -> List[Dict]:
    """å°†æ–‡æœ¬æŒ‰å›ºå®šçª—å£åˆ‡åˆ†ä¸ºå¤šä¸ª chunk"""
    if not text or not text.strip():
        return []
    # æŒ‰æ®µè½å…ˆåˆ‡ï¼Œå†åˆå¹¶åˆ°ç›®æ ‡å¤§å°
    paragraphs = re.split(r'\n\s*\n', text)
    chunks = []
    current = ""
    page_est = 0  # ç²—ç•¥çš„é¡µç ä¼°è®¡

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        if len(current) + len(para) + 1 <= chunk_size:
            current = (current + "\n" + para).strip()
        else:
            if current:
                chunks.append({"text": current, "page_start": page_est})
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡ chunk_sizeï¼Œå¼ºåˆ¶åˆ‡åˆ†
            if len(para) > chunk_size:
                for start in range(0, len(para), chunk_size - overlap):
                    piece = para[start:start + chunk_size]
                    chunks.append({"text": piece, "page_start": page_est})
            else:
                current = para
                continue
            current = ""
        # ç²—ç•¥ä¼°è®¡é¡µç ï¼ˆæ¯ 3000 å­—ç¬¦çº¦ 1 é¡µï¼‰
        page_est = len(text[:text.find(para) + len(para)]) // 3000

    if current:
        chunks.append({"text": current, "page_start": page_est})

    return chunks


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è·¯å¾„è§£æ â†’ å…ƒæ•°æ®
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_path_metadata(rel_path: str) -> Dict[str, str]:
    """
    ä»åˆ†ç±»ç›®å½•çš„ç›¸å¯¹è·¯å¾„è§£æå‚å•† / æ–‡æ¡£ç±»å‹ã€‚
    çº¦å®šå±‚çº§:  <vendor>/<doc_type>/<topic>/<topology>/file.pdf
    è‡³å°‘éœ€è¦ vendor å’Œ doc_type ä¸¤çº§ã€‚
    """
    parts = Path(rel_path).parts
    vendor   = parts[0] if len(parts) > 1 else "unknown"
    doc_type = parts[1] if len(parts) > 2 else "general"
    return {"vendor": vendor, "doc_type": doc_type}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SQLite å»ºè¡¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def create_schema(conn: sqlite3.Connection):
    """åˆ›å»ºçŸ¥è¯†åº“è¡¨ç»“æ„ (documents / chunks / chunks_fts / embeddings)"""
    conn.executescript("""
        CREATE TABLE IF NOT EXISTS documents (
            doc_id   INTEGER PRIMARY KEY AUTOINCREMENT,
            path     TEXT UNIQUE,
            vendor   TEXT,
            doc_type TEXT,
            title    TEXT
        );

        CREATE TABLE IF NOT EXISTS chunks (
            chunk_id    INTEGER PRIMARY KEY AUTOINCREMENT,
            doc_id      INTEGER REFERENCES documents(doc_id),
            text        TEXT,
            page_start  INTEGER
        );

        CREATE TABLE IF NOT EXISTS embeddings (
            chunk_id    INTEGER REFERENCES chunks(chunk_id),
            vector_id   INTEGER
        );
    """)

    # FTS5 è™šæ‹Ÿè¡¨ï¼ˆç‹¬ç«‹å­˜å‚¨ï¼Œä¸ä¾èµ– content syncï¼‰
    try:
        conn.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS chunks_fts
            USING fts5(text, chunk_id UNINDEXED)
        """)
    except sqlite3.OperationalError:
        pass  # å·²å­˜åœ¨

    conn.commit()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ ¸å¿ƒï¼šæ„å»ºçŸ¥è¯†åº“
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_kb(classified_dir: Path, kb_dir: Path,
             rebuild: bool = False,
             build_faiss: bool = True,
             progress_callback=None) -> Dict:
    """
    ä»åˆ†ç±»å®Œæˆçš„ PDF ç›®å½•æ„å»º / å¢é‡æ›´æ–°çŸ¥è¯†åº“ã€‚

    Args:
        classified_dir: åˆ†ç±»åçš„ PDF æ ¹ç›®å½•
        kb_dir:         çŸ¥è¯†åº“è¾“å‡ºç›®å½•
        rebuild:        True = æ¸…ç©ºé‡å»ºï¼›False = å¢é‡è¿½åŠ 
        build_faiss:    æ˜¯å¦æ„å»º FAISS å‘é‡ç´¢å¼•
        progress_callback: è¿›åº¦å›è°ƒ fn(current, total, msg)

    Returns:
        {"docs_added": N, "chunks_added": N, "faiss_vectors": N, "skipped": N, "errors": N}
    """
    kb_dir.mkdir(parents=True, exist_ok=True)
    db_path    = kb_dir / "kb.sqlite"
    faiss_path = kb_dir / "kb.faiss"

    stats = {"docs_added": 0, "chunks_added": 0,
             "faiss_vectors": 0, "skipped": 0, "errors": 0}

    if rebuild and db_path.exists():
        db_path.unlink()
        if faiss_path.exists():
            faiss_path.unlink()
        logger.info("ğŸ—‘ï¸  å·²æ¸…ç©ºæ—§çŸ¥è¯†åº“")

    conn = sqlite3.connect(str(db_path))
    create_schema(conn)

    # æ”¶é›†å·²æœ‰æ–‡æ¡£è·¯å¾„ï¼ˆå¢é‡å»é‡ï¼‰
    existing_paths = set()
    if not rebuild:
        cursor = conn.execute("SELECT path FROM documents")
        existing_paths = {row[0] for row in cursor.fetchall()}

    # æ‰«ææ‰€æœ‰ PDF
    all_pdfs = sorted(classified_dir.rglob("*.pdf"))
    logger.info(f"ğŸ“‚ æ‰«æåˆ° {len(all_pdfs)} ä¸ª PDF")

    all_chunk_texts = []  # ç”¨äºæœ€åæ‰¹é‡ç¼–ç å‘é‡
    chunk_id_list = []    # å¯¹åº”çš„ chunk_id

    for i, pdf in enumerate(all_pdfs):
        rel_path = pdf.relative_to(classified_dir).as_posix()

        if rel_path in existing_paths:
            stats["skipped"] += 1
            continue

        if progress_callback:
            progress_callback(i + 1, len(all_pdfs), pdf.name)

        # æå–æ–‡æœ¬
        text, page_count = extract_full_text(pdf)
        if not text or len(text.strip()) < 50:
            stats["errors"] += 1
            logger.debug(f"  è·³è¿‡ (æ–‡æœ¬è¿‡çŸ­): {rel_path}")
            continue

        # å…ƒæ•°æ®
        meta = parse_path_metadata(rel_path)
        title = extract_title(text, pdf.name)

        # åˆ†å—
        chunks = chunk_text(text)
        if not chunks:
            stats["errors"] += 1
            continue

        try:
            # æ’å…¥ document
            cursor = conn.execute(
                "INSERT INTO documents (path, vendor, doc_type, title) VALUES (?, ?, ?, ?)",
                (rel_path, meta["vendor"], meta["doc_type"], title)
            )
            doc_id = cursor.lastrowid

            # æ’å…¥ chunks + FTS
            for chunk in chunks:
                c2 = conn.execute(
                    "INSERT INTO chunks (doc_id, text, page_start) VALUES (?, ?, ?)",
                    (doc_id, chunk["text"], chunk["page_start"])
                )
                cid = c2.lastrowid

                # åŒæ­¥åˆ° FTS5
                conn.execute(
                    "INSERT INTO chunks_fts (rowid, text, chunk_id) VALUES (?, ?, ?)",
                    (cid, chunk["text"], cid)
                )

                all_chunk_texts.append(chunk["text"])
                chunk_id_list.append(cid)
                stats["chunks_added"] += 1

            stats["docs_added"] += 1

            # æ¯ 50 ç¯‡ commit ä¸€æ¬¡
            if stats["docs_added"] % 50 == 0:
                conn.commit()
                logger.info(f"  è¿›åº¦: {i+1}/{len(all_pdfs)} â€” "
                            f"å·²æ·»åŠ  {stats['docs_added']} ç¯‡, "
                            f"{stats['chunks_added']} å—")

        except sqlite3.IntegrityError:
            stats["skipped"] += 1
        except Exception as e:
            stats["errors"] += 1
            logger.warning(f"  âŒ {rel_path}: {e}")

    conn.commit()
    logger.info(f"âœ… SQLite + FTS5 æ„å»ºå®Œæˆ: "
                f"{stats['docs_added']} ç¯‡, {stats['chunks_added']} å—")

    # â”€â”€ FAISS å‘é‡ç´¢å¼• â”€â”€
    if build_faiss and FAISS_AVAILABLE and all_chunk_texts:
        logger.info(f"ğŸ§  æ„å»º FAISS å‘é‡ç´¢å¼• ({len(all_chunk_texts)} å—)...")
        try:
            model = SentenceTransformer(EMBED_MODEL)
            dim = model.get_sentence_embedding_dimension()

            # å¦‚æœæ˜¯å¢é‡ä¸”å·²æœ‰ç´¢å¼•ï¼Œå…ˆåŠ è½½
            if not rebuild and faiss_path.exists():
                index = faiss.read_index(str(faiss_path))
                next_vid = index.ntotal
            else:
                index = faiss.IndexFlatIP(dim)  # å†…ç§¯ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                next_vid = 0

            # åˆ†æ‰¹ç¼–ç 
            for batch_start in range(0, len(all_chunk_texts), BATCH_SIZE):
                batch_end = min(batch_start + BATCH_SIZE, len(all_chunk_texts))
                batch_texts = all_chunk_texts[batch_start:batch_end]
                batch_cids  = chunk_id_list[batch_start:batch_end]

                vecs = model.encode(batch_texts, show_progress_bar=False,
                                    normalize_embeddings=True)
                vecs = vecs.astype("float32")
                index.add(vecs)

                # å†™ embeddings æ˜ å°„
                for j, cid in enumerate(batch_cids):
                    vid = next_vid + j
                    conn.execute(
                        "INSERT INTO embeddings (chunk_id, vector_id) VALUES (?, ?)",
                        (cid, vid)
                    )
                next_vid += len(batch_cids)
                stats["faiss_vectors"] += len(batch_cids)

                if batch_end % (BATCH_SIZE * 4) == 0 or batch_end == len(all_chunk_texts):
                    logger.info(f"  å‘é‡è¿›åº¦: {batch_end}/{len(all_chunk_texts)}")

            conn.commit()
            faiss.write_index(index, str(faiss_path))
            logger.info(f"âœ… FAISS ç´¢å¼•å·²ä¿å­˜: {faiss_path} "
                        f"({index.ntotal} å‘é‡, {faiss_path.stat().st_size/(1024**2):.1f} MB)")

        except Exception as e:
            logger.error(f"âš ï¸  FAISS æ„å»ºå¤±è´¥ (FTS5 ä»å¯ç”¨): {e}")
    elif not FAISS_AVAILABLE:
        logger.info("â„¹ï¸  FAISS æœªå®‰è£…ï¼Œä»…ä½¿ç”¨ FTS5 å…³é”®è¯æ£€ç´¢")
        logger.info("   å®‰è£…: pip install sentence-transformers faiss-cpu")

    conn.close()
    return stats


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¿®å¤ï¼šä»å·²æœ‰ chunks è¡¨é‡å»º FTS5 ç´¢å¼•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def repair_fts(kb_dir: Path) -> Dict:
    """
    ä¿®å¤æŸåçš„ FTS5 ç´¢å¼• â€” ä»å·²æœ‰çš„ chunks è¡¨é‡å»ºã€‚
    ä¸éœ€è¦é‡æ–°æå– PDFï¼Œé€Ÿåº¦å¾ˆå¿«ã€‚

    Returns:
        {"chunks_indexed": N}
    """
    db_path = kb_dir / "kb.sqlite"
    if not db_path.exists():
        logger.error(f"âŒ çŸ¥è¯†åº“ä¸å­˜åœ¨: {db_path}")
        return {"chunks_indexed": 0}

    conn = sqlite3.connect(str(db_path))

    # æ£€æŸ¥ chunks è¡¨æ˜¯å¦å­˜åœ¨
    tables = {row[0] for row in conn.execute(
        "SELECT name FROM sqlite_master WHERE type='table'"
    ).fetchall()}
    if "chunks" not in tables:
        logger.error("âŒ chunks è¡¨ä¸å­˜åœ¨ï¼Œéœ€è¦å®Œæ•´é‡å»º: build-kb --rebuild")
        conn.close()
        return {"chunks_indexed": 0}

    # åˆ é™¤æ—§çš„ FTS5 è™šæ‹Ÿè¡¨ + æ‰€æœ‰ shadow è¡¨ï¼ˆå½»åº•æ¸…ç†ï¼‰
    shadow_tables = [
        "chunks_fts", "chunks_fts_data", "chunks_fts_idx",
        "chunks_fts_content", "chunks_fts_docsize", "chunks_fts_config",
    ]
    for tbl in shadow_tables:
        try:
            conn.execute(f"DROP TABLE IF EXISTS {tbl}")
        except sqlite3.DatabaseError:
            pass
    conn.commit()
    logger.info("ğŸ—‘ï¸  å·²åˆ é™¤æ—§ FTS5 ç´¢å¼•åŠ shadow è¡¨")

    # é‡æ–°åˆ›å»º FTS5 è™šæ‹Ÿè¡¨ï¼ˆç‹¬ç«‹å­˜å‚¨ï¼Œä¸ç”¨ content syncï¼‰
    conn.execute("""
        CREATE VIRTUAL TABLE chunks_fts
        USING fts5(text, chunk_id UNINDEXED)
    """)
    conn.commit()
    logger.info("âœ… å·²åˆ›å»ºæ–° FTS5 è™šæ‹Ÿè¡¨")

    # ä» chunks è¡¨å¡«å…… FTS5
    total = conn.execute("SELECT COUNT(*) FROM chunks").fetchone()[0]
    logger.info(f"ğŸ“ æ­£åœ¨ç´¢å¼• {total} ä¸ªåˆ†å—...")

    batch_size = 5000
    indexed = 0
    errors = 0
    cursor = conn.execute("SELECT chunk_id, text FROM chunks ORDER BY chunk_id")

    while True:
        rows = cursor.fetchmany(batch_size)
        if not rows:
            break
        for cid, text in rows:
            if not text:
                errors += 1
                continue
            try:
                conn.execute(
                    "INSERT INTO chunks_fts (text, chunk_id) VALUES (?, ?)",
                    (text, cid)
                )
                indexed += 1
            except Exception as e:
                errors += 1
                if errors <= 3:
                    logger.debug(f"  FTS INSERT å¤±è´¥ chunk_id={cid}: {e}")
        conn.commit()
        total_processed = indexed + errors
        if total_processed % 10000 < batch_size or total_processed >= total:
            logger.info(f"  è¿›åº¦: {total_processed}/{total} (æˆåŠŸ {indexed}, å¤±è´¥ {errors})")

    logger.info(f"âœ… FTS5 ç´¢å¼•é‡å»ºå®Œæˆ: {indexed} ä¸ªåˆ†å—")
    conn.close()
    return {"chunks_indexed": indexed}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI å…¥å£ï¼ˆç‹¬ç«‹è¿è¡Œï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    import argparse
    parser = argparse.ArgumentParser(description="AlaiDocs çŸ¥è¯†åº“æ„å»ºå™¨")
    parser.add_argument("--source", type=Path, required=True,
                        help="åˆ†ç±»åçš„ PDF ç›®å½• (classified_dir)")
    parser.add_argument("--output", type=Path, required=True,
                        help="çŸ¥è¯†åº“è¾“å‡ºç›®å½• (kb_dir)")
    parser.add_argument("--rebuild", action="store_true",
                        help="æ¸…ç©ºé‡å»ºï¼ˆé»˜è®¤å¢é‡ï¼‰")
    parser.add_argument("--no-faiss", action="store_true",
                        help="è·³è¿‡ FAISS å‘é‡ç´¢å¼•")
    parser.add_argument("--debug", action="store_true")
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.INFO,
        format="%(asctime)s â”‚ %(message)s", datefmt="%H:%M:%S"
    )

    if not args.source.exists():
        logger.error(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {args.source}")
        sys.exit(1)

    def progress(cur, total, name):
        if cur % 20 == 0 or cur == total:
            logger.info(f"  [{cur}/{total}] {name}")

    stats = build_kb(args.source, args.output,
                     rebuild=args.rebuild,
                     build_faiss=not args.no_faiss,
                     progress_callback=progress)

    print(f"\n{'â•'*50}")
    print(f"  ğŸ“Š æ„å»ºå®Œæˆ")
    print(f"     æ–°å¢æ–‡æ¡£:  {stats['docs_added']}")
    print(f"     æ–°å¢åˆ†å—:  {stats['chunks_added']}")
    print(f"     FAISSå‘é‡: {stats['faiss_vectors']}")
    print(f"     è·³è¿‡:      {stats['skipped']}")
    print(f"     é”™è¯¯:      {stats['errors']}")
    print(f"{'â•'*50}")


if __name__ == "__main__":
    main()
