#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¿€è¿›æ¸…ç† - åªä¿ç•™é«˜è´¨é‡å…³é”®è¯
åˆ é™¤æ‰€æœ‰åˆ†è¯ç¢ç‰‡å’Œç”Ÿæˆå¤±è´¥çš„å…³é”®è¯
"""

import json
from pathlib import Path
from datetime import datetime

def aggressive_clean():
    """æ¿€è¿›æ¸…ç†ï¼šåªä¿ç•™æ˜æ˜¾çš„é«˜è´¨é‡å…³é”®è¯"""
    file_path = Path("downloads_continuous/explored_keywords.json")
    
    if not file_path.exists():
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # è¯»å–åŸå§‹æ•°æ®
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    original_keywords = data.get('keywords', [])
    print(f"ğŸ“‹ åŸå§‹å…³é”®è¯æ•°é‡: {len(original_keywords)}")
    
    # é«˜è´¨é‡å…³é”®è¯ç‰¹å¾ï¼š
    # 1. é•¿åº¦é€‚ä¸­ï¼š10-80 å­—ç¬¦
    # 2. åŒ…å«å¸¸è§æ–‡æ¡£ç±»å‹è¯æˆ–æŠ€æœ¯æœ¯è¯­
    # 3. ä¸æ˜¯çº¯ç¢çš„3è¯ç»„åˆç¢ç‰‡
    
    quality_keywords = []
    removed = []
    
    # å¿…é¡»åŒ…å«çš„é«˜è´¨é‡æŒ‡ç¤ºè¯
    quality_indicators = [
        'datasheet', 'application note', 'design guide', 'reference design',
        'user guide', 'technical manual', 'evaluation board',
        'converter', 'regulator', 'controller', 'driver', 'pmic',
        'buck', 'boost', 'flyback', 'forward', 'llc', 'sepic', 'cuk'
    ]
    
    # ä½è´¨é‡æŒ‡ç¤ºè¯ï¼ˆé€šå¸¸æ˜¯ç¢ç‰‡ï¼‰
    low_quality_patterns = [
        'stage automotive', 'charger industrial', 'regulator telecom',
        'management battery monitoring', 'ic synchronous boost',
        'switching gate driver', 'circuit integrated power',
        'protection current limit', 'thermal shutdown feature'
    ]
    
    for kw in original_keywords:
        # é•¿åº¦æ£€æŸ¥
        if not (10 <= len(kw) <= 80):
            removed.append(('length', kw))
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä½è´¨é‡ç¢ç‰‡
        if any(pattern in kw.lower() for pattern in low_quality_patterns):
            removed.append(('fragment', kw))
            continue
        
        # å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªé«˜è´¨é‡æŒ‡ç¤ºè¯
        if not any(word in kw.lower() for word in quality_indicators):
            removed.append(('no_quality_indicator', kw))
            continue
        
        # é€šè¿‡æ‰€æœ‰æ£€æŸ¥
        quality_keywords.append(kw)
    
    print(f"âœ… é«˜è´¨é‡å…³é”®è¯: {len(quality_keywords)} ä¸ª")
    print(f"âŒ ç§»é™¤å…³é”®è¯: {len(removed)} ä¸ª")
    
    # ç»Ÿè®¡ç§»é™¤åŸå› 
    reasons = {}
    for reason, _ in removed:
        reasons[reason] = reasons.get(reason, 0) + 1
    
    print(f"\nç§»é™¤åŸå› ç»Ÿè®¡:")
    for reason, count in reasons.items():
        print(f"  {reason}: {count} ä¸ª")
    
    # æ˜¾ç¤ºä¸€äº›è¢«ç§»é™¤çš„ç¤ºä¾‹
    if removed:
        print(f"\nè¢«ç§»é™¤ç¤ºä¾‹ (å‰10ä¸ª):")
        for i, (reason, kw) in enumerate(removed[:10], 1):
            preview = kw[:80] + '...' if len(kw) > 80 else kw
            print(f"  {i}. [{reason}] {preview}")
    
    # æ˜¾ç¤ºä¿ç•™çš„å…³é”®è¯ç¤ºä¾‹
    print(f"\nä¿ç•™çš„å…³é”®è¯ç¤ºä¾‹ (å‰10ä¸ª):")
    for i, kw in enumerate(quality_keywords[:10], 1):
        print(f"  {i}. {kw}")
    
    print(f"\nä¿ç•™çš„å…³é”®è¯ç¤ºä¾‹ (å10ä¸ª):")
    for i, kw in enumerate(quality_keywords[-10:], len(quality_keywords)-9):
        print(f"  {i}. {kw}")
    
    # è¯¢é—®ç¡®è®¤
    print(f"\n{'='*60}")
    print(f"å°†ä» {len(original_keywords)} ä¸ªå‡å°‘åˆ° {len(quality_keywords)} ä¸ª")
    print(f"{'='*60}")
    
    confirm = input("ç¡®è®¤æ‰§è¡Œæ¸…ç†? (y/n): ")
    if confirm.lower() != 'y':
        print("å–æ¶ˆæ“ä½œ")
        return
    
    # å¤‡ä»½
    backup_path = file_path.with_suffix('.json.backup2')
    with open(backup_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"âœ… å·²å¤‡ä»½åˆ°: {backup_path}")
    
    # ä¿å­˜æ¸…ç†åçš„æ•°æ®
    cleaned_data = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'keywords': quality_keywords,
        'count': len(quality_keywords),
        'cleaned_from': len(original_keywords),
        'removed': len(removed)
    }
    
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(cleaned_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æ¿€è¿›æ¸…ç†å®Œæˆ!")
    print(f"   åŸå§‹: {len(original_keywords)} ä¸ª")
    print(f"   æ¸…ç†å: {len(quality_keywords)} ä¸ª")
    print(f"   ç§»é™¤: {len(removed)} ä¸ª")

if __name__ == '__main__':
    aggressive_clean()
